<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1>Advanced Mirror Configuration</h1>
        <h1><a name="custom-mirror-service-name"></a>Custom Mirror Service Name</h1>
        <p>A Mirror Service can be configured per Space cluster. You can't have multiple Mirror services configured for the same space cluster.</p>
        <div class="tc-admon-tip">
            <p>If you need "multiple mirrors" for the same space cluster you can implement a Mirror Service that will route the data and operations to other multiple "agents" that will persist the data - effectively make the default Mirror act as a dispatcher.</p>
        </div>
        <p>If you have multiple different space clusters, each with its own Mirror service running, you should use a different name for each Mirror Service.</p>
        <p>The Mirror Service name is used as part of the space config, specified via the <code>cluster-config.mirror-service.url</code> property. Its default is <code>jini://*/mirror-service_container/mirror-service</code> which match the "mirror-service" that is used as part of the <code>url</code> property used to start the mirror service.</p>
        <p>As an example, let's say we would like to call my mirror service <code>mymirror-service</code> (instead of the default <code>mirror-service</code>). Here is how the mirror service should be started:</p><pre><code class="language-xml">&lt;os-core:embedded-space id="space" space-name="myMirror-service" schema="mirror"
    space-sync-endpoint="mirrorSynchronizationEndpoint" /&gt;
</code></pre>
        <p>Here is how the space should be started:</p><pre><code class="language-xml">&lt;os-core:embedded-space id="space" space-name="mySpace" schema="persistent"
    mirror="true" space-data-source="spaceDataSource"&gt;
    &lt;os-core:properties&gt;
        &lt;props&gt;
            &lt;prop key="cluster-config.mirror-service.url"&gt;
                jini://*/mymirror-service_container/mymirror-service
            &lt;/prop&gt;
        &lt;/props&gt;
    &lt;/os-core:properties&gt;
&lt;/os-core:space&gt;
</code></pre>
        <h1><a name="implementing-a-custom-mirror-data-source"></a>Implementing a Custom Mirror Data Source</h1>
        <p><MadCap:variable name="General.ProductNameXAP" /> has a built in <a href="hibernate-space-persistency.html">Hibernate Space Persistency</a> implementation which is a <code>SpaceSynchronizationEndpoint</code> extension. You can implement your own Mirror very easily to accommodate your exact needs. See example below:</p>
        <div class="row">
            <div class="easyui-accordion" data-options="selected:'-1'" plain="true">
                <div title="MirrorSpaceSynchronizationEndpoint" style="padding:10px;"><pre><code class="language-java">package mypackage;

public class MirrorSpaceSynchronizationEndpoint extends SpaceSynchronizationEndpoint {

    private String parameter;

    public void setParameter(String parameter) {
        this.parameter = parameter;
    }

    @Override
    public void onOperationsBatchSynchronization(OperationsBatchData batchData) {
        for (DataSyncOperation operation : batchData.getBatchDataItems()) {
            switch (operation.getDataSyncOperationType()) {
                case WRITE:
                    // new entry has been written
                    break;
                case UPDATE:
                    // entry was updated
                    break;
                case REMOVE:
                    // entry was removed
                    break;
                case CHANGE:
                    // entry was modified using change API
                    break;
            }
        }
    }
}
</code></pre>
                </div>
            </div>
        </div>
        <p>And here is how this can be configured within the mirror configuration:</p>
        <div class="row">
            <div class="easyui-accordion" data-options="selected:'-1'" plain="true">
                <div title="Mirror Configuration" style="padding:10px;">
                    <div class="easyui-tabs" plain="true" data-options="">
                        <div title="  Namespace " style="padding:10px"><pre><code class="language-xml">&lt;bean id="mirrorSpaceSynchronizationEndpoint" class="mypackage.MirrorSpaceSynchronizationEndpoint"&gt;
    &lt;property name="parameter" value="some value"/&gt;
&lt;/bean&gt;

&lt;os-core:embedded-space id="space" space-name="mirror-service" schema="mirror"
    space-sync-endpoint="mirrorSpaceSynchronizationEndpoint" /&gt;
</code></pre>
                        </div>
                        <div title="  Plain XML " style="padding:10px"><pre><code class="language-xml">&lt;bean id="mirrorSpaceSynchronizationEndpoint" class="mypackage.MirrorSpaceSynchronizationEndpoint"&gt;
    &lt;property name="parameter" value="some value"/&gt;
&lt;/bean&gt;

&lt;bean id="space" class="org.openspaces.core.space.EmbeddedSpaceFactoryBean"&gt;
    &lt;property name="name" value="mirror-service" /&gt;
    &lt;property name="schema" value="mirror" /&gt;
    &lt;property name="spaceSyncEndpoint" ref="mirrorSpaceSynchronizationEndpoint" /&gt;
&lt;/bean&gt;
</code></pre>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <p>In order to use the data source as the read mechanism for the cluster Space that connects to the mirror, a <code>SpaceDataSource</code> extension needs to be implemented.</p>
        <div class="tc-admon-important">
            <p>Regarding error handling for mirror failures, throwing <code>java.lang.RuntimeException</code> from the <code>onTransactionSynchronization</code> or <code>onOperationsBatchSynchronization</code> methods of the <code>SpaceSynchronizationEndpoint</code> implementation will signal the primary to keep the replicated data within its redo log and retry the replication operation. Your code should support this in case it fails (for example database transaction failure).</p>
        </div>
        <h1><a name="multiple-mirrors"></a>Multiple Mirrors</h1>
        <p>In some cases you may need to asynchronously persist data both into a relational database and a file, or persist the data into a relational database and transfer some of the data into some other system.</p>
        <p>In such a case you may need to have multiple mirrors. In order to implement this, you should have one base mirror (for example the Hibernate Space Persistency) and extend it to include the extra functionality you may need.</p>
        <div class="tc-admon-note">
            <p>See the <a href="/sbp/mirror-monitor.html">Mirror Monitor</a> page for a simple example how such approach should be implemented.</p>
        </div>
        <p>There are cases with large space clusters or with systems that produce large volume of activity where the amount of activity performed by a clustered space would require a distributed (multi-instance) mirror setup.</p>
        <div class="tc-admon-note">
            <p>See the <a href="/sbp/distributed-mirror.html">Distributed Mirror</a> page for an example how to configure this scenario.</p>
        </div>
        <h1><a name="handling-mirror-exceptions"></a>Handling Mirror Exceptions</h1>
        <p>Since the space synchronization endpoint configured for the mirror service communicates with the database, it may run into database related errors, such as constraint violations, wrong class mappings (if the Hibernate-based space synchronization endpoint implementation is used), or other database-related errors.</p>
        <p>By default, these errors are propagated to the replicating space (primary space instance), and will appear in its logs. In such a case, the primary space will try to replicate the batch the caused the error to the mirror service again, until it succeeds (meaning that no exception has been exposed to the user's application code).</p>
        <p>To override and extend this behavior, you can implement an exception handler that will be called when an exception is thrown from the Mirror back to the primary space. This exception handler can log the exception at the mirror side, throw it back to the space, ignore it or execute any user specific code. Here is an example of how this is done using the <code>org.openspaces.persistency.patterns.SpaceSynchronizationEndpointExceptionHandler</code> provided with OpenSpaces:</p><pre><code class="language-xml">&lt;bean id="hibernateSpaceSynchronizationEndpoint"
    class="org.openspaces.persistency.hibernate.DefaultHibernateSpaceSynchronizationEndpointFactoryBean"&gt;
    &lt;property name="sessionFactory" ref="sessionFactory"/&gt;
&lt;/bean&gt;

&lt;bean id="exceptionHandler" class="eg.MyExceptionHandler"/&gt;

&lt;bean id="exceptionHandlingSpaceSynchronizationEndpoint"
    class="org.openspaces.persistency.patterns.SpaceSynchronizationEndpointExceptionHandler"&gt;
    &lt;constructor-arg ref="hibernateSpaceSynchronizationEndpoint"/&gt;
    &lt;constructor-arg ref="exceptionHandler"/&gt;
&lt;/bean&gt;

&lt;os-core:embedded-space id="space" space-name="mirror-service" schema="mirror"
    space-sync-endpoint="exceptionHandlingSpaceSynchronizationEndpoint"/&gt;
</code></pre>
        <p>With the above, we use the <code>SpaceSynchronizationEndpointExceptionHandler</code>, and wrap the <code>DefaultHibernateSpaceSynchronizationEndpoint</code> with it (and pass that to the space). On the <code>SpaceSynchronizationEndpointExceptionHandler</code>, we set our own implementation of the <code>ExceptionHandler</code>, to be called when there is an exception. With the <code>ExceptionHandler</code> you can decide what to do with the Exception: "swallow it", execute some logic, or re-throw it.</p>
        <h2><a name="testing-space-persistence-in-the-mirror"></a>Testing Space Persistence in the Mirror</h2>
        <p>A configured mirror will repeatedly try to store things in the database. If there is an unrecoverable failure (such as an invalid mapping or a constraint issue), this can cause the redo log to grow, eventually resulting in overflow of the redo to disk, and then, when the predefined disk capacity is exhausted, leading to a rejection of any non-read space operation (similar to how the memory manager works). The exception that clients will see in this case is <code>RedologCapacityExceededException</code> (which inherits from <code>ResourceCapacityExceededException</code>).</p>
        <p>The application can handle this by using the <code>ExceptionHandler</code> at the mirror EDS level. It can count the number of consecutive failures returned from the database, and when a certain threshold is reached, log it somewhere and move on, for example.</p>
        <p>That said, the easiest thing to do is test your persistence in the mirror.</p>
        <h1><a name="mirror-behavior-with-distributed-transactions"></a>Mirror Behavior with Distributed Transactions</h1>
        <p>When using the Distributed Transaction Manager and persisting data through the mirror service, each partition sends its transaction data to the Mirror on commit. The mirror service receives the replication messages in bulk from each partition that participated in the transaction. In order to keep database data consistent when persisting the data, these bulks should to be consolidated at the mirror service.</p>
        <p>This can be achieved by :</p>
        <ol>
            <li>Setting the following property in the mirror configuration:</li>
        </ol><pre><code class="language-xml">&lt;os-core:embedded-space id="space" space-name="mirror-service" schema="mirror" space-sync-endpoint="spaceSynchronizationEndpoint"&gt;
    &lt;os-core:properties&gt;
    &lt;props&gt;
         &lt;prop key="space-config.mirror-service.operation-grouping"&gt;
               group-by-space-transaction
             &lt;/prop&gt;
    &lt;/props&gt;
    &lt;/os-core:properties&gt;
&lt;/os-core:embedded-space&gt;
</code></pre>
        <p>By default this property is set to <code>group-by-replication-bulk</code> and <code>executeBulk()</code> groups several transactions and executes them together. The group size is defined by the mirror replication <code>bulk-size</code>.</p>
        <p>Setting this property will cause a <code>SpaceSynchronizationEndpoint.onTransactionSynchronization</code> invocation for each transaction separately.</p>
        <h1><a name="getting-the-transaction-metadata"></a>Getting the Transaction Metadata</h1>
        <p>The following demonstrates how the transaction metadata can be retrieved:</p><pre><code class="language-java">public class MySpaceSynchronizationEndpoint extends SpaceSynchronizationEndpoint {
    public void onTransactionSynchronization(TransactionData transactionData) {
        TransactionParticipantMetaData metaData = transactionData.getTransactionParticipantMetaData();
        int participantId = metaData.getParticipantId();
        int participantsCount = metaData.getTransactionParticipantsCount();
        TransactionUniqueId transactionId = metaData.getTransactionUniqueId();
        // ...
    }
}
</code></pre>
        <div class="tc-admon-note">
            <ul>
                <li><code>onOperationsBatchSynchronization()</code>/<code>onTransactionSynchronization</code> are called concurrently, so implementations should take concurrency into consideration.</li>
                <li>Non-transactional operations are grouped according to the replication policy (<code>bulk-size</code> and <code>interval-millis</code>) and sent to the Mirror Service.</li>
                <li>Transactional and non-transactional items are not mixed.</li>
            </ul>
        </div>
        <p>The transaction participant metadata interface describes the transaction's unique ID, which consists of the transaction's ID and the transaction manager ID who created it:</p><pre><code class="language-java">/**
 * Represents a transaction meta data for a specific transaction participant.
 * @since 9.0.1
 */
public interface TransactionParticipantMetaData {
    /**
     * The id of the space that committed the transaction.
     * @return the participantId
     */
    public int getParticipantId();

    /**
     * Number of participants in the transaction
     * @return the participantsCount
     */
    public int getTransactionParticipantsCount();

    /**
     * The id of the transaction
     * @return the transactionId
     */
    public TransactionUniqueId getTransactionUniqueId();
}
</code></pre><pre><code class="language-java">/**
 * Represents a transaction unique id constructed from the transaction manager which created the transaction
 * Uuid and the transaction id.
 * @since 9.0.1
 */
public interface TransactionUniqueId
{
    /**
     * @return The transaction manager which created the transaction {@link Uuid}.
     */
    Uuid getTransactionManagerId();

    /**
     * @return The transaction id.
     */
    Object getTransactionId();
}
</code></pre>
        <h2><a name="mirror-distributed-transaction-consolidation-atomic-transaction-delegation"></a>Mirror Distributed Transaction Consolidation - Atomic Transaction Delegation</h2>
        <p>Once a the entire distributed transaction content arriving from each data grid partition participating in the transaction into the mirror to be persist, you may consolidate it to perform one database transaction rather several separate database transactions. This keeps the database data consistent and reduce database overhead.</p>
        <p>Distributed transaction consolidation is configured via the space configuration using the <code>cluster-config.groups.group.repl-policy.processing-type</code> property.</p>
        <p>In the following example we configure a space using its <code>pu.xml</code> to have transaction consolidation mode as enabled:</p>
        <div class="tc-admon-note">
            <p>Distributed transaction consolidation is enabled by default.</p>
        </div><pre><code class="language-xml">&lt;os-core:embedded-space id="space" space-name="mySpace"&gt;
  &lt;os-core:properties&gt;
    &lt;props&gt;
      &lt;prop key="cluster-config.groups.group.repl-policy.processing-type"&gt;
        multi-source
      &lt;/prop&gt;
    &lt;/props&gt;
  &lt;/os-core:properties&gt;
&lt;/os-core:embedded-space&gt;
</code></pre>
        <p>As specified in the example above, it is required to set the <code>cluster-config.groups.group.repl-policy.processing-type</code> property to <code>multi-source</code>.</p>
        <p>The <code>cluster-config.groups.group.repl-policy.processing-type</code> may have the following values:</p>
        <ul>
            <li><code>global-order</code>  - Do not maintain any ordering. Mirror consolidation is not executed.</li>
            <li><code>multi-source</code> - Maintain ordering per partition. Transaction consolidation is executed.</li>
        </ul>
        <div class="tc-admon-note">
            <p>If you have set the value of the <code>on-redo-log-capacity-exceeded</code> parameter to <code>drop-oldest</code>, you must set the <code>global order</code> value for this property.</p>
        </div>
        <p>In order to take advantage of this feature, mirror operation grouping should be set to <code>group-by-space-transaction</code> in mirror <code>pu.xml</code>:</p><pre><code class="language-xml">&lt;os-core:embedded-space id="space" space-name="mirror-service"
  schema="mirror" space-sync-endpoint="spaceSynchronizationEndpoint"&gt;
  &lt;os-core:properties&gt;
    &lt;props&gt;
      &lt;prop key="space-config.mirror-service.operation-grouping"&gt;
         group-by-space-transaction
      &lt;/prop&gt;
    &lt;/props&gt;
  &lt;/os-core:properties&gt;
&lt;/os-core:embedded-space&gt;
</code></pre>
        <h1><a name="distributed-transaction-consolidation-example"></a>Distributed Transaction Consolidation Example:</h1><pre><code class="language-java">public class MySpaceSynchronizationEndpoint extends SpaceSynchronizationEndpoint {

@Override
public void onTransactionSynchronization(TransactionData transactionData) {
    if (transactionData.isConsolidated()) {
    // this is a consolidated distributed transaction...
    ConsolidatedDistributedTransactionMetaData metaData = transactionData.getConsolidatedDistributedTransactionMetaData();
    }
}

@Override
public void onTransactionConsolidationFailure(ConsolidationParticipantData participantData) {
    // intercept transaction consolidation failure &amp; decide whether to commit or abort this participant data
    if (sunnyDay)
    participantData.commit();
    else
    participantData.abort();
}

}
</code></pre>
        <p>With the above example the "participantData.commit()` call assumes the logic accepting each transaction participant data to be persist separately despite a failure with the transaction consolidation. "participantData.abort()' indicates the logic must have all transaction participant's data to be fully consolidated.</p>
        <p>Distributed transaction consolidation is done by waiting for all the transaction participants' data to arrive the mirror from all relevant data grid partitions before processed by the mirror and persisting into the database.</p>
        <p>In some cases certain distributed transaction participants data might be delayed due-to network delay or disconnection and therefore may cause delays in replication into the mirror. In order to prevent this delay, you may set a timeout parameter that indicates how much time the mirror will wait the for distributed transactions participants data to arrive before processing the data individually for each participant.</p>
        <p>Please note that while waiting for a distributed transaction to entirely arrive the mirror, replication isn't waiting but keeping the data flow and preventing from conflicts to happen.</p>
        <p>The following example demonstrates how to set the timeout for waiting for distributed transaction data to arrive, it is also possible to set the amount of new operations to perform before processing data individually for each participant:</p><pre><code class="language-xml">&lt;os-core:embedded-space id="space" space-name="mirror-service"  schema="mirror" space-sync-endpoint="spaceSynchronizationEndpoint"&gt;
  &lt;os-core:properties&gt;
    &lt;props&gt;
      &lt;prop key="space-config.mirror-service.operation-grouping"&gt;
        group-by-space-transaction
      &lt;/prop&gt;
      &lt;prop key="space-config.mirror-service.distributed-transaction-processing.wait-timeout"&gt;120000&lt;/prop&gt;
      &lt;prop key="space-config.mirror-service.distributed-transaction-processing.wait-for-operations"&gt;100&lt;/prop&gt;
    &lt;/props&gt;
  &lt;/os-core:properties&gt;
&lt;/os-core:embedded-space&gt;
</code></pre>
        <p>Distributed transaction participants' data will be processed individually if ten seconds have passed and all of the participants data has <span class="tc-bold">not</span> arrived or if 20 new operations were executed after the distributed transaction.</p>
        <table>
            <thead>
                <tr>
                    <th align="left">Attribute</th>
                    <th align="left">Default Value</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td align="left">space-config.mirror-service.distributed-transaction-processing.wait-timeout</td>
                    <td align="left">60000 milliseconds</td>
                </tr>
                <tr>
                    <td align="left">space-config.mirror-service.distributed-transaction-processing.wait-for-operations</td>
                    <td align="left">unlimited (-1 = unlimited)</td>
                </tr>
            </tbody>
        </table>
        <p>Note that by setting the <code>cluster-config.groups.group.repl-policy.processing-type</code> property to <code>multi-source</code> all reliable asynchronous replication targets for that space will work in distributed transaction consolidation mode (For example: Gateway Sink).</p>
        <div class="tc-admon-note">
            <p>Setting both <code>dist-tx-wait-timeout-millis</code> and <code>dist-tx-wait-for-opers</code> to unlimited (or very high value) is risky and may cause replication backlog accumulation due to a packet which is unconsolidated and waits for consolidation which may never occur due to various reasons.</p>
        </div>
        <h1><a name="usage-scenarios"></a>Usage Scenarios</h1>
        <h2><a name="writing-synchronously-to-the-mirror-data-source"></a>Writing Synchronously to the Mirror Data Source</h2>
        <p>The following is a schematic flow of a synchronous replicated cluster with three members, which are communicating with a Mirror Service:</p>
        <div class="tc-align-center">
            <img src="../Resources/Static/attachment_files/IMG101.gif" alt="" title="" class="tc-picture80" />
        </div>
        <h2><a name="reading-from-the-data-source"></a>Reading from the Data Source</h2>
        <p>The Mirror Service space is used to asynchronously <span class="tc-bold">persist</span> data into the data source. As noted elsewhere, the Mirror is <span class="tc-bold">not</span> a regular space, and should <span class="tc-bold">not</span> be interacted with directly. Thus, data can't be read from the data source using the Mirror Service space. Nonetheless, the data might be read by other spaces which are configured with a space data source.</p>
        <p>The data-grid pu.xml needs to be configured to use an <span class="tc-bold">space data source</span> which, when dealing with a Mirror, is <span class="tc-bold">central</span> to the cluster.</p>
        <p>Here is a schematic flow of how a Mirror Service asynchronously receives data, to persist into an data source, while the cluster is reading data directly from the data source.</p>
        <div class="tc-align-center">
            <img src="../Resources/Static/attachment_files/IMG103.gif" alt="" title="" class="tc-picture80" />
        </div>
        <h2><a name="partitioning-over-a-central-mirror-data-source"></a>Partitioning over a Central Mirror Data Source</h2>
        <p>When partitioning data, each partition asynchronously replicates data into the Mirror Service. Each partition can read back data that belongs to it (according to the load-balancing policy defined).Here is a schematic flow of how two partitions (each a primary-backup pair) asynchronously interact with a data source:</p>
        <div class="tc-align-center">
            <img src="../Resources/Static/attachment_files/IMG104.gif" alt="" title="" class="tc-picture80" /><a name="dist-txn-mirror"></a>
        </div>
        <h1><a name="considerations-and-known-issues"></a>Considerations and Known Issues</h1>
        <ul>
            <li>The Mirror Service cannot be used with a single space. It is supported with the <code>async-replicated</code>, and <code>partitioned</code> cluster schemas.</li>
            <li>The Mirror Service is a single space which joins a replication group. The Mirror Service is not a clustered space or part of the replication group declaration.</li>
            <li>When the Mirror Service is loaded, it does not perform memory recovery.</li>
            <li><a href="transient-entries.html">Transient Entries</a> are not persisted into the data source, just as in any other persistent mode.</li>
            <li>You should have one Mirror Service running per Data-Grid cluster.</li>
            <li>The Mirror Service cannot be clustered. Deploying it as a Processing unit ensures its high availability.</li>
            <li>The Mirror does not load data back into the space. The <code>SpaceDataSource</code> implementation of the space should be used to initialize the space when started.</li>
        </ul>
        <div class="tc-admon-note">
            <p><a href="space-persistency-advanced-topics.html#limits">Space persistency considerations</a> also apply to the Mirror Service.</p>
        </div>
        <h1><a name="troubleshooting"></a>Troubleshooting</h1>
        <h2><a name="log-messages"></a>Log Messages</h2>
        <p>The space persistency logging level can be modified as part of the <code><MadCap:variable name="General.HomePath" />\config\log\xap_logging.properties</code> file. By default, it is set to <code>java.util.logging.Level.INFO</code>:</p><pre><code class="language-java">com.gigaspaces.persistent.level = INFO
</code></pre>
        <p>Logging is divided according to <code>java.util.logging.Level</code> as follows:</p>
        <table>
            <thead>
                <tr>
                    <th align="left">Level</th>
                    <th align="left">Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td align="left">INFO</td>
                    <td align="left">The default level for informative messages.</td>
                </tr>
                <tr>
                    <td align="left">CONFIG</td>
                    <td align="left">Mirror Service-relevant configuration messages.</td>
                </tr>
                <tr>
                    <td align="left">FINER</td>
                    <td align="left">Fairly detailed messages of:<br />- <span class="tc-bold">Entering and exiting</span> interface methods (displaying the parameter's <code>toString()</code> method)<br />- <span class="tc-bold">Throwing of exceptions</span> between the space and the underlying implementation.</td>
                </tr>
            </tbody>
        </table>
        <h1><a name="failover-handling"></a>Failover Handling</h1>
        <p>This section describes how the GigaSpaces Mirror Service handles different failure scenarios. The following table lists the services involved, and how the failure is handled in the cluster.</p>
        <p>Active services are <span class="tc-textgreen">green</span>, while failed services are <span class="tc-textred">red</span>.</p>
        <table>
            <thead>
                <tr>
                    <th align="left">Active/Failed Services</th>
                    <th align="left">Cluster Behavior</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td align="left">* <span class="tc-textgreen">Primary</span><br />- <span class="tc-textgreen">Backup</span><br />- <span class="tc-textgreen">Mirror</span><br />- <span class="tc-textgreen">Database</span><br /></td>
                    <td align="left">* The primary and backup spaces, each include a copy of the mirror replication queue (which is created in the backup, as part of the synchronized replication between the primary and the backup). <br />- The mirror doesn't acknowledge the replication until the data is successfully committed to the database.<br />- Every time the primary gets an acknowledgment from the mirror, it notifies the backup of the last exact point in the replication queue where replication to the mirror was successful.<br />- This way, the primary and backup space include the same copy of the data and are also in sync with whatever data was replicated to the mirror and written to the database.</td>
                </tr>
                <tr>
                    <td align="left">* <span class="tc-textred">Primary</span><br />- <span class="tc-textgreen">Backup</span><br />- <span class="tc-textgreen">Mirror</span><br />- <span class="tc-textgreen">Database</span><br /></td>
                    <td align="left">* The backup space holds all the information in-memory, since the replication channel between them is synchronous. <br />- The backup space is constantly notified of the last exact point in the replication queue where replication to the mirror was successful. This means that it knows specifically when the failure occurred. Therefore, it replicates the data received from that point onwards, to the mirror.<br />- One possible scenario is that the same Entry is sent to the mirror, both by the primary and the backup space. However, the mirror handles this situation, so no data is lost or duplicated.<br />- If the primary space is restarted (typically by the Service Grid infrastructure), it recovers all of the data from the backup space. Once the primary has retrieved all data from the backup, it continues replicating as usual. No data is lost.</td>
                </tr>
                <tr>
                    <td align="left">* <span class="tc-textgreen">Primary</span><br />- <span class="tc-textred">Backup</span><br />- <span class="tc-textgreen">Mirror</span><br />- <span class="tc-textgreen">Database</span><br /></td>
                    <td align="left">* The primary keeps functioning as before: replicating data to the mirror and persisting data asynchronously, so no data is lost. <br />- The primary space is constantly notified of the last exact point in the replication queue where replication to the mirror was successful. This means that it knows specifically when the failure occurred. Therefore, it replicates the data received from that point onwards to the mirror.<br />- One possible scenario is that the same Entry is sent to the mirror both by the primary and the backup space. However, the mirror handles this situation, so no data is lost or duplicated.<br />- If the backup space is restarted (typically by the Service Grid infrastructure), it recovers all of the data from the primary space. Once the backup has retrieved all data from the primary, it continues replicating as usual. No data is lost.</td>
                </tr>
                <tr>
                    <td align="left">* <span class="tc-textgreen">Primary</span><br />- <span class="tc-textgreen">Back Up</span><br />- <span class="tc-textred">Mirror</span><br />- <span class="tc-textgreen">Database</span><br /></td>
                    <td align="left">* The primary and backup spaces accumulate the Entries and replicate them to their mirror replication queue (which is highly available since they both share it). <br />- When the mirror is restarted, replication is resumed from the point it was stopped, prior to the failure. No data is lost.</td>
                </tr>
                <tr>
                    <td align="left">* <span class="tc-textgreen">Primary</span><br />- <span class="tc-textgreen">Backup</span><br />- <span class="tc-textgreen">Mirror</span><br />- <span class="tc-textred">Database</span><br /></td>
                    <td align="left">* The primary space is constantly synchronized with the mirror, which stops sending acknowledgments or starts sending errors to it. <br />- The primary and backup spaces accumulate the Entries and replicate them to their mirror replication queue (which is highly available since they both share it). <br />- When the database is restarted, the mirror reconnects to it and persistency is resumed from the point it was stopped, prior to the failure. No data is lost.</td>
                </tr>
            </tbody>
        </table>
        <h1><a name="unlikely-failure-scenarios"></a>Unlikely Failure Scenarios</h1>
        <p>The following failure scenarios are highly unlikely. However, it might be useful to understand how such scenarios are handled by GigaSpaces. This is detailed in the table below.</p>
        <p>Active services are <span class="tc-textgreen">green</span>, while failed services are <span class="tc-textred">red</span>.</p>
        <table>
            <thead>
                <tr>
                    <th align="left">Active/Failed Services</th>
                    <th align="left">Cluster Behavior</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td align="left">* <span class="tc-textred">Primary</span><br />- <span class="tc-textgreen">Backup</span><br />- <span class="tc-textred">Mirror</span><br />- <span class="tc-textgreen">Database</span><br /></td>
                    <td align="left">* Data which has already been saved in the database is safe.<br />- Data held in the mirror replication queue still exists in the backup, so no data is lost.</td>
                </tr>
                <tr>
                    <td align="left">* <span class="tc-textred">Primary</span><br />- <span class="tc-textgreen">Backup</span><br />- <span class="tc-textgreen">Mirror</span><br />- <span class="tc-textred">Database</span><br /></td>
                    <td align="left">* Data which has already been saved in the database is safe. <br />- Data held in the mirror replication queue still exists in the backup, so no data is lost.</td>
                </tr>
                <tr>
                    <td align="left">* <span class="tc-textgreen">Primary</span><br />- <span class="tc-textred">Backup</span><br />- <span class="tc-textred">Mirror</span><br />- <span class="tc-textgreen">Database</span><br /></td>
                    <td align="left">Same as above – no data is lost.</td>
                </tr>
                <tr>
                    <td align="left">* <span class="tc-textgreen">Primary</span><br />- <span class="tc-textred">Backup</span><br />- <span class="tc-textgreen">Mirror</span><br />- <span class="tc-textred">Database</span><br /></td>
                    <td align="left">Same as above – no data is lost.</td>
                </tr>
                <tr>
                    <td align="left">* <span class="tc-textgreen">Primary</span><br />- <span class="tc-textgreen">Backup</span><br />- <span class="tc-textred">Mirror</span><br />- <span class="tc-textred">Database</span><br /></td>
                    <td align="left">* Data which has already been saved in the database is safe. <br />- Data queued in the mirror replication queue still exists in the primary and the backup, so no data is lost.</td>
                </tr>
                <tr>
                    <td align="left">* <span class="tc-textred">Primary</span><br />- <span class="tc-textred">Backup</span><br />- <span class="tc-textgreen">Mirror</span><br />- <span class="tc-textgreen">Database</span><br /></td>
                    <td align="left">* All data that was successfully replicated to the mirror (and hence persisted to the database) is safe.<br />- Data queued in the mirror replication queue in the primary and backup spaces is lost. <br />- If you encounter this scenario, a configuration with two backups per partition should be considered.</td>
                </tr>
            </tbody>
        </table>
    </body>
</html>