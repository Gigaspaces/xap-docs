<?xml version="1.0" encoding="utf-8"?>
<html>
<head><title></title></head>
<body>
<h1>XAP Manager</h1>
  

<p>The XAP Manager (or simply The Manager) is a component which stacks together the <a href="../overview/the-runtime-environment.htm#lus">LUS</a> and <a href="../overview/the-runtime-environment.htm#gsm">GSM</a>
along with <a href="http://zookeeper.apache.org/" target="_blank">Apache ZooKeeper</a> and an embedded web application which hosts an admin instance with a <a href="xap-manager-rest.htm">RESTful management API</a> on top of it.</p>

<p>In addition to simplifying setup and management, the Manager also provides the following benefits:</p>

<ul>
<li>Space leader election will use zookeeper instead of LUS, providing a more robust process (consistent when network partitions occur), and eliminating <a href="split-brain-and-primary-resolution.htm">split brains</a>.</li>
<li>When using MemoryXtend, last primary will automatically be stored in Zookeeper (instead of you needing to setup a shared NFS and configure the PU to use it)</li>
<li>The GSM will use Zookeeper for leader election (instead of an active-active topology used today). This provides a more robust process (consistent when network partitions occur). Also, having a single leader GSM means that the general behaviour is more deterministic and logs are easier to read.</li>
<li>RESTful API for managing the environment remotely from any platform.</li>
</ul>

<h1 id="getting-started">Getting Started</h1>

<p>The easiest way to get started is to run a standalone manager on your machine - simply run the following command:</p>

<div class="easyui-tabs" plain="true" data-options=""><div title="Linux" style="padding: 10px;"><pre xml:space="preserve"><code class="language-bash">./gs-agent.sh --manager-local
</code></pre>
</div>
<div title="Windows" style="padding: 10px;"><pre xml:space="preserve"><code class="language-bash">gs-agent.bat --manager-local
</code></pre>
</div>
</div>

<p>In the manager log file (<code>$XAP_HOME/logs</code>), you can see:</p>

<ul>
<li>The maanger has started LUS, Zookeeper, GSM and REST API have started and various other details about them.</li>
<li>Zookeeper files reside in <code>$XAP_HOME/work/manager/zookeeper</code></li>
<li>REST API is started on <a href="http://localhost:8090">localhost:8090</a></li>
</ul>

<div class="tc-admon-note">
  <p class="tc-admon-title">Remote Access</p>
  <p>The local manager is intended for local usage on the developer's machine, hence it binds to <code>localhost</code>, and is not accessible from other machines. If you wish to start a manager and access it from other hosts, follow the procedure described in <span class="tc-bold">High Availability</span> below with a single host.</p>
</div>

<h1 id="high-availability">High Availability</h1>

<p>In a production environment, you'll probably want a cluster of managers on multiple hosts, to ensure high availability. You'll need 3 machines (odd number is required to ensure quorum during network partitions). For examplem, suppose you’ve selected machines alpha, bravo and charlie to host the managers:</p>

<ol>
<li>Edit the <code>$XAP_HOME/bin/setenv-overrides.sh/bat</code> script and set <code>XAP_MANAGER_SERVERS</code> to the list of hosts. For example: <code>export XAP_MANAGER_SERVERS=alpha,bravo,charlie</code></li>
<li>Copy the modified <code>setenv-overrides.sh/bat</code> to each machine which runs a <code>gs-agent</code>.</li>
<li>Run <code>gs-agent --manager</code> on the manager machines (alpha, bravo, charlie, in this case).</li>
</ol>

<p>Note that starting more than one manager on the same host is not supported.</p>

<h1 id="configuration">Configuration</h1>

<h2 id="ports">Ports</h2>

<p>The following ports can be modified using system properties, e.g. via the <code>setenv-overrides</code> script located in <code>$XAP_HOME/bin</code>:</p>

<table>
<thead>
<tr>
<th>Port</th>
<th>System property</th>
<th>Default</th>
</tr>
</thead>

<tbody>
<tr>
<td>REST</td>
<td><code>com.gs.manager.rest.port</code></td>
<td>8090</td>
</tr>

<tr>
<td>Zookeeper</td>
<td><code>com.gs.manager.zookeeper.discovery.port</code><br /><code>com.gs.manager.zookeeper.leader-election.port</code></td>
<td>2888<br />3888</td>
</tr>

<tr>
<td>Lookup Service</td>
<td><code>com.gs.multicast.discoveryPort</code></td>
<td>4174</td>
</tr>
</tbody>
</table>

<div class="tc-admon-note">
  <p class="tc-admon-title">Note:</p>
  <p>Zookeeper requires that each manager can reach any other manager. If you are changing the Zookeeper ports, make sure you use the same port on all machines. If that is not possible for some reason, you may specify the ports via the <code>XAP_MANAGER_SERVERS</code> environment variable. For example:</p>

<pre xml:space="preserve"><code class="language-bash">XAP_MANAGER_SERVERS="alpha;zookeeper=2000:3000;lus=4242,bravo;zookeeper=2100:3100,charlie;zookeeper=2200:3200"
</code></pre>

<p>When using this syntax in unix/linux systems, make sure to wrap it in quotes (as shown), because of the semi-colons.</p>
</div>

<h2 id="zookeeper">Zookeeper</h2>

<p>ZooKeeper's behavior is governed by the ZooKeeper configuration file (<code>zoo.cfg</code>).
When using XAP manager, an embedded Zookeeper instance is started using a default configuration located at <code>$XAP_HOME/config/zookeeper/zoo.cfg</code>.
If you need to override the default settings, either edit the default file, or use the <code>XAP_ZOOKEEPER_SERVER_CONFIG_FILE</code> environment variable or the <code>com.gs.zookeeper.config-file</code> system property to point to your custom configuration file.
Default port of Zookeeper is 2181.</p>

<div class="tc-admon-refer">
  
  <p>Additional information on Zookeeper configuration can be found at <a href="https://zookeeper.apache.org/doc/r3.4.9/zookeeperAdmin.htm#sc_configuration" target="_blank">ZooKeeper configuration</a>.</p>
</div>

<h1 id="backwards-compatibility">Backwards Compatibility</h1>

<p>The Manager is offered side-by-side with the existing stack (GSM, LUS, etc.). We think this is a better way of working with XAP, and we want new users and customers to work solely with it.
On the same note we understand that it requires some effort from existing users which upgrade to 12.1 (probably not too much, mostly on changing the scripts they use to start the environment),
so if you’re upgrading for bug fixes/other features and don’t want the manager for now, you can switch from 12.0 to 12.1 and continue using the old components - it’s all still there.</p>

<div class="tc-admon-note">
  <p class="tc-admon-title">Note:</p>
  <p>The Manager uses a different selection strategy when selecting resources where to deploy a processing unit instance. The strategy is to choose the container with the least relative weight. This is achieved by calculating the relative weight of each container in regards to other containers. Prior to 12.1 the strategy was to calculate the weight of a container based on gathering remote state. In large deployments, the network overhead and the overall deployment time is costly. We can achieve almost the same behavior with the new strategy.</p>

<p>Notice that you may be experiencing a different instance distribution than before. Although in both strategies we take a "best-effort" approach, in some cases it may still be an uneven distribution due to simultaneous selection process.</p>

<p>To change between selector strategies, use the following system property (org.jini.rio.monitor.serviceResourceSelector). For example, to set the strategy to the on prior to 12.1, assign the following when loading the manager (in <code>XAP_MANAGER_OPTIONS</code> environment variable):</p>

<pre xml:space="preserve"><code class="language-bash">-Dorg.jini.rio.monitor.serviceResourceSelector=org.jini.rio.monitor.WeightedSelector
</code></pre>
</div>

<h1 id="faq">FAQ</h1>

<h3 id="q-why-do-i-need-3-managers-in-previous-versions-2-lus-2-gsm-was-enough-for-high-availability">Q. Why do I need 3 managers? In previous versions 2 LUS + 2 GSM was enough for high availability</h3>

<p>With an even number of managers, consistency cannot be assured in case of a network partitioning, hence the 3 managers requirement.</p>

<h3 id="q-i-want-higher-availability-can-i-use-5-managers-instead-of-3">Q. I want higher availability - can I use 5 managers instead of 3?</h3>

<p>Theoretically this is possible (e.g. Zookeeper supports this), but currently this is not supported in XAP - starting 5 managers would also start 5 Lookup Services, which will lead to excessive chatinnes and performance drop. This issue is in our backlog, though - if it's important for you please contact support or your sales rep to vote it up.</p>

<h3 id="q-can-i-use-a-load-balancer-in-front-of-the-rest-api">Q. Can I use a load balancer in front of the REST API?</h3>

<p>Sure. However, make sure to use sticky sessions, as some of the operations (e.g. upload/deploy) take time to resonate to the other managers.</p>

</body>
</html>