<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1>Data Lake Acceleration</h1>
        <p MadCap:conditions="Default.DoNotShow"><MadCap:variable name="General.ProductNameIE" /> only</p>
        <h1>Overview</h1>
        <p>Big data adoption in enterprises is increasing all the time, with no sign of stopping. The Apache Spark MLlib&#160;and&#160;Tensorflow&#160;are among the most-adopted big data analytics platforms, while Cloudera,&#160;Amazon EMR,&#160;Hortonworks, and&#160;MapR&#160;are popular big data distributions. Patching all these technologies together into the classic Lambda architecture presents a number of customer challenges. </p>
        <ul>
            <li>Increasing system complexity - traditional Lambda architectures simply keep adding components as new technologies come into play, which slows down queries and is difficult to maintain.</li>
            <li>Data freshness - New data may only be accessible to users once every X hours, which is too slow to make real-time decisions.</li>
            <li>One-way, immutable data flow - If data contains a lot of update operations, and big data platforms such as HDFS and Parquet don't support data updates, all ingestion jobs needed to create new snapshots.</li>
        </ul>
        <p>AnalyticsXtreme is a data lake accelerator that  operationalizes your data lake for real-time analytics, which can run simultaneously on both real-time, mutable streaming data and on historical data that is stored on data lakes based on Hadoop, Amazon S3 or Azure Blob Storage, as well as data warehouses such as Snowflake, without requiring a separate data load procedure or data duplication.  Moving from on-premise to the cloud, or changing technology stacks for example from Cloudera to Amazon S3, is seamless to machine learning applications; increasing flexibility while reducing development and maintenance. </p>
        <p>With AnalyticsXtreme, your data is available for immediate searching, queries, and running analytics; there is a single logical view for hot, warm and cold data. The hot data resides on <MadCap:variable name="General.ProductNameIE" />'s in-memory data grid, while cold (historical) data can be stored on any big-data platform such as HDFS or Amazon S3. Additionally, the hot data is mutable, supporting real-time updates. The data becomes immutable when it is stored on the external big data platform.</p>
        <p>&#160;</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/AnalyticsXtreme.png" class="tc-picture80" />
            </p>
        </div>
        <p>This approach enables fast access to frequently used historical data, and applications can access any data - hot, warm, or cold - via a unified layer using Spark SQL or JDBC. You can easily integrate BI&#160;tools such as <a href="tableau.html">Tableau</a>, Looker, and PowerBI.</p>
        <p> AnalyticsXtreme provides automatic life cycle management, handling the underlying data movement, optimization and deletion using an internal data life cycle policy.</p>
        <h1>Implementation</h1>
        <p>One of the main functions of AnalyticsXtreme is managing the movement of data from the speed layer (data grid) to the batch layer (external data storage) as it ages and becomes cold. But in order to handle this data transfer transparently, several things must to be taken into consideration. For example, the business application may trigger a query on the data at any point in time, and so needs to know where the data is located in order to successfully complete the query and return accurate results. The query may be complex, and therefore may take a relatively long time to complete. Additionally, there may be remote clients sending their queries, which means network latency needs to be taken into account. And finally, if the network connection isn't stable, the latency period may be even longer for some queries before they are finally received and executed.</p>
        <p>The data life cycle policy was designed to handle this movement of data from the speed layer to the batch layer in a safe and predictable way. For example, we may have a system where data that is up to 5 hours old is considered hot and should be held in the speed layer, while anything older is considered cold and therefore must be moved to the batch layer. This 5-hour interval is the <span class="tc-italic">speedPeriod </span>. The end of the <code>speedPeriod </code>is the query threshold; if a query is sent at 6 PM that requires data up to 5 hours old, the query threshold is 1 PM, and the query manager will look for the data in the speed layer. If the query needs data that is more than 5 hours old, the query manager will look for that data in the batch layer.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-1-speedperiod.png" class="tc-picture80" />
            </p>
        </div>
        <p>While the data is in the speed layer, it is dynamic and can be updated as necessary. When it is moved to the batch layer, it becomes immutable. As the data nears the end of the <code>speedPeriod</code>, the data life cycle policy has to prepare for moving it to the batch layer. Therefore, the policy includes a <span class="tc-italic">mutabilityPeriod</span>, during which time the data remains fully dynamic. When the data ages out of the <code>mutabilityPeriod</code>, it becomes immutable so that it is ready to be moved to the batch layer. By default, the <code>mutabilityPeriod </code>is set to 80% of the <code>speedPeriod</code>. If the <code>speedPeriod </code>is 5 hours, then the <code>mutabilityPeriod </code>is 4 hours, and data that is between 4-5 hours old is in an immutable window.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-2-mutabilityperiod.png" class="tc-picture80" />
            </p>
        </div>
        <p>In order to keep system performance consistent, and to ensure that the data can be easily verified when it is moved between layers, the data life cycle policy copies the data from the speed layer to the batch layer in small chunks as it nears the end of the immutable window, according to the <span class="tc-italic">batchFeedInterval</span>. At this point, the aging data exists both in the speed layer and in the batch layer.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-3-batchfeed.png" class="tc-picture80" />
            </p>
        </div>
        <p>After the aging data has been copied safely to the batch layer and the <code>speedPeriod </code>expires, the data needs to be evicted from the speed layer. However, since the query threshold is a sliding window, a small safety margin is needed to ensure that long-running queries can complete, and to account for network latency regarding remote clients that may have sent queries before the <code>speedPeriod </code>for that data expired. This safety margin is the <span class="tc-italic">evictionBuffer</span>, set by default to 10 minutes.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-4-eviction.png" class="tc-picture80" />
            </p>
        </div>
        <h1>Configuring the Data Life Cycle Policy</h1>
        <p>The data life cycle policy defines how data is archived from the data grid (speed layer) to the external data storage (batch layer). This policy is defined in the Space, with a global configuration that defines the polling interval and log level for the application itself, along with a life cycle policy for each data object, or table.</p>
        <h2>Global Properties</h2>
        <p>The global configuration contains a list of DataLifecyclePolicy properties, as well as the logging level that is set for the entire application:</p>
        <table style="width: 100%;" class="tc-standard">
            <col />
            <col />
            <col />
            <col />
            <col />
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Description</th>
                    <th>Unit</th>
                    <th>Default Value</th>
                    <th>Required/Optional</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>verbose </td>
                    <td>Increases the log levels for both the client and the server to provide verbose information (useful for troubleshooting).</td>
                    <td>True/False</td>
                    <td>False</td>
                    <td>Optional</td>
                </tr>
            </tbody>
        </table>
        <h2>Table (Object) Properties</h2>
        <p>The DataLifecyclePolicy table contains the following parameters.</p>
        <table style="width: 100%;" class="tc-standard">
            <col />
            <col />
            <col />
            <col />
            <col />
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Description</th>
                    <th>Unit</th>
                    <th>Default Value</th>
                    <th>Required/Optional</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>typeName *</td>
                    <td>Name of the table/type/class to which this policy applies.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>timeColumn** </td>
                    <td>Name of column/property/field that contains the time data used to manage this policy.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>timeFormat </td>
                    <td>Time format for the data in the timeColumn parameter.</td>
                    <td>&#160;</td>
                    <td>by java</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>speedPeriod </td>
                    <td>Time period or fixed timestamp.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>batchDataSource </td>
                    <td>endpoint for querying the batch layer</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>batchDataTarget </td>
                    <td>endpoint for feeding data to the batch layer</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>mutabilityPeriod </td>
                    <td>Period of time during which data can be updated.</td>
                    <td> Duration in minutes (m) or % of speedPeriod.</td>
                    <td>80%</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>batchFeedInterval </td>
                    <td>Data is fed from the Space (speed layer) to the batch layer in these time-based intervals at the end of the speedPeriod, after the mutabilityPeriod has expired.</td>
                    <td>Minutes (m)</td>
                    <td>1 m</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>batchFeedSize </td>
                    <td>Maximum data entries per batch feed interval.</td>
                    <td>&#160;</td>
                    <td>
                        <p>1000 </p>
                    </td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>evictionPollingInterval </td>
                    <td>Polling interval for querying and evicting each policy.</td>
                    <td>Minutes (m)</td>
                    <td>1 m</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>evictionBuffer </td>
                    <td>Additional waiting period before evicting data from the Space (speed layer)&#160;after it was fed to the batch layer, so that long queries and clock differences won't cause errors or generate exceptions. </td>
                    <td>Duration in minutes (m) or % of speedPeriod.</td>
                    <td>10 m</td>
                    <td>Optional</td>
                </tr>
            </tbody>
        </table>
        <p>* This correlates to an object or JSON in the object store.</p>
        <p>** This correlates to a property or entity in an object or JSON.</p>
        <p MadCap:conditions="Default.DoNotShow">You can configure the following properties for the data life cycle policy:</p>
        <ul>
            <li MadCap:conditions="Default.DoNotShow">Storage/FS:  HDFS/S3 (for batch layer)</li>
            <li MadCap:conditions="Default.DoNotShow">Date &amp; Time: last N h</li>
            <li MadCap:conditions="Default.DoNotShow">Partition: by hour</li>
            <li MadCap:conditions="Default.DoNotShow">Primary: by ID (key)</li>
            <li MadCap:conditions="Default.DoNotShow">Size: up to NNN GB</li>
            <li MadCap:conditions="Default.DoNotShow">Deletion: Every N hours</li>
        </ul>
        <h2>Supported Data Formats</h2>
        <p>AnalyticsXtreme supports all the data formats that are supported by Apache Spark, such as Apache Parquet and Apache Avro.</p>
        <h1>Using the <MadCap:variable name="General.ProductNameIE" /> JDBC&#160;Driver</h1>
        <p MadCap:conditions="Default.DoNotShow">(Get from Niv)</p>
        <p>TBD</p>
        <h2>Supported Query Languages</h2>
        <ul>
            <li>Spark</li>
            <li>SQL</li>
            <li>JDBC</li>
        </ul>
        <h1>Limitations</h1>
        <p MadCap:conditions="Default.DoNotShow">Gaps: Join (Ayelet)</p>
        <p>TBD</p>
    </body>
</html>