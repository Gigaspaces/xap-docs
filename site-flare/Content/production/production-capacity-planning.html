<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
		<div class="product-bar">
			<p><a>Smart Cache</a>
			</p>
		</div>
        <h1 class="tc-pagetitle">Capacity Planning</h1>
        <p>Capacity planning is calculating the number of machines your application requires. This is one of the most important tasks you must complete before moving into production and making your application available to your customers/users. Typically, it should be done during the early stages of your project, in order to budget for the hardware and relevant software products your system will use. At a minimum, capacity planning involves estimating the number of CPUs and cores plus the memory each machine must have.</p>
        <p>Another important deployment decision is to calculate the maximum number of data grid partitions the application requires. This deployment parameter determines the scalability of your application data grid. When the application is deployed, the number of data grid partitions remains constant, but their physical location, their activation mode (primary or backup), and hosting containers are dynamic.</p>
        <p>The data grid instances can fail and relocate themselves to another machine automatically. They can relocate as a result of an SLA (Service Level Agreement) event (for example - CPU and memory utilization breach), or they can relocate based on manual intervention by an administrator. This data grid "mobility" is a critical capability that, beyond providing the application with the ability to scale dynamically and self-heal itself, can prevent over-provisioning and unnecessary overbudgeting of your hardware. You can start small, and grow as needed with your hardware utilization. Your capacity planning should take this fact into consideration.</p>
        <p>To avoid over provisioning, you should start small, and expand your data grid capacity when needed. The maximum number of data grid partitions can be calculated, based on a simple estimation of the number of machines you have available, or based on the size and quantity of objects your application generates. This allows your application to scale while remaining resilient and robust.</p>
        <p>This topic addresses the following capacity planning issues:</p>
        <ol>
            <li>
                <p>How do I calculate the footprint of objects when stored within the data grid?</p>
            </li>
            <li>
                <p>What is the balance between the amount of active clients, machine cores, and the data grid process heap size?</p>
            </li>
            <li>
                <p>How should the number of data grid partitions be calculated?</p>
            </li>
        </ol>
        <div class="tc-admon-note">
            <p>It is not necessary to specify the maximum number of partitions for services that are not co-located with data grid instances. These can be scaled dynamically without having to specify maximum instances.</p>
        </div>
        <div MadCap:conditions="Version.15-5-born">
            <h1>Capacity Planning Using Ops Manager </h1>
            <p>Using the Ops manage in the testing environment, you can plan capacity according to the expected number of objects per each type.</p>
            <ol>
                <li>
                    <p>Trigger an Object Analysis by pressing the Run Objects Analysis icon on the Ops Manager Space view screen (the circle Icon on the right):</p>
                    <p>
                        <img src="../Resources/Static/images/capacity-planning-using-ops-manager-1.png" style="width: 1022px;height: 590px;" />
                    </p>
                </li>
            </ol>
            <div class="tc-admon-note">
                <p>There will be no option to generate a report if the space has no backup.</p>
            </div>
            <p>When the analysis is complete, a message will appear as follows:</p>
            <p>
                <img src="../Resources/Static/images/capacity-planning-using-ops-manager-2.png" style="width: 703px;height: 72px;" />
            </p>
            <p>Press <b>Show object analysis report</b> to view the memory usage report.</p>
            <p>Alternatively, you can click the icon next to the Report icon to see the report.</p>
            <p>When generating a report we will take a heap dump of the relevant backup and will query it, based on that we will conclude avg property footprint and index footprint. If a previous report exists we will compare the current report with it and show the changes.</p>
            <p>This is mainly intended at development and testing stages to optimize data structure and  capacity planning.</p>
            <p>but can also run by admin once in a while to see that memory consumption behaves as expected.</p>
            <p>The reports are kept as JSON at :GS_HOME/config/ui/heapReports</p>
            <p>There will be no option to generate reports in case there is no backup.</p>
            <p>You can see per type how much each property consume on average:</p>
            <p>And in a type definition at the index tab you can also see index efficiency, estimation of its memory consumption vs its usage:</p>
            <p>(In the properties tab you will see estimate ram per each property)</p>
            <p>After getting estimations for how much memory will be consumed in the system by each type and the expected number of objects per each, You should decide on the proper number of partitions, and if you should use memoryXtend in order to reduce the RAM usage</p>
            <p>see: MemoryXtend</p>
            <p>In addition, You can consider reducing object footprints by storing some of the properties in binary or compressed modes.</p>
            <p>Starting in version 15.8 we offer OOTB implementations for reducing memory footprint with minimal effect on performance.</p>
            <p>Some consideration regarding the number of partitions:</p>
            <p>1. How many cores are needed per processing and doing the calculations on X data</p>
            <p>2. How fast should be the initial load from the data source?</p>
            <p>3. How fast should query, or tasks return?</p>
            <p>The more partitions you have, the More parallelism is possible, and you can divide the work into more process</p>
            <p>The larger the heap of one partition - the more GC tuning will be required.</p>
            <p>The partition number is a balance between CPU and memory requirement</p>
            <p>And routing properties should be chosen in a way memory consumption is evenly balanced between the partitions</p>
            <p>We added data distribution per partition graph to help visualize it.</p>
            <p>&#160;</p>
        </div>
        <h1><a name="object-footprint-within-the-imdg"></a>Object Footprint Within the Data Grid</h1>
        <p>The object footprint within the data grid is determined based on the following:</p>
        <ul>
            <li>
                <p>The original object size - the number of object fields and their content size.</p>
            </li>
            <li>
                <p>The JVM type (32- or 64-bit) - a 64-bit JVM may consume more memory due to the pointer address size.</p>
            </li>
            <li>
                <p>The number of indexed fields - every indexed value means another copy of the value within the index list.</p>
            </li>
            <li>
                <p>The number of different indexed values - more different values means a different index list.</p>
            </li>
            <li>
                <p>The object UID size - the UID is a string-based field, and consumes about 35 characters. You may have the object UID based on your own unique ID.</p>
            </li>
        </ul>
        <h2><a name="calculating-the-object-footprint"></a>Calculating the Object Footprint</h2>
        <p>The best way to determine the exact object footprint is via a simple test that allows you to perform some extrapolation when running the application in production.</p>
        <p>To calculate the object footprint:</p>
        <ol>
            <li>
                <p>Start a single data grid instance.</p>
            </li>
            <li>
                <p>Take a measurement of the free memory.</p>
            </li>
            <li>
                <p>Write a sample number of objects to the data grid (~100,000 is a good number).</p>
            </li>
            <li>
                <p>Measure the free memory again.</p>
            </li>
        </ol>
        <p>This test provides a very good understanding of the object footprint within the data grid. Bear in mind that if you have a backup running, you need double the amount of memory  to accommodate your objects.</p>
        <p>The following sample calculation of an object footprint uses a 32- and 64-Bit JVM with different amounts of indexes. The numbers below are for example only, and real results may vary based on the actual Object data and index values.</p>
        <p>The test was set up as follows:</p>
        <ul>
            <li>
                <p>Version XAP 7.1.2.</p>
            </li>
            <li>
                <p>All objects values are different.</p>
            </li>
            <li>
                <p>The Object has one String field, one Integer field, one Long field and one Double field.</p>
            </li>
            <li>
                <p>Footprint measured in Bytes.</p>
            </li>
            <li>
                <p>Basic Index type is used. An Extended Index type will have an additional footprint (20%) compared to a Basic Index type.</p>
            </li>
        </ul>
        <table>
            <thead>
                <tr>
                    <th align="left">Indexes</th>
                    <th align="left">Footprint 32-Bit JVM - Windows</th>
                    <th align="left">Footprint 64-Bit JVM - Linux</th>
                    <th align="left">Footprint 64-Bit JVM - UseCompressedOops - Linux</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td align="left">0</td>
                    <td align="left">331</td>
                    <td align="left">306</td>
                    <td align="left">308</td>
                </tr>
                <tr>
                    <td align="left">1 - String</td>
                    <td align="left">456</td>
                    <td align="left">705</td>
                    <td align="left">493</td>
                </tr>
                <tr>
                    <td align="left">2 - String+Integer</td>
                    <td align="left">493</td>
                    <td align="left">989</td>
                    <td align="left">671</td>
                </tr>
                <tr>
                    <td align="left">3 - String+Integer+Long</td>
                    <td align="left">533</td>
                    <td align="left">1002</td>
                    <td align="left">680</td>
                </tr>
                <tr>
                    <td align="left">4 - String+Integer+Long+Double</td>
                    <td align="left">571</td>
                    <td align="left">1026</td>
                    <td align="left">691</td>
                </tr>
            </tbody>
        </table>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/sbp/footprint_bench7.1.2.jpg" alt="footprint_bench7.1.2.jpg" class="tc-picture100" />
            </p>
        </div>
        <div class="tc-admon-tip">
            <ul>
                <li>
                    <p>You can decrease the raw object footprint (not the index footprint) using the <a href="/sbp/lowering-the-space-object-footprint.html">GigaSpaces Serialization API</a>.</p>
                </li>
                <li>
                    <p>You can reduce the JVM memory footprint using the <code>-XX:+UseCompressedOops</code> JVM option. It is part of the JDK6u14 and JDK7. See more details <a href="https://wikis.oracle.com/display/HotSpotInternals/CompressedOops" target="_blank">here</a>. It is highly recommended to use the latest JDK release when using this option.</p>
                </li>
            </ul>
        </div>
        <h1><a name="active-clients-vs-cores-vs-heap-size"></a>Active Clients vs. Cores vs. Heap Size</h1>
        <p>The data grid kernel is a highly multi-threaded process, and therefore has a relatively large number of active threads handling incoming requests. These requests can come from remote clients or co-located clients, such as:</p>
        <ul>
            <li>
                <p>Any remote call involves a thread on the data grid side that handles the request.</p>
            </li>
            <li>
                <p>A notification delivery may involve multiple threads sending events to registered clients.</p>
            </li>
            <li>
                <p>Any destructive operation (write, update, take) also triggers a replication event that is handled via a dedicated replication channel, which uses a dedicated thread to handle the replication request to the backup data grid instance.</p>
            </li>
            <li>
                <p>There is periodic background activity, used to monitor the relevant components that are using its own dedicated threads within the data grid kernel.</p>
            </li>
        </ul>
        <p>A co-located client does not go through the network layer, and interacts with the data grid kernel very fast. This means that the machine CPU core that has been assigned to deal with this thread activity is very busy, and does not wait for the operating system to handle IO operations. Taking this fact into consideration means we can have less concurrent clients served by the same core when compared to remote client activity.</p>
        <p>The number of active threads and machine cores is an important consideration when calculating the maximum heap size to be allocated for the JVM running the GSC. You should keep memory in reserve for the JVM garbage collection activity, to deal with cleaning allocated resources and reclaiming unused memory. A large heap size means a potentially large number of temporary objects that can generate work for the garbage collection activity. You should have a reasonable balance between the number of cores the machine is running, the number of GSCs/data grids that are running, and the number of active clients/threads accessing the data grid.</p>
        <p>A machine running 4 quad-core cores with fast CPUs (3GHz clock) can handle 20-30 concurrent collocated clients and 100-150 concurrent remote clients without any special delay. This JVM configuration should have at least a 2-3GB heap size to handle the data and additional resources that utilize the memory. With the above, we assume the application business logic is very simple and does not have any IO operations, and the data grid persistency mode is asynchronous.</p>
        <h1><a name="calculating-the-number-of-imdg-partitions"></a>Calculating the Number of Data Grid Partitions</h1>
        <p MadCap:conditions="Version.15-0-died">Calculating the number of data grid partitions required by an application is essentially on the maximum number of machines available. Theoretically, in an ideal world where you have unlimited budget and resources, you should have a dedicated machine per data grid instance hosted within a dedicated Grid Service Container (GSC). In the real world, in order to avoid a large hardware budget, you can initially have multiple data grid instances running on the same machine within a single GSC. This deployment topology can be modified later to move closer to the ideal of a dedicated GSC hosting a single data grid instance (one data grid instance per GSC).</p>
        <p MadCap:conditions="Version.15-0-born">Calculating the number of data grid partitions required by an application is essentially on the maximum number of machines available. You should have a dedicated machine per data grid instance hosted within a dedicated Grid Service Container (GSC).</p>
        <p>The initial required number of GSCs per machine is calculated based on the machine's physical RAM, and the amount of heap memory you want to allocate for the JVM running the GSC. In many cases, the heap size is determined based on the operating system: for a 32-bit OS, you can allocate a 2GB maximum heap size, and for a 64-bit OS, you need 6-10GB maximum heap size (the JVM -Xmx argument). For performance optimization, you should have the initial heap size the same as the maximum size. The sections below demonstrate capacity planning using a simple, real-life example.</p>
        <p>Here are a few basic formulas you can use:</p><pre><code class="language-java">Amount of GSCs per Machine = Amount of Total Machine Cores/2
</code></pre><pre><code class="language-java">Total Amount of GSC = Amount of GSCs per Machine X Initial amount of Machines
</code></pre><pre><code class="language-java">GSC max heap Size = min (6, (Machine RAM Size * 0.8) / Amount of GSCs per Machine))
</code></pre><pre><code class="language-java">Amount of Data-Grid Partitions = Total Amount of GSC X Scaling Growth Rate / 2
</code></pre>
        <p>Where:</p>
        <ul>
            <li>
                <p>Number of Total Machine Cores - total number of cores the machine is running. For a quad-core with 2 CPUs (Duo) machine this value is 8.</p>
            </li>
            <li>
                <p>Number of Data Grid Partitions - number of data grid partitions you need to set when deploying.</p>
            </li>
            <li>
                <p>GSC max heap Size - JVM Xmx value</p>
            </li>
            <li>
                <p>Number of GSCs per machine - number of GSCs you run per machine. This is a GSA parameter.</p>
            </li>
            <li>
                <p>Total amount of data in memory - number that should be estimated based on the object footprint you are storing within the space.</p>
            </li>
            <li>
                <p>Scaling Growth Rate - expansion ratio,  usually between 2-10. This value determines how much to expand the data grid capacity without downtime.</p>
            </li>
            <li>
                <p>Initial number of machines - initial available machines to have when deploying the data grid.</p>
            </li>
            <li>
                <p>Machine RAM Size - amount of physical RAM a machine has.</p>
            </li>
            <li>
                <p>Total number of GSCs - total number of GSCs that are initially running when deploying the data grid.</p>
            </li>
        </ul>
        <h2><a name="example-1-basic-capacity-planning"></a>Example 1 - Basic Capacity Planning</h2>
        <p>In this example, there are initially 2 machines used to run the data grid application. 10 machines may be allocated for the project within the next 12 months. Each machine has 32GB of RAM with 4 quad-core CPUs. This provides a total of 64GB of RAM. Later, when all 10 machines are available, there will be a potential 320GB of total memory. The memory is used both by the primary data grid and the backup data grid instances (exact replica of the primary machines).</p>
        <div class="tc-admon-note">
            <p>The number of backups per partition is zero or one.</p>
        </div>
        <p>The machines are running a Linux 64-bit operating system. Allocating 6GB per JVM as the maximum heap size for the GSC results in 5 GSCs per machine; 10 GSCs initially across 2 machines. When all of the 10 machines are in use, there will be 50 GSCs.</p>
        <div class="tc-align-center" MadCap:conditions="Version.15-0-died">
            <p>
                <img src="../Resources/Static/attachment_files/sbp/capacity_planning1.jpg" alt="capacity_planning1.jpg" class="tc-picture80" />
            </p>
        </div>
        <p MadCap:conditions="Version.15-0-died">Figure 1: 2-Machine Topology, 5 data grid Instances per GSC, total 64GB RAM</p>
        <p>When a maximum of 40 GSCs are hosting the data grid, it is a good idea to have half of them (20 GSCs) running primary data grid instances and the other half running backup instances. Use this number to define the number of partitions the data grid is deployed with<MadCap:conditionalText MadCap:conditions="Version.15-0-died">; with the 2 machines there are initially 10 GSCs hosting 40 data grid instances. Later, as needed, these data grid instances will be relocated to new GSCs that are started on new machine</MadCap:conditionalText><MadCap:conditionalText MadCap:conditions="Version.15-0-died">s</MadCap:conditionalText>. Each machine will start 4 GSCs, which will join the GigaSpaces grid and allow the administrator to manually or automatically expand the capacity of the data grid during runtime.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/sbp/capacity_planning2.jpg" alt="capacity_planning2.jpg" class="tc-picture80" />
            </p>
        </div>
        <p>Figure <MadCap:conditionalText MadCap:conditions="Version.15-0-born">1</MadCap:conditionalText><MadCap:conditionalText MadCap:conditions="Version.15-0-died">2</MadCap:conditionalText>: 10-Machine Topology. - 1 data grid Instance per GSC, total 320GB RAM</p>
        <p>This rebalancing of the data grid instances can be done via the UI, or via a simple program using the Admin API.</p>
        <p>Being able to determine the number of data grid partitions prior to the application deployment allows you to have exact knowledge of how your routing field values should be distributed, and how your data will be partitioned across the different JVMs that will be hosting your data in memory.</p>
        <h2><a name="example-2-capacity-planning-when-running-on-the-cloud"></a>Example 2 - Capacity Planning when Running on the Cloud</h2>
        <p>In a cloud environment, you have access to essentially unlimited resources. You can spin up new virtual machines and have practically unlimited amounts of memory and CPU power. In this type of environment, calculating the maximum number of data grid partitions is not based on the maximum number of machines you might have allocated for your project because theoretically, you can have an unlimited number of machines started on the cloud to host your data grid. Still, you must have some value for the maximum number of data grid partitions when deploying your application. In this case, calculate the number of data grid partitions based on the amount of memory your application might generate and store within the data grid.</p>
        <p>For example, if you have an application using 3 types of classes to store its data within the data grid:</p>
        <ul>
            <li>
                <p>Class A - object average size is 1KB</p>
            </li>
            <li>
                <p>Class B - object average size is 10KB</p>
            </li>
            <li>
                <p>Class C - object average size is 100KB</p>
            </li>
        </ul>
        <p>The application needs to generate 1 million objects for each type of class during its life cycle:</p>
        <ul>
            <li>
                <p>Class A - total memory needed = 1KB X 1M = 1GB</p>
            </li>
            <li>
                <p>Class B - total memory needed = 10KB X 1M = 10GB</p>
            </li>
            <li>
                <p>Class C - total memory needed = 100KB X 1M = 100GB</p>
            </li>
        </ul>
        <p>The total memory required to store the application data in memory = 111GB</p>
        <p>When using machines with 32GB of RAM, 4 machines are needed to run enough primary data grid instances to store 111GB of data in memory, and another 4 machines are needed for the backup data grid instances. For a 64-bit operating system, the numbers are 5 GSCs, each having a 6GB maximum heap size for a total of 40 GSCs (5 X 4 X 2). <MadCap:conditionalText MadCap:conditions="Version.15-0-died">With 20 GSCs used to run primary instances, it is reasonable to target 80 as the potential final number of partitions needed (each GSC will initially host 4 data grid instances). This means that we have 160 data grid instances (half primary and half backups) hosted within 40 GSCs. Theoretically, t</MadCap:conditionalText><MadCap:conditionalText MadCap:conditions="Version.15-0-born">T</MadCap:conditionalText>his allows us to expand the data grid to run across 160 machines (one GSC per machine). This means 160 X 10GB as the heap size = 1.6TB of data grid memory capacity to host the data grid objects.  This is a huge amount of capacity for the data grid, and actually 10 times larger than the estimated size. It provides a lot of room for error in case our initial memory utilization was wrong.</p>
        <h1><a name="used-memory-utility"></a>Used Memory Utility</h1>
        <p>Checking the used memory on all primary instances can be done using the following (we assume we have one GSC per primary instance):</p><pre><code class="language-java">GigaSpace gigaSpace = new GigaSpaceConfigurer(new UrlSpaceConfigurer(spaceURL)).gigaSpace();
Future&lt;Long&gt; taskresult = gigaSpace.execute(new FreeMemoryTask());
long usedMem = taskresult.get();
System.out.println("Used Mem[MB] " + (double)usedMem/(1024*1024));
</code></pre>
        <p>The <code>FreeMemoryTask</code> implementation:</p><pre><code class="language-java">import java.util.Iterator;
import java.util.List;

import org.openspaces.core.executor.DistributedTask;

import com.gigaspaces.annotation.pojo.SpaceRouting;
import com.gigaspaces.async.AsyncResult;

public class FreeMemoryTask implements DistributedTask&lt;Long, Long&gt;{

    Integer routing;
    public Long execute() throws Exception {
        
        Runtime rt = Runtime.getRuntime();
        System.out.println("Calling GC...");
        rt.gc();
        Thread.sleep(5000);
        System.out.println("Done GC..." + 
                " Used memory " + (rt.totalMemory() - rt.freeMemory() )+
                " Free Memory " + rt.freeMemory() + 
                " MaxMemory " + rt.maxMemory() + 
                " Committed memory "+ rt.totalMemory());
        
        return (rt.totalMemory() - rt.freeMemory() );
    }

    @Override
    public Long reduce(List&lt;AsyncResult&lt;Long&gt;&gt; _usedMemList) throws Exception {
        
        long totalUsed =0;
        Iterator&lt;AsyncResult&lt;Long&gt;&gt; usedMemList = _usedMemList.iterator();
        while (usedMemList.hasNext())
        {
            totalUsed  = totalUsed + usedMemList.next().getResult();
        }
        return totalUsed ;
    }

    @SpaceRouting
    public Integer getRouting() {
        return routing;
    }

    public void setRouting(Integer routing) {
        this.routing = routing;
    }
}

</code></pre>
    </body>
</html>