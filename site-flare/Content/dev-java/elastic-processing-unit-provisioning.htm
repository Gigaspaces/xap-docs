<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
<head></head>
<body>
<h1>Machine Provisioning</h1>
  

<h1><a name="system-bootstrapping">&#160;</a>System Bootstrapping</h1>

<p>Each machine requires a single running <a href="../overview/the-runtime-environment.htm#gsa">GigaSpaces Agent</a>. The example below shows how to start a new XAP agent. The command line parameters instruct the agents to communicate with each other and start the specified amount of managers. It does not start any containers automatically. The EPU starts containers on demand.</p>

<p>That means that potentially any machine could be a management machine:</p>

<div class="easyui-tabs" plain="true" data-options=""><div title="Windows" style="padding:10px"><pre><code class="language-bash">rem Agent deployment that potentially can start management processes
set XAP_LOOKUP_GROUPS=myGroup
set XAP_HOME=d:\gigaspaces
start cmd /c "%XAP_HOME%\bin\gs-agent.bat --global.esm=1 --global.gsm=2 --global.lus=2"
</code></pre>
</div>

<div title="Linux" style="padding:10px"><pre><code class="language-bash">1. Agent deployment that potentially can start management processes
export XAP_LOOKUP_GROUPS=myGroup
export XAP_HOME=~/gigaspaces
nohup ${XAP_HOME}/bin/gs-agent.sh --global.esm=1 --global.gsm=2 --global.lus=2 &gt; /dev/null 2&gt;&amp;1 &amp;
</code></pre>
</div>
</div>

<div class="tc-admon-note">
  
  <p>Basic assumption: When using the ESM component, each machine must have exactly one Grid Service Agent</p>
</div>

<h1><a name="dedicated-management-machines">&#160;</a>Dedicated Management Machines</h1>

<p>In case you prefer having dedicated management machines, start GigaSpaces agents with the above settings on two machines, and start the rest of the GigaSpaces agents with the settings below. The command line parameters instruct the GigaSpaces agents not to start managers. It does not start any containers automatically. The EPU starts containers on demand:</p>

<div class="easyui-tabs" plain="true" data-options=""><div title="Windows" style="padding:10px"><pre><code class="language-bash">rem Agent that does not start management processes
set XAP_LOOKUP_GROUPS=myGroup
set XAP_HOME=d:\gigaspaces
start cmd /c "%XAP_HOME%\bin\gs-agent.bat "
</code></pre>
</div>

<div title="Linux" style="padding:10px"><pre><code class="language-bash">1. Agent that does not start management processes
export XAP_LOOKUP_GROUPS=myGroup
export XAP_HOME=~/gigaspaces
nohup ${XAP_HOME}/bin/gs-agent.sh &gt; /dev/null 2&gt;&amp;1 &amp;
</code></pre>
</div>
</div>

<p>Configure the EPU scale config to use <code>dedicatedManagementMachines</code>, and reduce the <code>reservedMemoryCapacityPerMachine</code>. For more information consult the <a href="[%=Links.ApiJavaDoc%]/org/openspaces/admin/pu/elastic/config/DiscoveredMachineProvisioningConfigurer.htm">discovered machine provisioning configuration JavaDoc</a>.</p>

<h1><a name="zone-based-machine-provisioning">&#160;</a>Zone Based Machine Provisioning</h1>

<p>The EPU can be deployed into specific zone. This allows you to determine the specific locations of the EPU instances. You may have multiple EPU deployed, each into a difference zone. To specify the agent zone you should set the zone name before starting the agent - here is an example how to set the agent zone to <span class="tc-bold">ZoneX</span>:</p>

<pre><code class="language-bash">export XAP_GSA_OPTIONS="-Dcom.gs.zones=zoneX ${XAP_GSA_OPTIONS}"
gs-agent.sh --global.lus=1 --global.gsm=1 --global.esm=1
</code></pre>

<p>When deploying the EPU you should specify the zone you would like the EPU will be deployed into:</p>

<pre><code class="language-java">ProcessingUnit pu1 = gsm.deploy(
        new ElasticSpaceDeployment("mySpace")
        .maxMemoryCapacity(512, MemoryUnit.GIGABYTES)
        .memoryCapacityPerContainer(10, MemoryUnit.GIGABYTES)
        // discover only agents with "zoneX"
        .dedicatedMachineProvisioning(new DiscoveredMachineProvisioningConfigurer()
            .addGridServiceAgentZone("zoneX")
            .removeGridServiceAgentsWithoutZone()
            .create())
            .scale(new ManualCapacityScaleConfigurer()
                .memoryCapacity(256,MemoryUnit.GIGABYTES)
                .create())
</code></pre>

<p>With the above the <code>mySpace</code> EPU will be deployed only into agents associated with <span class="tc-bold">zoneX</span> where other agents without a zone specified will be ignored.</p>

<h1><a name="automatic-machine-provisioning">&#160;</a>Automatic Machine Provisioning</h1>

<p>When deploying an EPU pass an instance of <a href="[%=Links.ApiJavaDoc%]/org/openspaces/admin/pu/elastic/ElasticMachineProvisioningConfig.htm">ElasticMachineProvisioningConfig</a> as the <a href="[%=Links.ApiJavaDoc%]/org/openspaces/admin/pu/elastic/topology/ElasticDeploymentTopology.htm">machineProvisioning</a> deployment property.</p>

<pre><code class="language-java">ProcessingUnit pu = gsm.deploy(
        new ElasticStatefulProcessingUnitDeployment(new File("myPU.jar"))
           .memoryCapacityPerContainer(16,MemoryUnit.GIGABYTES)
           .maxMemoryCapacity(512,MemoryUnit.GIGABYTES)
           .maxNumberOfCpuCores(32)
           //automatically start new virtual machines on demand
           .dedicatedMachineProvisioning(new XenServerMachineProvisioning("xenserver.properties"))
);
</code></pre>

<p>When deploying Gigaspaces XAP on the management machine(s) place the plug-in JAR file under <code>/gigaspaces-xap/lib/platform/esm</code> folder. The ESM then loads classes specified by the <code>machineProvisioning</code> configuration. These classes need to implement either <a href="[%=Links.ApiJavaDoc%]/org/openspaces/grid/gsm/machines/plugins/ElasticMachineProvisioning.htm">ElasticMachineProvisioning</a> or <a href="[%=Links.ApiJavaDoc%]/org/openspaces/grid/gsm/machines/plugins/NonBlockingElasticMachineProvisioning.htm">NonBlockingElasticMachineProvisioning</a>. That class must also implement <a href="[%=Links.ApiJavaDoc%]/org/openspaces/core/bean/Bean.htm">Bean</a> which has resemblance to the Spring Bean.</p>

<h1><a name="automatic-rebalancing">&#160;</a>Automatic Rebalancing</h1>

<p>Each stateful processing unit (embedding a space) has a fixed number of logical partitions. The number of logical partitions can be specified by the user or calculated by GigaSpaces using the memory capacity requirements during the deployment time. Each logical partition has by default two instances - a primary and a backup. Instances of an EPU are automatically relocated between containers until the following criteria is met:</p>

<ul>
<li>Number of instances per container is nearly the same across all containers.</li>
<li>Number primary instances per cores is nearly the same across all machines.</li>
</ul>

<div class="tc-align-center"><p><img src="../Resources/Static/attachment_files/rebalance_util.jpg" alt="rebalance_util.jpg" /></p>
</div>

<h2><a name="how-rebalancing-works">&#160;</a>How Rebalancing works</h2>

<p>GigaSpaces runtime environment differentiate between a Container (GSC) or <span class="tc-bold">Grid node</span> that is running within a single JVM instance and an <span class="tc-bold">IMDG node</span>, also called a logical partition. A partition has one primary instance and zero or more backup instances.</p>

<p>A grid node hosts zero or more logical partitions, these may be primary or backup instances belong to different logical partitions. Logical partitions (primary or backup instances) may relocate between Grid nodes during runtime. A deployed stateful PU may expand its capacity or shrink its capacity in real-time by adding or removing Grid nodes and relocating existing logical partitions to the newly started Grid nodes.</p>

<p>If the system selects to relocate a primary instance, it will first switch its activity mode into a backup mode and the existing backup instance of the same logical partition will be switched into a primary mode. Once the new backup will be relocated, it will recover its data from the existing primary. This how the PU expands its capacity without disruption to the client application.</p>

<p>GigaSpaces rebalancing controls which logical partition is moving, where it is moving to and whether to move a primary to a backup instance. Without such control, the system might choose to move partitions into containers that are fully consumed or move too many instances into the same container. This will obviously crash the system.</p>

<h1><a name="adaptive-sla">&#160;</a>Adaptive SLA</h1>

<p>GigaSpaces adjust the high-availability SLA dynamically to cope with the current system memory resources. This means that if there is not sufficient memory capacity to instantiate all the backup instances, GigaSpaces will relax the SLA in runtime to allow the system to continue and run. Once the system identifies that there are enough resources to accommodate all the backups, it will start the missing backups automatically.</p>

<h2><a name="re-balancing-automatic-process-considerations">&#160;</a>Re-balancing Automatic Process Considerations</h2>

<ul>
<li>It assumes that all partitions are equal (in terms of memory and CPU consumption).</li>
<li>When manual capacity scale is used, the specified memory capacity and CPU cores must be provisioned before the re-balancing takes place. As long as the available machines do not meet the specified memory capacity or CPU cores re-balancing does not take place.</li>
<li>The maximum number of CPU cores that can be occupied by the Processing Unit primary instances is:</li>
</ul>

<pre><code class="language-java">numberOfPartitions X numberOfCpuCoresPerMachine
</code></pre>

<ul>
<li>The maximum amount of memory that can be used by the primary and backup instances is:</li>
</ul>

<pre><code class="language-java">numberOfPartitions X ( 1 + numberOfBackupsPerPartition ) X memoryCapacityPerContainer
</code></pre>

<ul>
<li>During relocation of a specific instance, primary election takes place. For a few seconds, operations on that partition and operations on the whole cluster is denied. Internally, the client proxy retries the operation until the primary election takes place and masks the failure, but the delay exists.
This delay can be reduced by modifying configuration settings as explained in <a href="../admin/troubleshooting-failure-detection.htm">Failure Detection</a>. Overriding the default value of these context properties is achieved with the <a href="[%=Links.ApiJavaDoc%]/org/openspaces/admin/pu/elastic/topology/ElasticDeploymentTopology.htm">addContextProperty</a> deployment property. For example:</li>
</ul>

<pre><code class="language-java">ProcessingUnit pu = gsm.deploy(
 new ElasticStatefulProcessingUnitDeployment(new File("myPU.jar"))
  .memoryCapacityPerContainer(16,MemoryUnit.GIGABYTES)
  .maxMemoryCapacity(512,MemoryUnit.GIGABYTES)
  .maxNumberOfCpuCores(32)
  .addContextProperty("cluster-config.groups.group.fail-over-policy.active-election.yield-time","300")
);
</code></pre>

<h1><a name="shared-machine-provisioning">&#160;</a>Shared Machine Provisioning</h1>

<p>In the code examples above, we showed the usage of <code>.dedicatedMachineProvisioning</code>. "Dedicated' machine provisioning means that the whole machine is "reserved' for instances of a single processing unit. In other words, other processing units are denied to co-exist on the same machine. This policy is useful if one processing unit should not be affected by other resource consuming processes.</p>

<p>For sharing a machine between processing units, you would need to specify a <span class="tc-bold"><span class="tc-italic">sharing ID</span></span> - a simple string identifying your sharing policy. When a processing unit is requested to scale-out, a new machine is marked with the <span class="tc-italic">sharing ID</span> of the processing unit. This ensures that no other processing unit will race to occupy the machine. This <span class="tc-italic">sharing ID</span> is later used to match against other processing unit's <span class="tc-italic">sharing ID</span> - to allow or deny sharing the same machine resource.</p>

<p>To simulate a "public' sharing policy, use the same <span class="tc-italic">sharing ID</span> for all deployments. For example <code>.sharedMachineProvisioning("public")</code>.</p>

<p>The following example, shows two elastic stateless processing units that may share each others machine resources.</p>

<pre><code class="language-java">// Deploy the Elastic Stateless Processing Unit on "site1"
ProcessingUnit puA = gsm.deploy(
    new ElasticStatelessProcessingUnitDeployment("servlet.war")
           .name("servlet-A")
           .memoryCapacityPerContainer(4,MemoryUnit.GIGABYTES)
           //initial scale
           .scale(
                new ManualCapacityScaleConfigurer()
            .memoryCapacity(1,MemoryUnit.GIGABYTES)
            .create())
           .sharedMachineProvisioning("site1", machineProvisioningConfig)
);

// Deploy the Elastic Stateless Processing Unit on "site1"
ProcessingUnit puB = gsm.deploy(
    new ElasticStatelessProcessingUnitDeployment("servlet.war")
           .name("servlet-B")
           .memoryCapacityPerContainer(4,MemoryUnit.GIGABYTES)
           //initial scale
           .scale(
                new ManualCapacityScaleConfigurer()
            .memoryCapacity(1,MemoryUnit.GIGABYTES)
            .create())
           .sharedMachineProvisioning("site1", machineProvisioningConfig)
);
</code></pre>

</body>
</html>