<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1 class="tc-pagetitle">Integrating Kafka with <MadCap:variable name="General.CompanyName" /> Products</h1>
        <p>This topic describes how to implement the <MadCap:variable name="General.ProductNameXAP" /> connector for Apache Kafka, which has been verified by Confluent and is available as an add-on component in the <MadCap:variable name="General.ProductNameXAP" /> software package. Using the connector, you can integrate <MadCap:variable name="General.CompanyName" /> products with Apache Kafka, using our write-behind data grid operations to Kafka to make data available for subscribers. Hadoop or any other data warehousing system can use the data for reporting and processing. <MadCap:variable name="General.CompanyName" /> products can be implemented as the consumer by subscribing to Kafka messages, and as a producer by publishing Kafka messages. Both implementations are described below.</p>
        <h1>Introduction</h1>
        <p><a href="http://kafka.apache.org">Apache Kafka</a> is a distributed publish-subscribe messaging system, which supports sending messages between applications, processes, and servers. A message is any kind of information that is sent from a producer (application that sends the messages) to a consumer (application that receives the messages).</p>
        <p>Producers write their messages, or data, to Kafka topics. These topics are divided into partitions that function like logs. Each message is written to a partition and has a unique offset, or identifier. Consumers can specify a particular offset point where they can begin to read messages. </p>
        <p MadCap:conditions="Default.DoNotShow">A Kafka server is called a broker, and a Kafka cluster contains one or more brokers. Topic partitions can reside on different brokers in the clusters, so that data in a specific topic can be split among multiple brokers. Consumers can be associated with consumer groups (each group reads from a specific topic). This enables high throughput because messages can be written and read in parallel to and from the different brokers in the cluster.</p>
        <p MadCap:conditions="Default.DoNotShow">Kafka is designed to support persistent messaging with a O(1) disk structure that provides constant time performance, even with multiple terabytes of stored messages.
Kafka provides high throughput even with very modest hardware, supporting hundreds of thousands of messages per second. Messages can be split among Kafka brokers, distributing consumption over a cluster of consumer machines while maintaining per-partition ordering semantics. For example, Kafka is often used to perform parallel data loading to Hadoop.</p>
        <h2>The Space as a Consumer</h2>
        <p>The Kafka persistence library provides a wrapper around the native Kafka Consumer API for the <MadCap:variable name="General.CompanyName"></MadCap:variable>-Kafka protocol serialization. For an example of how to use this wrapper, see the <code>com.epam.openspaces.persistency.kafka.consumer.KafkaConsumer</code> example in the <code><MadCap:variable name="General.Home-directory"></MadCap:variable>/example module</code> folder.</p>
        <p>The Space sits behind the Kafka consumer service, and the data objects are passed to the Space instances using routing keys.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/solution-hub/gs-kafka-consumer.png" class="tc-picture50" />
            </p>
        </div>
        <h2>The Space as a Producer</h2>
        <p>The <MadCap:variable name="General.CompanyName" />-Kafka integration is done via the <code>SpaceSynchronizationEndpoint</code> interface deployed as a Kafka producer service. The service consumes a batch of data grid operations, converts them into custom Kafka messages, and sends these to the Kafka broker using the Kafka Producer API.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/solution-hub/gs-kafka-producer.png" alt="xap-kafka.jpg" class="tc-picture50" />
            </p>
        </div>
        <p>The <MadCap:variable name="General.CompanyName" />-Kafka protocol  represents the data and data grid operations. A message consists of the data grid operation type (Write, Update , Remove, etc.) and the actual data object. The data object itself may be represented either as a single object or as a <code>SpaceDocument</code> with key/value pairs.
Kafka messages are sent via the network, so must be serialized into bytes.
The default encoder utilizes the Java serialization mechanism, which implies Space classes (domain model) to be <code>Serializable</code>.</p>
        <div class="tc-admon-note">
            <p>By default, Kafka messages are uniformly distributed across Kafka partitions. As such, even though data grid operations appear ordered in <code>SpaceSynchronizationEndpoint</code>, this doesn't imply the correct data processing ordering  in Kafka consumers. </p>
            <div class="tc-align-center">
                <p>
                    <img src="../Resources/Static/attachment_files/solution-hub/gs-kafka-data-order.png" alt="xap-kafka-ordering.jpg" class="tc-picture80" />
                </p>
            </div>
        </div>
        <h1>Adding Kafka Streaming to the <MadCap:variable name="General.ProductNameXAP" /> Environment</h1>
        <p>This topic includes an example of <MadCap:variable name="General.CompanyName" /> as a producer, which demonstrates how to configure Kafka persistence. It then shows how to implement a simple Kafka consumer, which pulls data from the Kafka broker and uses HsqlDB for storage.</p>
        <p>Implementing <MadCap:variable name="General.ProductNameXAP" /> as a consumer is also explained along with how to configure the services via the pu.xml file, and how to customize the integration.</p>
        <div class="tc-admon-note">
            <p>You need to install Kafka before installing the <MadCap:variable name="General.ProductNameXAP" /> connector.</p>
        </div>
        <h2>Implementing the GigaSpaces Connector for Apache Kafka</h2>
        <p>The  <MadCap:variable name="General.ProductNameXAP" /> connector for Apache Kafka is Confluent verified. We provide sample implementations of the connector as an open-source add-on to the <MadCap:variable name="General.ProductNameXAP" /> distribution. </p>
        <p class="tc-todo">To download and install the <MadCap:variable name="General.CompanyName" />-Kafka connector:</p>
        <ol>
            <li>
                <p>Download the connector from <a href="https://github.com/Gigaspaces-sbp/xap-kafka" target="_blank">Github</a>.</p>
            </li>
            <li>
                <p>Unzip the connector package into an empty folder. The connector is located under <code>&lt;project_root&gt;/example/</code>. </p>
            </li>
            <li>
                <p>To start Zookeeper and the Kafka server, type the following commands:</p><pre><code class="language-bash">bin/zookeeper-server-start.sh config/zookeeper.properties
bin/kafka-server-start.sh config/server.properties</code></pre>
            </li>
            <li>
                <p>To build the project, type the following command:</p><pre><code class="language-bash">cd &lt;project_root&gt;/example/dev-scripts
./rebuild.sh<![CDATA[
]]></code></pre>
            </li>
            <li>
                <p>To deploy the connector, type the following command:</p><pre><code class="language-bash">cd &lt;project_root&gt;/example/dev-scripts
 ./deploy.sh</code></pre>
            </li>
            <li>Check the log files for messages from the Feeder and Consumer to confirm that the connector was installed successfully.</li>
        </ol>
        <h1>Configuring the Services</h1>
        <p>After you download and install the connector, you can configure the pu.xml with any necessary properties, such as </p>
        <ul>
            <li>Kafka persistence</li>
            <li>Kafka producer or consumer properties </li>
            <li>Space class</li>
            <li>Data modeling (SpaceDocument).</li>
        </ul>
        <h2>Kafka Persistence</h2>
        <p>Kafka persistence is essentially an implementation of the SpaceSynchronizationEndpoint. It takes a batch of data sync operations, converts them to a custom message protocol and sends them to the Kafka server using the Kafka Producer API.</p>
        <h3>Library Dependency</h3>
        <p>The following Maven dependency must be included in your project in order to use Kafka persistence. This artifact is built from the <code>&lt;project_rootd&gt;/kafka-persistence</code> source directory.</p><pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;com.epam&lt;/groupId&gt;
    &lt;artifactId&gt;kafka-persistence&lt;/artifactId&gt;
    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
        <h2>Kafka Producer Service</h2>
        <p>You can configure the <code>kafkaSpaceSynchronizationEndpoint</code> as shown in the following example code, located under <code>&lt;project_root&gt;/example/mirror/src/main/resources/META-INF/spring.</code>where it is implemented as a <MadCap:variable name="General.CompanyName" /> mirror service:</p><pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:os-core="http://www.openspaces.org/schema/core"
       xmlns:os-events="http://www.openspaces.org/schema/events"
       xmlns:os-remoting="http://www.openspaces.org/schema/remoting"
       xmlns:os-sla="http://www.openspaces.org/schema/sla"
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
       http://www.openspaces.org/schema/core http://www.openspaces.org/schema/9.1/core/openspaces-core.xsd
       http://www.openspaces.org/schema/events http://www.openspaces.org/schema/9.1/events/openspaces-events.xsd
       http://www.openspaces.org/schema/remoting http://www.openspaces.org/schema/9.1/remoting/openspaces-remoting.xsd
       http://www.openspaces.org/schema/sla http://www.openspaces.org/schema/sla/9.1/openspaces-sla.xsd"&gt;

    &lt;bean id="propertiesConfigurer" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt;
        &lt;property name="locations"&gt;
            &lt;list&gt;
                &lt;value&gt;classpath:kafka.properties&lt;/value&gt;
            &lt;/list&gt;
        &lt;/property&gt;
    &lt;/bean&gt;

    &lt;bean id="kafkaSpaceSynchronizationEndpoint" class="com.epam.openspaces.persistency.kafka.KafkaSpaceSynchronizationEndpointFactoryBean"&gt;
        &lt;property name="producerProperties"&gt;
            &lt;props&gt;
                &lt;!-- Kafka producer properties. Consult Kafka documentation for a list of available properties --&gt;
                &lt;prop key="metadata.broker.list"&gt;${metadata.broker.list}&lt;/prop&gt;
                &lt;prop key="request.required.acks"&gt;${request.required.acks}&lt;/prop&gt;
            &lt;/props&gt;
        &lt;/property&gt;
    &lt;/bean&gt;

    &lt;!--
        The mirror space. Uses the Kafka external data source. Persists changes done on the Space that
        connects to this mirror space into the Kafka.
    --&gt;
    &lt;os-core:mirror id="mirror" url="/./mirror-service" space-sync-endpoint="kafkaSpaceSynchronizationEndpoint" operation-grouping="group-by-replication-bulk"&gt;
        &lt;os-core:source-space name="space" partitions="2" backups="1"/&gt;
    &lt;/os-core:mirror&gt;

&lt;/beans&gt;
</code></pre>
        <div class="tc-admon-note">
            <p>The number of backups per partition is zero or one.</p>
        </div>
        <div class="tc-admon-refer">
            <p>For more information about the <MadCap:variable name="General.CompanyName" /> mirror service, see the <MadCap:xref href="../dev-java/asynchronous-persistency-with-the-mirror.html">Asynchronous Persistency - Mirror Service</MadCap:xref> topic in the Developer guide.</p>
        </div>
        <p>You can configure the Kafka processor service as shown in the following sample code, located under &lt;project root&gt;/example/processor/src/main/resources/META-INF/spring:</p><pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:os-core="http://www.openspaces.org/schema/core"
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
       http://www.openspaces.org/schema/core http://www.openspaces.org/schema/9.1/core/openspaces-core.xsd"&gt;

    &lt;!--
        Spring property configurer which allows us to use system properties (such as user.name).
    --&gt;
    &lt;bean id="propertiesConfigurer" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"/&gt;

    &lt;!--
        Enables the usage of @GigaSpaceContext annotation based injection.
    --&gt;
    &lt;os-core:giga-space-context/&gt;

    &lt;!--
        A bean representing a space (an IJSpace implementation).
    --&gt;
    &lt;os-core:space id="space" url="/./space" schema="default" mirror="true"&gt;
        &lt;os-core:space-type type-name="Product"&gt;
            &lt;os-core:id property="CatalogNumber"/&gt;
            &lt;os-core:basic-index path="Name"/&gt;
            &lt;os-core:extended-index path="Price"/&gt;
        &lt;/os-core:space-type&gt;
    &lt;/os-core:space&gt;

    &lt;!--
        OpenSpaces simplified space API built on top of IJSpace/JavaSpace.
    --&gt;
    &lt;os-core:giga-space id="gigaSpace" space="space" /&gt;
&lt;/beans&gt;
</code></pre>
        <h3>Kafka Producer Properties</h3>
        <p>The following properties are the default applied to the Kafka producer in the <MadCap:variable name="General.CompanyName" />-Kafka protocol. You can override them if necessary. </p>
        <table>
            <thead>
                <tr>
                    <th align="left">Property</th>
                    <th align="left">Default Value</th>
                    <th align="left">Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td align="left">key.serializer.class</td>
                    <td align="left">com.epam.openspaces.persistency.kafka.<br />protocol.impl.serializer.KafkaMessageKeyEncoder</td>
                    <td align="left">Message key serializer of the default Gigaspace-Kafka protocol.</td>
                </tr>
                <tr>
                    <td align="left">serializer.class</td>
                    <td align="left">com.epam.openspaces.persistency.kafka.<br />protocol.impl.serializer.KafkaMessageEncoder</td>
                    <td align="left">Message serializer of the default Gigaspace-Kafka protocol.</td>
                </tr>
            </tbody>
        </table>
        <div class="tc-admon-note">
            <p> For a full list of available producer properties, see the Kafka <a href="https://docs.confluent.io/current/installation/configuration/producer-configs.html" target="_blank">Producer Configurations</a> page in the Confluent documentation.</p>
        </div>
        <h2>Kafka Consumer Service</h2>
        <p>You can configure the Kafka consumer service as shown in the following sample code, located under &lt;project root&gt;/example/consumer/src/main/resources/META-INF/spring:</p><pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:context="http://www.springframework.org/schema/context"
	   xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
       http://www.openspaces.org/schema/core http://www.openspaces.org/schema/core/openspaces-core.xsd"&gt;

    &lt;bean class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt;
        &lt;property name="locations"&gt;
            &lt;list&gt;
                    &lt;value&gt;classpath:kafka.properties&lt;/value&gt;
            &lt;/list&gt;
        &lt;/property&gt;
    &lt;/bean&gt;

    &lt;!-- Kafka Consumer, wrapper around native Kafka producer API --&gt;
	&lt;bean id="kafkaConsumer" class="com.epam.openspaces.persistency.kafka.consumer.KafkaConsumerFactoryBean"&gt;
		&lt;property name="consumerProperties"&gt;
			&lt;props&gt;
                &lt;!-- Kafka consumer properties. Consult Kafka documentation for a list of available properties --&gt;
				&lt;prop key="zookeeper.connect"&gt;${zookeeper.connect}&lt;/prop&gt;
				&lt;prop key="group.id"&gt;${group.id}&lt;/prop&gt;
				&lt;prop key="zookeeper.session.timeout.ms"&gt;${zookeeper.session.timeout.ms}&lt;/prop&gt;
				&lt;prop key="zookeeper.sync.time.ms"&gt;${zookeeper.sync.time.ms}&lt;/prop&gt;
				&lt;prop key="auto.commit.interval.ms"&gt;${auto.commit.interval.ms}&lt;/prop&gt;
			&lt;/props&gt;
		&lt;/property&gt;
	&lt;/bean&gt;

    &lt;!-- Consumer which implements business logic, consumes data from Kafka and saves to database using Hibernate --&gt;
	&lt;bean id="consumer" class="com.epam.consumer.Consumer"&gt;
		&lt;property name="consumer" ref="kafkaConsumer" /&gt;
	&lt;/bean&gt;

&lt;/beans&gt;</code></pre>
        <h2>Associating a Kafka Topic with a Space Class</h2>
        <p>In order to associate a Kafka topic with the domain model class, the class must be annotated with the <code>@KafkaTopic</code> annotation and declared as <code>Serializable</code>. See the following sample code.</p><pre><code class="language-java">@KafkaTopic("user_activity")
@SpaceClass
public class UserActivity implements Serializable {
    ...
}
</code></pre>
        <h2>Space Documents</h2>
        <p>To configure a Kafka topic for a <code>SpaceDocument</code> or <code>Extended SpaceDocument</code>, the <code>KafkaPersistenceConstants.SPACE_DOCUMENT_KAFKA_TOPIC_PROPERTY_NAME</code> property should be added to the document. See the following sample code.</p><pre><code class="language-java">public class Product extends SpaceDocument {

public Product() {
    super("Product");
    super.setProperty(SPACE_DOCUMENT_KAFKA_TOPIC_PROPERTY_NAME, "product");
}
</code></pre>
        <p>You can also configure the name of the property that defines the Kafka topic for SpaceDocuments. Set the <code>spaceDocumentKafkaTopicName</code> to the required value, as shown in the sample code below.</p><pre><code class="language-xml">&lt;bean id="kafkaSpaceSynchronizationEndpoint" class="com.epam.openspaces.persistency.kafka.KafkaSpaceSynchronizationEndpointFactoryBean"&gt;
    ...
    &lt;property name="spaceDocumentKafkaTopicName" value="topic_name" /&gt;
&lt;/bean&gt;
</code></pre>
        <h1>Customizing the Integration</h1>
        <p>You can customize the <MadCap:variable name="General.CompanyName" />-Kafka protocol as necessary, to suit your specific use case and environment. The following guidelines may be helpful:</p>
        <ul>
            <li>Kafka persistence was designed to be extensible and customizable.</li>
            <li>If you want to create a custom protocol between your <MadCap:variable name="General.CompanyName" /> product and Kafka, provide an implementation of <code>AbstractKafkaMessage</code>, <code>AbstractKafkaMessageKey</code>, and <code>AbstractKafkaMessageFactory</code>.</li>
            <li>If you want to customize how the data grid operations are sent to Kafka, or how the Kafka topic is chosen for a given entity, provide an implementation of <code>AbstractKafkaSpaceSynchronizationEndpoint</code>.</li>
            <li>If you want to create a custom serializer, look at <code>KafkaMessageDecoder</code> and <code>KafkaMessageKeyDecoder</code>.</li>
            <li MadCap:conditions="Default.DoNotShow">The Kafka producer service, which is used under the hood, can be configured with a number of settings, see Kafka documentation.</li>
        </ul>
    </body>
</html>