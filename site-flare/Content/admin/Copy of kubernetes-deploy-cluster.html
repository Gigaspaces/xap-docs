<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1 class="tc-pagetitle"><a name="deploying-a-space-cluster"></a>Deploying a Space Cluster in Kubernetes</h1>
        <p>The demo in <MadCap:xref href="kubernetes-data-grid.html">Getting Started with GigaSpaces in Kubernetes</MadCap:xref> created a data grid that contained a single Space instance. Real-life environments generally have to store large volumes of data, and therefore need more than a single Space instance (a cluster), and need to communicate with application clients that may reside outside of the Kubernetes environment.</p>
        <p>Type the following Helm command to deploy a Space cluster with <span class="tc-italic">n</span> Data Pods, with a partition count from 1 to <span class="tc-italic">n</span>:</p><pre><code>helm install insightedge --name test --set pu.partitions=n
</code></pre>
        <p MadCap:conditions="Version.15-0-born">This topic describes these aspects of deploying a Space.</p>
        <ul>
            <li MadCap:conditions="Version.15-0-born">Define the Kubernetes service type for the space</li>
            <li MadCap:conditions="Version.15-0-born">Define high availability for the space</li>
            <li MadCap:conditions="Version.15-0-born">Define multiple spaces in the same Kubernetes environment</li>
            <li MadCap:conditions="Version.15-0-born">Define space cluster to enable data operations  on the cluster running in Kubernetes from remote clients</li>
        </ul>
        <h1 MadCap:conditions="Version.15-0-born">Defining Service Type</h1>
        <p MadCap:conditions="Version.15-0-born">Kubernetes services define a logical set of Pods and a policy by which to access them. Kubegrid can be deployed with these Kubernetes <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/" target="_blank">service types</a>:</p>
        <ul MadCap:conditions="Version.15-0-born">
            <li>
                <p>LoadBalancer - External load balancer in the cloud with dedicated IP address</p>
            </li>
            <li>
                <p>NodePort - Exposes the Service on the same port of each selected Node</p>
            </li>
        </ul>
        <p MadCap:conditions="Version.15-0-born">In a stateful partitioned system, such as Giga Data Grid, each pod has a different state and role. Since the Giga client is smart and connects directly to each member of the cluster to avoid redundant network hops, multiple Kube services must be used. One of the following options can be deployed to access the services.</p>
        <h2 MadCap:conditions="Version.15-0-born">LoadBalancer</h2>
        <p MadCap:conditions="Version.15-0-born">The default service type for the manager and the processing units is LoadBalancer. To provide client access with this service:</p>
        <ol MadCap:conditions="Version.15-0-born">
            <li>The helm chart creates a dedicated service for each pod.</li>
            <li>During startup the pod registers itself in the lookup sevice with the loadBalancer IP.</li>
            <li>The client queries the lookup service and retrieves all member addresses.</li>
        </ol>
        <div class="tc-admon-note" MadCap:conditions="Version.15-0-born">
            <p>This option requires a loadBalancer per service.</p>
        </div>
        <h2 MadCap:conditions="Version.15-0-born">NodePort</h2>
        <p MadCap:conditions="Version.15-0-born">NodePort exposes each service on a specific port that can be assigned randomly or specified when you install the helm charts. With NodePort the pod registers in the lookup service with the assigned IP.</p>
        <p MadCap:conditions="Version.15-0-born">By default Kubernetes will assign the NodePort for each service randomly in the range 30000-32767. If you wish to specify the ports, rather than exposed the entire range of ports, specify the initial port number, as in the following example, and Kubegrid will then assign sequential port numbers for each instance.</p><pre MadCap:conditions="Version.15-0-born"><code class="language-bash">helm install insightedge-pu --name testspace --set manager.name=testmanager, service.type=NodePort, service.lrmi.InitialNodePort=31200</code></pre>
        <div class="tc-admon-note" MadCap:conditions="Version.15-0-born">
            <p>This option requires exposing the assigned ports to external access. You also have to ensure the assigned ports are not used by another processing unit in your cluster.</p>
        </div>
        <h2 MadCap:conditions="Version.15-0-born">Service Ports</h2>
        <p MadCap:conditions="Version.15-0-born">Default ports are assigned to services - such as api, lookup. You can configure the service to use a different port, as in this example setting the manager api port for the REST Manager API</p><pre MadCap:conditions="Version.15-0-born"><code class="language-bash">helm install insightedge-manager --name testspace --set service.type=NodePort, service.api.port=8290</code></pre>
        <p>You can view the services and ports for each service with the command <code class="language-bash">kubectl describe svc &lt;service name&gt; </code></p>
        <h1>Defining High Availability (HA)</h1>
        <p>There are several aspects to configuring a data grid for high availability. Each primary Data Pod needs a minimum of one backup Data Pod, and three Management Pods are deployed instead of one so that a quorum of Platform Managers is always available to manage the Spaces. Both the Data Pods and the Management Pods should have the Pod anti-affinity property set to true, so that the primary/backup sets and the managers are deployed on different nodes. This enables successful failover if a node gets disrupted.</p>
        <div class="tc-admon-note">
            <p>The Kubernetes minikube runs on a single node and therefore doesn't provide anti-affinity, so you may want to evaluate <MadCap:variable name="General.ProductNameXAP" /> and <MadCap:variable name="General.ProductNameIE" /> high-availability behavior on a Kubernetes cluster that contains multiple nodes.</p>
        </div>
        <h2><a name="configuring-high-availability-for-the-platform-manager"></a>Configuring High Availability for the Platform Manager</h2>
        <p>When the manager high availability property (<code>ha</code>) is set to true, Kubernetes deploys three Management Pods. You should enable the manager high availability property so these Management Pods are deployed on different nodes.</p>
        <p>The following Helm command deploys three Management Pods (instead of one) with high availability enabled:</p><pre><code class="language-bash">helm install insightedge-manager --name test --set manager.ha=true,manager.antiAffinity.enabled=true
</code></pre>
        <h2><a name="defining-the-space-topology"></a>Defining the Space Topology</h2>
        <p>When you set the Space high availability property to true, Kubernetes deploys a backup Data Pod for each primary Data Pod. You must also enable the Space anti-affinity (<code>antiAffinity</code>) property so that the backup Data Pods are deployed on different nodes than the primary Data Pods.</p>
        <div class="tc-admon-note">
            <p>If you apply Pod anti-affinity on a minikube, not all of the Pods will be deployed, because the environment contains only a single node.</p>
        </div>
        <p>The following Helm command deploys a Space cluster called <code>test</code> in a high availability topology, with anti-affinity enabled:</p><pre><code class="language-bash">helm install insightedge --name test --set pu.ha=true,pu.antiAffinity.enabled=true
</code></pre>
        <h1><a name="deploying-multiple-spaces-on-kubernetes"></a>Deploying Multiple Spaces on Kubernetes</h1>
        <p>If you want to deploy multiple data grids in the same Kubernetes environment, to better utilize resources it is best to deploy one Platform Manager cluster and then configure the Spaces to use this cluster, instead of deploying each data grid with its own Platform Manager.</p>
        <p>To enable users to customize the installation, we provide several additional Helm charts that can be used to define specific constellations in more advanced scenarios:</p>
        <ul>
            <li>Manager (<code>insightedge-manager</code> or <code>xap-manager</code>)</li>
            <li>Processing Unit (<code>insightedge-pu</code> or <code>xap-pu</code>)</li>
            <li>Apache Zeppelin (<code>insightedge-zeppelin</code>)</li>
        </ul>
        <p>Before using these charts for the first time, you must fetch the charts as described in Getting Started section.</p>
        <h2><a name="deploying-the-platform-manager"></a>Deploying the Platform Manager</h2>
        <p>The helm command by default creates a Management Pod and a Data Pod together. When deploying a Platform Manager that will connect to multiple Spaces, you have to disable the part of the command that creates the Data Pod. Type the following Helm command to create a Management Pod called <code>testmanager</code> without the accompanying Space:</p><pre><code class="language-bash">helm install insightedge-manager --name testmanager
</code></pre>
        <h2><a name="deploying-the-spaces"></a>Deploying the Spaces</h2>
        <p>After the Management Pod has been deployed and the Platform Manager is available, you can deploy the Space instances and connect them to the Platform Manager. Use the following Helm command to deploy a cluster of Data Pods called <code>testspace</code>, and to specify that the cluster should connect to the <code>testmanager</code> Management Pod:</p><pre><code class="language-bash">helm install insightedge-pu --name testspace --set manager.name=testmanager
</code></pre>
        <h1 MadCap:conditions="Version.15-0-born">Space Based Remoting Support</h1>
        <p MadCap:conditions="Version.15-0-born">You can configure the space cluster to enable data operations  on the cluster running in Kubernetes from remote clients. Once enabled, a <MadCap:variable name="General.CompanyName" /> proxy can connect to the external service that is exposed in Kubernetes.</p>
        <h2 MadCap:conditions="Version.15-0-born">Enable Client Access</h2>
        <p MadCap:conditions="Version.15-0-born">To enable space based remoting we need to expose each space instance as a service. To expose the spaces enable the <code class="language-bash">lrmi </code>service  for the processing unit when you install the Helm charts, as in the following command:</p><pre xml:space="preserve" MadCap:conditions="Version.15-0-born"><code>helm install insightedge-pu --name testspace --set manager.name=testmanager,set service.lrmi.enabled=true</code></pre>
        <div class="tc-admon-note">
            <p>If the service type is loadBalancer disable <code class="language-bash">lrmi</code> when space based remoting is no longer required, This will free the loadBalancer resources that were allocated for space based remoting.</p>
        </div>
        <h2 MadCap:conditions="Version.15-0-born">Create Proxy</h2>
        <p MadCap:conditions="Version.15-0-born">Once you've enabled external client access, you can create a GigaSpaces proxy (see  <a href="../started/xap-tutorial-part1.html">Interacting with the Space</a>). Use the external IP that is exposed in Kubernetes as the lookup locator when creating the proxy. You can retrieve the external IP by running the command <code class="language-bash">kubectl get svc</code> with the following output:</p>
        <p MadCap:conditions="Version.15-0-born">
            <img src="../Resources/Static/attachment_files/admin/kubectlGetSvc.png" />
        </p>
        <p MadCap:conditions="Version.15-0-born">For example in your Java code</p>
        <p MadCap:conditions="Version.15-0-born,Default.DoNotShow"><b>&gt;&gt;&gt; Should we update sample output for current loadbalance IP (4174) or use Nodeport</b>
        </p><pre MadCap:conditions="Version.15-0-born">GigaSpace gigaSpace = new GigaSpaceConfigurer(new SpaceProxyConfigurer
("testspace").lookupLocators(“17.204.211.27:30174”)).gigaSpace();</pre>
        <p MadCap:conditions="Version.15-0-born">If you are using Minikube, you can use 192.168.99.100:30174.</p>
        <p MadCap:conditions="Version.15-0-born,Default.DoNotShow" style="font-weight: bold;">&gt;&gt;&gt; TBD if following section should be included. Can we provide any reason to change default class loaing mechanism?</p>
        <h2 MadCap:conditions="Version.15-0-born">Remote Class Loading</h2>
        <p MadCap:conditions="Version.15-0-born">Kubegrid implements remote class loading with simple class loading, In non-Kubernetes enviroments lrmi class loading is used. You can optionally use simple class loading in all environments by setting the system property com.gs.transport_protocol.lrmi.simple-classloading to true.</p>
        <h2 MadCap:conditions="Version.15-0-born">Minikube Configuration</h2>
        <p MadCap:conditions="Version.15-0-born">With remote space loading in minikube environment with LoadBalancer service type, the manager will not start because it does not obtain an external IP. To expose the IP you must enter the command <code class="language-bash">minikube tunnel</code>, that tunnels each service to the IP of the virtual machine. After the command runs, the manager service status changes to Running.</p>
        <p MadCap:conditions="Version.15-0-born">If you configure NodePort service type, the manager will start without entering the tunnl command.</p>
    </body>
</html>