<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1 class="tc-pagetitle">Running a Spark Session in a Processing Unit</h1>
        <p>If you want to run a Spark session directly inside a Processing Unit, <MadCap:variable name="General.CompanyName" /> supports using <code>SparkSession</code>. This is the entry point to interacting with Spark, and to enable programming with the Dataset and DataFrame APIs.</p>
        <p>You can initiate and run a <code>SparkSession </code>inside a Processing Unit with the <MadCap:variable name="General.CompanyName" /> <code>SparkSessionProvider</code>.</p>
        <h2>Creating a SparkSession</h2>
        <p>The <code>SparkSessionProvider.Builder()</code> method enables users to configure and create the <code>SparkSession</code> that they want to run in the Processing Unit. The functionality is very similar to the generic <code>SparkSession.builder()</code> method. See the sample code provided here:</p><pre><code class="language-java">SparkSessionProvider sparkSessionProvider = new SparkSessionProvider.Builder()
       .master("spark://localhost:7077")
       .create();
SparkSessionProvider.Wrapper sparkSessionWrapper = sparkSessionProvider.getOrCreate();</code></pre>
        <p>The <code>SparkSessionProvider.getOrCreate()</code> method returns a <code>sparkSessionWrapper</code>. If a wrapper already exists (if the method was previously called), the same wrapper is returned. Calling the method increments a global reference counter.</p>
        <p>The <code>SparkSessionWrapper</code> has a close method that decrements the global reference counter and closes the <code>SparkSession</code> if the counter is zero.</p>
        <h2>Example</h2>
        <p>Let’s assume we have a service that exposes a <code>countLines</code> method, as follows:</p><pre><code class="language-java">package com.mycompany.app;

public interface MyService {
   long countLines(String path);
}</code></pre>
        <p>An implementation that uses a <code>SparkSession</code> to perform the counting via the DataFrame API would look like this:</p><pre><code class="language-java">package com.mycompany.app;

import org.apache.spark.sql.SparkSession;
import org.insightedge.spark.SparkSessionProvider;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import javax.annotation.PostConstruct;
import java.io.Closeable;
import java.io.IOException;

public class MyServiceImpl implements MyService, Closeable {
   private static final Logger logger = LoggerFactory.getLogger(MyServiceImpl.class);
   private SparkSessionProvider.Wrapper sparkSessionWrapper;


   @PostConstruct
   public void initialize() {

       // Use SparkSessionProvider.Builder() instead of SparkSession.builder() to create a Spark Session
       SparkSessionProvider sparkSessionProvider = new SparkSessionProvider.Builder()
               .master("spark://localhost:7077")
               .create();
       sparkSessionWrapper = sparkSessionProvider.getOrCreate();
   }

   @Override
   public long countLines(String path) {
       logger.info("Started to count rows of [" + path + "]");
       SparkSession sparkSession = sparkSessionWrapper.get();
       long totalRows = sparkSession.read().text(path).count();
       logger.info("Total rows in file: " + totalRows);
       return totalRows;
   }

   @Override
   public void close() throws IOException {
       if (sparkSessionWrapper != null)
           sparkSessionWrapper.close();
   }
}</code></pre>
        <p>And then you can initialize the bean with a Configuration class, as follows:</p><pre><code class="language-java">package com.mycompany.app;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class MyConfiguration {

   @Bean
   public MyServiceImpl myService() {
       return new MyServiceImpl();
   }

}</code></pre>
        <div class="tc-admon-attention">
            <p>Spark-related JARs are added to the classloader during runtime when the <code>SparkSessionProvider</code> is used. As a result, if you use Spark classes before this class is initialized, you will get a ClassNotFound error. For example, if you have a method that returns <code>SparkSession</code> in a bean that is configured in the pu.xml file.</p>
        </div>
    </body>
</html>