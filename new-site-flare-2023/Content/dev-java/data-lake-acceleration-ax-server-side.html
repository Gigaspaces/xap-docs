<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1>Configuring AnalyticsXtreme on the Server Side</h1>
        <p MadCap:conditions="Default.DoNotShow"><MadCap:variable name="General.ProductNameIE" /> only</p>
        <p> AnalyticsXtreme is implemented in conjunction with a specific Space. This is part of the standard configuration of a pu.xml file.</p>
        <div class="tc-admon-note">
            <p>For more information about pu.xml files and their properties, read the <MadCap:xref href="configuring-processing-unit-elements.html">Configuration</MadCap:xref> topic in <MadCap:xref href="the-processing-unit-overview.html">The Processing Unit</MadCap:xref> section of the developer guide.</p>
        </div>
        <p>Configuring AnalyticsXtreme on the server side involves the following steps:</p>
        <ol>
            <li>Defining the Space.</li>
            <li>Setting up the AnalyticsXtreme Manager</li>
            <li> Configuring the data life cycle policy.</li>
            <li>Defining the data source and the data target.</li>
            <li>Exporting the AnalyticsXtreme Manager service for discovery (so that client applications can access it).</li>
        </ol>
        <p>The Space definition is part of the standard pu.xml configuration as described in <MadCap:xref href="the-processing-unit-overview.html">The Processing Unit</MadCap:xref> section of the developer guide. The rest of the steps, which are specific to AnalyticsXtreme, are explained in the sections below.</p>
        <h1>Setting Up the AnalyticsXtreme Manager</h1>
        <p>As mentioned above,  all of the relevant information about AnalyticsXtreme is provided to the data grid in a dedicated pu.xml file. The first step in the configuration process is creating an AnalyticsXtreme Manager bean, with a bean ID and class. In this part of the pu.xml, you can also modify the AnalyticsXtreme logging policy configuration if necessary.</p>
        <p>In the following code snippet, taken from the full pu.xml file, the AnalyticsXtreme Manager is assigned a bean ID of <code>ax-manager</code>, and the logging policy has been changed from its default value.</p><pre><code class="language-xml">&lt;bean id="ax-manager" class="com.gigaspaces.analytics_xtreme.server.AnalyticsXtremeManagerFactory"&gt;
     &lt;property name="config"&gt;
          &lt;bean class="com.gigaspaces.analytics_xtreme.AnalyticsXtremeConfigurationFactoryBean"&gt;
               &lt;!-- Verbose is recommended for getting started, usually turned off in production --&gt;
               &lt;property name="verbose" value="true"/&gt;
               &lt;!-- more properties --&gt;
          &lt;/bean&gt;
     &lt;/property&gt;
&lt;/bean&gt;</code></pre>
        <h2>Cold Starts</h2>
        <p>For persistency reasons, <MadCap:variable name="General.ProductNameIE" /> maintains a record of the last confirmed data that was copied from the Space to the external data source. If you need to undeploy and then redeploy a Processing Unit, for example as part of a maintenance activity, you can configure whether you want to perform a cold start of the Space (copy data from the Space to the external data source from the beginning), or whether you want the redeployed Processing Unit to continue copying the data from where it left off when it was undeployed.</p>
        <h2>Global Properties</h2>
        <p>The global configuration contains a list of DataLifecyclePolicy properties, as well as the AnalyticsXtreme logging level.</p>
        <table style="width: 100%;" class="tc-standard">
            <col />
            <col />
            <col />
            <col />
            <col />
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Description</th>
                    <th>Unit</th>
                    <th>Default Value</th>
                    <th>Required/Optional</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>verbose </td>
                    <td>Increases the log levels for both the client and the server to provide verbose AnalyticsXtreme information (useful for troubleshooting).</td>
                    <td>True/False</td>
                    <td>False</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>coldStart</td>
                    <td>(Automatic data tiering only) After redeploying a Processing Unit, set the value to "true" if you want to copy data from the <code>batchDataSource</code> to the<code> batchDataTarget</code> from the beginning. Leave the default value to continue copying data from the last confirmed data that was copied from the <code>batchDataSource</code> to the <code>batchDataTarget</code>.</td>
                    <td>True/False</td>
                    <td>False</td>
                    <td>Optional</td>
                </tr>
            </tbody>
        </table>
        <h1>Configuring the Data Life Cycle Policy</h1>
        <p>The next step in configuring AnalyticsXtreme is to define the data life cycle policy, which specifies how and when data is archived from the data grid (speed layer) to the external data storage (batch layer). This policy is configured in the AnalyticsXtreme Manager for each data object, or table.</p>
        <p>You must define the following properties in the policy: </p>
        <ul>
            <li><code>typeName</code>
            </li>
            <li><code>timeColumn</code>
            </li>
            <li><code>speedPeriod</code>
            </li>
            <li><code>batchDataSource</code>
            </li>
        </ul>
        <p>All other properties are optional, and you can leave the default values unless your specific environment has different requirements. See <MadCap:xref href="#Table">Table (Object) Properties</MadCap:xref> below for a full list of the data life cycle policy properties and their descriptions.</p>
        <div class="tc-admon-attention">
            <p>The relationship between the data object and the data life cycle policy is one to one. You cannot have more than one policy per data object, and each policy applies to a single data object.</p>
        </div>
        <h2><a name="Adding"></a>Defining the Data Source and Data Target</h2>
        <p>Part of configuring the data life cycle policy is defining the <code>batchDataSource</code> property. You can define it directly, or you can use a <code>ref </code>to point to the definition. If you are implementing AnalyticsXtreme in external data tiering mode, this is the only required property because the data is simply evicted at the end of the life cycle.</p>
        <p>If you are implementing AnalyticsXtreme in automatic data tiering mode, you must also define the <code>batchDataTarget</code> property using the same method. Towards the end of the <code>speedPeriod</code>, the data will be moved to the target defined here.</p>
        <h3>Example</h3>
        <p>The following code snippet from the pu.xml file shows a data life cycle policy that was configured for specific trading information that needs to be stored in the data grid for 5 hours, and then moved to external data storage. Both the data source and the data target have their full definitions in another area of the pu.xml file.</p><pre><code class="language-xml">&lt;list&gt;
     &lt;!-- Data life cycle policy for Trade class --&gt;
     &lt;bean class="com.gigaspaces.analytics_xtreme.DataLifecyclePolicyFactoryBean"&gt;
          &lt;property name="typeName" value="com.gigaspaces.demo.Trade"/&gt;
          &lt;property name="timeColumn" value="dateTimeTrade"/&gt;
          &lt;property name="speedPeriod" value="pt5h"/&gt;
          &lt;property name="batchDataSource" ref="ax-datasource"/&gt;
          &lt;property name="batchDataTarget" ref="ax-datatarget"/&gt;
     &lt;/bean&gt;
     &lt;!-- Add a life cycle policy for each additional class --&gt;
&lt;/list&gt;</code><![CDATA[    ]]></pre>
        <h3>Configuring the Data Source</h3>
        <p>AnalyticsXtreme supports HDFS<MadCap:conditionalText MadCap:conditions="Default.DoNotShow"> and Amazon S3</MadCap:conditionalText> out of the box. However, you can add any external data source that is supported by Spark. </p>
        <p>The following code snippet demonstrates how to configure AnalyticsXtreme to work with a Spark Hive data source.</p><pre><code class="language-xml">&lt;!-- Data source plugin based on Spark Hive --&gt;
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataSourceFactoryBean"&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;</code><![CDATA[
]]></pre>
        <div class="tc-admon-note">
            <p>The data source and the data target use the same <code>sparkSessionFactory </code>bean. </p>
        </div>
        <h3>Configuring the Data Target</h3>
        <p>There are three possible implementations for the data target out of the box; JDBC, Spark, and Hive. Each is wrapped in a factory bean. The <code>BatchDataTarget</code> interface can be used for other data targets by implementing the <code>feed</code> method. </p>
        <p><span class="tc-bold">Spark Hive Batch Data Target Exampl</span>e</p>
        <p>This example demonstrates the Spark Hive implementation.</p><pre><code class="language-xml">&lt;!-- Data target plugin based on Spark Hive --&gt;
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataTargetFactoryBean"&gt;
     &lt;property name="format" value="hive"/&gt;
     &lt;property name="mode" value="append"/&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Spark session provider --&gt;
&lt;bean id="ax-sparkSessionFactory" class="org.insightedge.spark.SparkSessionProviderFactoryBean"&gt;
     &lt;property name="master" value="local[*]"/&gt;
     &lt;property name="enableHiveSupport" value="true"/&gt;
     &lt;property name="configOptions"&gt;
          &lt;map&gt;
               &lt;entry key="hive.metastore.uris" value="thrift://hive-metastore:9083"/&gt;
          &lt;/map&gt;
     &lt;/property&gt;
&lt;/bean&gt;</code><![CDATA[
]]></pre>
        <p><span class="tc-bold">JDBC&#160;Batch Data Target Example</span>
        </p>
        <p>This example demonstrates the JDBC implementation.</p><pre><code class="language-xml">&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataSourceFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
&lt;/bean&gt;
	
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataTargetFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
     &lt;property name="useLowerCase" value="true"/&gt;
&lt;/bean&gt;</code></pre>
        <h2>Using a Different Apache Hive Version</h2>
        <p>Apache Spark currently is packaged with Apache Hive version 1.2.1. If you want to use a different Hive version for the batch data source or the batch data target, you need to do the following:</p>
        <ol>
            <li>Import the relevant JAR&#160;files.</li>
            <li>
                <p>Add the following to the pu.xml under the <code>sparkSessionFactory</code> <code>configOptions</code>:</p><pre><code class="language-xml">property name="configOptions"&gt;
       &lt;map&gt;
           &lt;entry key="hive.metastore.uris" value="thrift://hive-metastore:9083"/&gt;
           &lt;entry key="spark.sql.hive.metastore.version" value="2.3.2"/&gt;
           &lt;entry key="spark.sql.hive.metastore.jars"
                  value="#{systemEnvironment['SPARK_HOME']}/jars/hive/*:#{systemEnvironment['SPARK_HOME']}/jars/*"/&gt;
           &lt;entry key="spark.ui.enabled" value="false"/&gt;
       &lt;/map&gt;
   &lt;/property&gt;</code></pre>
            </li>
        </ol>
        <div class="tc-admon-note">
            <p>AnalyticsXtreme is tested and certified on Apache Hive version 2.3.2.</p>
        </div>
        <h2><a name="Table"></a>Table (Object) Properties</h2>
        <p>The DataLifecyclePolicy table contains the following parameters. The first four properties are required. The others are optional, and it is recommended to leave the default values as is unless your business application environment has specific requirements.</p>
        <table style="width: 100%;" class="tc-standard">
            <col />
            <col />
            <col />
            <col />
            <col />
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Description</th>
                    <th>Unit</th>
                    <th>Default Value</th>
                    <th>Required/Optional</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>typeName*</td>
                    <td>Name of the table/type/class to which this policy applies.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>timeColumn** </td>
                    <td>Name of column/property/field that contains the time data used to manage this policy.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>speedPeriod </td>
                    <td>Time period or fixed timestamp.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>batchDataSource </td>
                    <td>Endpoint for querying the batch layer</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>batchDataTarget </td>
                    <td>Endpoint for feeding data to the batch layer. Only necessary when AnalyticsXtreme is implemented in Automatic data tiering mode.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>timeFormat </td>
                    <td>Time format for the data in the timeColumn parameter.</td>
                    <td>See <MadCap:xref href="#Java">Java Time Pattern</MadCap:xref> below</td>
                    <td>Java time format (see the <a href="https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_LOCAL_DATE_TIME" target="_blank">Java API documentation)</a></td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>mutabilityPeriod </td>
                    <td>Period of time during which data can be updated.</td>
                    <td> Duration in time units or % of speedPeriod.</td>
                    <td>80%</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>batchFeedInterval </td>
                    <td>Data is fed from the Space (speed layer) to the batch layer in these time-based intervals at the end of the speedPeriod, after the mutabilityPeriod has expired.</td>
                    <td>See <MadCap:xref href="#Java">Java Time Pattern</MadCap:xref> below</td>
                    <td>pt1m</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>batchFeedSize </td>
                    <td>Maximum data entries per batch feed interval.</td>
                    <td>Integer&#160;(number of entries)</td>
                    <td>1000</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>evictionPollingInterval </td>
                    <td>Polling interval for querying and evicting each policy.</td>
                    <td>See <MadCap:xref href="#Java">Java Time Pattern</MadCap:xref> below</td>
                    <td>pt1m</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>evictionBuffer </td>
                    <td>Additional waiting period before evicting data from the Space (speed layer)&#160;after it was fed to the batch layer, so that long queries and clock differences won't cause errors or generate exceptions. </td>
                    <td>Duration in time units or % of speedPeriod.</td>
                    <td>pt10m</td>
                    <td>Optional</td>
                </tr>
            </tbody>
        </table>
        <p>* This correlates to an object or JSON in the object store.</p>
        <p>** This correlates to a property or entity in an object or JSON.</p>
        <h3><a name="Java"></a>Java Time Pattern</h3>
        <p>AnalyticsXtreme is a time-based feature, and therefore uses the Java time pattern (required as of Java 8) in the <code>timeFormat</code> property. This time pattern requires use of the letter "p" in either upper or lower case in all duration fields, as well as the letter "t" for any duration that is less than 24 hours long.</p>
        <p>For example, <code>pt15m</code> represents a duration of 15 minutes, and <code>pt10h</code> represents a duration of 10 hours. A duration of 2 days should be written as <code>p2d</code>.</p>
        <p>For more information about the Java time pattern, see the <a href="https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-" target="_blank">Java API documentation</a>.</p>
        <h1>Exporting the AnalyticsXtreme Manager Service</h1>
        <p>In order for client applications to access the speed layer, the AnalyticsXtreme Manager needs to be exported  to enable discovery. This mechanism leverages the data grid's remoting mechanism, which registers the AnalyticsXtreme Manager as a remote service.</p>
        <p>The following code snippet demonstrates how to implement this mechanism in the pu.xml.</p><pre><code class="language-xml">&lt;!-- Register the ax-manager bean as a remote service so clients can load configuration &amp; stats from it --&gt;
&lt;os-remoting:service-exporter id="serviceExporter"&gt;
     &lt;os-remoting:service ref="ax-manager"/&gt;
&lt;/os-remoting:service-exporter&gt;</code></pre>
        <div class="tc-admon-note">
            <p>For more information about the data grid's remoting mechanism, see the <MadCap:xref href="space-based-remoting-overview.html">Space-Based Remoting</MadCap:xref> section of the developer guide.</p>
        </div>
        <h1>Sample pu.xml File</h1>
        <p>The following pu.xml file shows all the above steps, including defining a Space. </p><pre MadCap:conditions="Version.14-5-died"><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;beans xmlns="http://www.springframework.org/schema/beans"
     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
     xmlns:context="http://www.springframework.org/schema/context"
     xmlns:os-core="http://www.openspaces.org/schema/core"
     xmlns:os-remoting="http://www.openspaces.org/schema/remoting"
     xsi:schemaLocation="
   http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-<MadCap:variable name="Versions.spring-short" />.xsd
   http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-<MadCap:variable name="Versions.spring-short" />.xsd
   http://www.openspaces.org/schema/core http://www.openspaces.org/schema/14.0/core/openspaces-core.xsd
   http://www.openspaces.org/schema/remoting http://www.openspaces.org/schema/remoting/openspaces-remoting.xsd"&gt;

     &lt;context:annotation-config /&gt;

     &lt;os-core:annotation-support /&gt;

     &lt;os-core:embedded-space id="space" space-name="demo-space" /&gt;

     &lt;os-core:giga-space id="gigaSpace" space="space"/&gt;


&lt;bean id="ax-manager" class="com.gigaspaces.analytics_xtreme.server.AnalyticsXtremeManagerFactory"&gt;
     &lt;property name="config"&gt;
          &lt;bean class="com.gigaspaces.analytics_xtreme.AnalyticsXtremeConfigurationFactoryBean"&gt;
               &lt;!-- Verbose is recommended for getting started, usually turned off in production --&gt;
               &lt;property name="verbose" value="true"/&gt;
               &lt;!-- more properties --&gt;
          &lt;/bean&gt;
     &lt;/property&gt;
&lt;/bean&gt;
	
&lt;!-- Data life cycle policy for Trade class --&gt;
&lt;bean class="com.gigaspaces.analytics_xtreme.DataLifecyclePolicyFactoryBean"&gt;
     &lt;property name="typeName" value="com.gigaspaces.demo.Trade"/&gt;
     &lt;property name="timeColumn" value="dateTimeTrade"/&gt;
     &lt;property name="speedPeriod" value="pt5h"/&gt;
     &lt;property name="batchDataSource" ref="ax-datasource"/&gt;
     &lt;property name="batchDataTarget" ref="ax-datatarget"/&gt;
&lt;/bean&gt;

&lt;!-- Register the ax-manager bean as a remote service so clients can load configuration &amp; stats from it --&gt;
&lt;os-remoting:service-exporter id="serviceExporter"&gt;
     &lt;os-remoting:service ref="ax-manager"/&gt;
&lt;/os-remoting:service-exporter&gt;


&lt;!-- Data source plugin based on Spark Hive --&gt;
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataSourceFactoryBean"&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Data target plugin based on Spark Hive --&gt;
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataTargetFactoryBean"&gt;
     &lt;property name="format" value="hive"/&gt;
     &lt;property name="mode" value="append"/&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Spark session provider --&gt;
&lt;bean id="ax-sparkSessionFactory" class="org.insightedge.spark.SparkSessionProviderFactoryBean"&gt;
     &lt;property name="master" value="local[*]"/&gt;
     &lt;property name="enableHiveSupport" value="true"/&gt;
     &lt;property name="configOptions"&gt;
          &lt;map&gt;
               &lt;entry key="hive.metastore.uris" value="thrift://hive-metastore:9083"/&gt;
          &lt;/map&gt;
     &lt;/property&gt;
&lt;/bean&gt;

&lt;!-- To use jdbc instead of spark, replace ax-datasource, ax-datatarget and sparkSessionFactory with these beans
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataSourceFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
&lt;/bean&gt;
	
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataTargetFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
     &lt;property name="useLowerCase" value="true"/&gt;
&lt;/bean&gt;
--&gt;
&lt;/beans&gt;</code><![CDATA[
]]></pre><pre MadCap:conditions="Version.14-5-born"><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;beans xmlns="http://www.springframework.org/schema/beans"
     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
     xmlns:context="http://www.springframework.org/schema/context"
     xmlns:os-core="http://www.openspaces.org/schema/core"
     xmlns:os-remoting="http://www.openspaces.org/schema/remoting"
     xsi:schemaLocation="
   http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
   http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd
   http://www.openspaces.org/schema/core http://www.openspaces.org/schema/14.0/core/openspaces-core.xsd
   http://www.openspaces.org/schema/remoting http://www.openspaces.org/schema/remoting/openspaces-remoting.xsd"&gt;

     &lt;context:annotation-config /&gt;

     &lt;os-core:annotation-support /&gt;

     &lt;os-core:embedded-space id="space" space-name="demo-space" /&gt;

     &lt;os-core:giga-space id="gigaSpace" space="space"/&gt;

&lt;bean id="ax-manager" class="com.gigaspaces.analytics_xtreme.server.AnalyticsXtremeManagerFactory"&gt;
     &lt;property name="config"&gt;
          &lt;bean class="com.gigaspaces.analytics_xtreme.AnalyticsXtremeConfigurationFactoryBean"&gt;
               &lt;!-- Verbose is recommended for getting started, usually turned off in production --&gt;
               &lt;property name="verbose" value="true"/&gt;
               &lt;property name="policies"&gt;
                   &lt;list&gt;
                       &lt;ref bean="dataLifecyclePolicy"/&gt;
                   &lt;/list&gt;
               &lt;/property&gt;
               &lt;!-- more properties --&gt;
          &lt;/bean&gt;
     &lt;/property&gt;
&lt;/bean&gt;
	
&lt;!-- Data life cycle policy for Trade class --&gt;
&lt;bean id="dataLifecyclePolicy" class="com.gigaspaces.analytics_xtreme.DataLifecyclePolicyFactoryBean"&gt;
     &lt;property name="typeName" value="com.gigaspaces.demo.Trade"/&gt;
     &lt;property name="timeColumn" value="dateTimeTrade"/&gt;
     &lt;property name="speedPeriod" value="pt5h"/&gt;
     &lt;property name="batchDataSource" ref="ax-datasource"/&gt;
     &lt;property name="batchDataTarget" ref="ax-datatarget"/&gt;
&lt;/bean&gt;

&lt;!-- Register the ax-manager bean as a remote service so clients can load configuration &amp; stats from it --&gt;
&lt;os-remoting:service-exporter id="serviceExporter"&gt;
     &lt;os-remoting:service ref="ax-manager"/&gt;
&lt;/os-remoting:service-exporter&gt;


&lt;!-- Data source plugin based on Spark Hive --&gt;
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataSourceFactoryBean"&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Data target plugin based on Spark Hive --&gt;
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataTargetFactoryBean"&gt;
     &lt;property name="format" value="hive"/&gt;
     &lt;property name="mode" value="append"/&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Spark session provider --&gt;
&lt;bean id="ax-sparkSessionFactory" class="org.insightedge.spark.SparkSessionProviderFactoryBean"&gt;
     &lt;property name="master" value="local[*]"/&gt;
     &lt;property name="enableHiveSupport" value="true"/&gt;
     &lt;property name="configOptions"&gt;
          &lt;map&gt;
               &lt;entry key="hive.metastore.uris" value="thrift://hive-metastore:9083"/&gt;
          &lt;/map&gt;
     &lt;/property&gt;
&lt;/bean&gt;

&lt;!-- To use jdbc instead of spark, replace ax-datasource, ax-datatarget and sparkSessionFactory with these beans
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataSourceFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
&lt;/bean&gt;
	
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataTargetFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
     &lt;property name="useLowerCase" value="true"/&gt;
&lt;/bean&gt;
--&gt;
&lt;/beans&gt;</code><![CDATA[
]]></pre>
        <h1>Limitations</h1>
        <p>Hot deploy&#160;of policy changes/updates is not supported. The Processing Unit must be undeployed and then redeployed.</p>
        <h1>Monitoring</h1>
        <p>You can use the following metrics to monitor how AnalyticsXtreme is functioning.</p>
        <table style="width: 100%;" class="tc-standard">
            <col />
            <col />
            <col />
            <thead>
                <tr>
                    <th>Metric</th>
                    <th>Description</th>
                    <th>Type</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>evicted-entries</td>
                    <td>Total number of data entries evicted from the Space.</td>
                    <td>Long</td>
                </tr>
                <tr>
                    <td>copied-entries</td>
                    <td>Total number of data entries copied to the batch layer (external data storage).</td>
                    <td>Long</td>
                </tr>
                <tr>
                    <td>\{type-name\}\_evicted-entries</td>
                    <td>Total number of type_name data entries evicted from the Space.</td>
                    <td>Long</td>
                </tr>
                <tr>
                    <td>\{type-name\}\_copied-entries</td>
                    <td>Total number of type_name entries copied to the batch layer.</td>
                    <td>Long</td>
                </tr>
                <tr>
                    <td>\{type-name\}\_confirmed-feed-timestamp</td>
                    <td>Last confirmed time stamp of when data entries of type_name were copied to the batch layer.</td>
                    <td>Time stamp</td>
                </tr>
            </tbody>
        </table>
    </body>
</html>