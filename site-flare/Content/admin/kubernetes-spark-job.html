<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1 class="tc-pagetitle">Running a Spark Job in Kubernetes</h1>
        <p>The <MadCap:variable name="General.ProductNameIEP" /> provides a first-class integration between Apache Spark and the <MadCap:variable name="General.ProductNameXAP" /> core data grid capability. This allows hybrid/transactional analytics processing by co-locating Spark jobs in place with low-latency data grid applications. <MadCap:variable name="General.ProductNameIE" /> includes a full Spark distribution.</p>
        <p MadCap:conditions="Version.14-5-died">Apache Spark 2.3.x has native Kubernetes support. Users can run Spark workloads in an existing Kubernetes 1.9+ cluster and take advantage of Apache Spark’s ability to manage distributed data processing tasks.</p>
        <MadCap:snippetBlock src="../Resources/Snippets/Content/Kubernetes/k8s-spark-issue-note.flsnp" MadCap:conditions="Version.15-0-only,Version.14-5-only" />
        <p>The Spark submission mechanism creates a Spark driver running within a Kubernetes pod. The driver creates executors running within Kubernetes pods, connects to them, and executes application code. Using <MadCap:variable name="General.ProductNameIE" />, application code can connect to a Data Pod and interact with the distributed data grid.</p>
        <p>This topic explains how to run the Apache Spark <code>SparkPi</code> example, and the <MadCap:variable name="General.ProductNameIE" /> <code>SaveRDD</code> example, which is one of the basic Scala examples provided in the <MadCap:variable name="General.ProductNameIE" /> software package. Do the following steps, detailed in the following sections, to run these examples in Kubernetes:</p>
        <ol>
            <li>Set the Spark configuration property for the <MadCap:variable name="General.ProductNameIE" /> Docker image.</li>
            <li>Get the Kubernetes Master URL for submitting the Spark jobs to Kubernetes.</li>
            <li>Configure the Kubernetes service account so it can be used by the Driver Pod.</li>
            <li>Deploy a data grid with a headless service (Lookup locator).</li>
            <li>Submit the Spark jobs for the examples.</li>
        </ol>
        <h2>Setting Spark Configuration Property</h2>
        <p><MadCap:variable name="General.ProductNameIE" /> provides a Docker image designed to be used in a container runtime environment, such as Kubernetes. This Docker image is used in the examples below to demonstrate how to submit the Apache Spark <code>SparkPi</code> example and the <MadCap:variable name="General.ProductNameIE" /> <code>SaveRDD</code> example.</p>
        <p>The following Spark configuration property <code>spark.kubernetes.container.image</code> is required when submitting Spark jobs for an <MadCap:variable name="General.ProductNameIE" /> application. Note how this configuration is applied to the examples in the <a href="#submitting-spark-jobs-with-insightedge-submit">Submitting Spark Jobs </a>section:</p><pre><code class="language-bash">--conf spark.kubernetes.container.image=gigaspaces/insightedge-enterprise:[%=Versions.helm-version-MX%]
</code></pre>
        <h2>Getting the Kubernetes Master URL</h2>
        <p>You can get the Kubernetes master URL using kubectl. Type the following command to print out the URL that will be used in the Spark and <MadCap:variable name="General.ProductNameIE" /> examples when submitting Spark jobs to the Kubernetes scheduler.</p><pre><code class="language-bash">kubectl cluster-info
</code></pre>
        <p>Sample output: <code>Kubernetes master is running at https://192.168.99.100:8443</code></p>
        <h2>Configuring the Kubernetes Service Accounts</h2>
        <p>In Kubernetes clusters with RBAC enabled, users can configure Kubernetes RBAC roles and service accounts used by the various Spark jobs on Kubernetes components to access the Kubernetes API server.</p>
        <p>Spark on Kubernetes supports specifying a custom service account for use by the Driver Pod via the configuration property that is passed as part of the submit command. To create a custom service account, run the following kubectl command:</p><pre><code class="language-bash">kubectl create serviceaccount spark
</code></pre>
        <p>After the custom service account is created, you need to grant a service account Role. To grant a service account Role, a RoleBinding is needed. To create a RoleBinding or ClusterRoleBinding, use the kubectl <code>create rolebinding</code> (or <code>clusterrolebinding</code> for ClusterRoleBinding) command. For example, the following command creates an <code>edit</code> ClusterRole in the <code>default</code> namespace and grants it to the <code>spark</code> service account you created above.</p><pre><code>kubectl create clusterrolebinding spark-role --clusterrole=edit --serviceaccount=default:spark --namespace=default
</code></pre>
        <p>After the service account has been created and configured, you can apply it in the Spark submit:</p><pre><code class="language-bash">--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark
</code></pre>
        <h2>Deploying the Data Grid on Kubernetes</h2>
        <p>Run the following Helm command in the command window to start a basic data grid called <code>demo</code>:</p>
        <div><pre MadCap:conditions="Version.15-5-died"><code>helm install insightedge --name demo</code></pre>
            <p MadCap:conditions="Version.15-5-born">
                <div class="easyui-tabs" plain="true">
                    <div title="Helm 3">
                        <p><pre><code>helm install demo insightedge</code></pre>
                        </p>
                    </div>
                    <div title="Helm 2" MadCap:conditions="Version.16-1-died">
                        <p><pre><code>helm install insightedge --name demo</code></pre>
                        </p>
                    </div>
                </div>
            </p>
        </div>
        <p>For the application to connect to the <code>demo</code> data grid, the name of the manager must be provided. This is required when running on a Kubernetes cluster (not a minikube).</p><pre MadCap:conditions="NewSet.Ask RnD"><code class="language-bash">--conf spark.insightedge.space.manager=demo</code></pre>
        <h2><a name="submitting-spark-jobs-with-insightedge-submit"></a>Submitting Spark Jobs with <MadCap:variable name="General.ProductNameIE" /> Submit</h2>
        <p>The <code>insightedge-submit</code> script is located in the <MadCap:variable name="General.ProductNameIE" /> home directory, in <code>insightedge/bin</code>. This script is similar to the <code>spark-submit</code> command used by Spark users to submit Spark jobs. The following examples run both a pure Spark example and an <MadCap:variable name="General.ProductNameIE" /> example by calling this script.</p>
        <h3><a name="sparkpi-example"></a>SparkPi Example</h3>
        <p>Run the following <MadCap:variable name="General.ProductNameIE" /> submit script for the SparkPi example. This example specifies a JAR file with a specific URI that uses the <code>local://</code> scheme. This URI is the location of the example JAR that is already available in the Docker image. If your application’s dependencies are all hosted in remote locations (like HDFS or HTTP servers), you can use the appropriate remote URIs, such as <code>https://path/to/examples.jar</code>.</p><pre><code class="language-bash">./insightedge-submit \
--master k8s://https://192.168.99.100:8443 \
--deploy-mode cluster \
--name spark-pi \
--class org.apache.spark.examples.SparkPi \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
--conf spark.kubernetes.container.image=gigaspaces/insightedge-enterprise:[%=Versions.helm-version-MX%] \
local:///opt/gigaspaces/insightedge/spark/examples/jars/spark-examples_2.11-[%=Versions.spark-version%].jar

</code></pre>
        <p>Refer to the Apache Spark documentation for more configurations that are specific to Spark on Kubernetes. For example, to specify the Driver Pod name, add the following configuration option to the submit command:</p><pre><code class="language-bash">--conf spark.kubernetes.driver.pod.name=spark-pi-driver
</code></pre>
        <h3>SaveRDD Example</h3>
        <p>Run the following <MadCap:variable name="General.ProductNameIE" /> submit script for the SaveRDD example, which generates "N" products, converts them to RDD, and saves them to the data grid. This example has the following configuration:</p>
        <ul>
            <li>The –master has the prefix <code>k8s://&lt;Kubernetes Master URL&gt;:&lt;port&gt;</code>.</li>
            <li>The <code>spark.insightedge.space.lookup.locator</code> is set with the headless service of the Manager Pod (&lt;release name&gt;-insightedge-manager-hs).</li>
            <li>The example lookup is the default Space called <code>demo</code>.</li>
            <li>In Kubernetes clusters with RBAC enabled, the service account must be set (e.g. <code>serviceAccountName=spark</code>).</li>
            <li>The <code>spark.kubernetes.container.image</code> is set with the desired Docker image (This is usually of the form <code>gigaspaces/insightedge-enterprise:1.0.0</code>).</li>
        </ul><pre><code class="language-bash">./insightedge-submit \
--master k8s://https://192.168.99.100:8443 \
--deploy-mode cluster \
--name i9e-saveRdd \
--class org.insightedge.examples.basic.SaveRdd \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
--conf spark.kubernetes.container.image=gigaspaces/insightedge-enterprise:[%=Versions.helm-version-MX%] \
--conf spark.insightedge.space.manager=demo \
local:///opt/gigaspaces/insightedge/examples/jars/insightedge-examples.jar

</code></pre>
        <p>Use the GigaSpaces CLI to query the number of objects in the <code>demo</code> data grid. The output should show <code>100,000</code> objects of type <code>org.insightedge.examples.basic.Product</code>.</p>
        <p>Port <code>8090</code> is exposed as the load balancer port <code>demo-insightedge-manager-service:<MadCap:conditionalText MadCap:conditions="Version.15-0-died">30990</MadCap:conditionalText><MadCap:conditionalText MadCap:conditions="Version.15-0-born">9090</MadCap:conditionalText>TCP</code>, and should be specified as part of the <code>--server</code> option. Navigate to the product bin directory and type the following CLI command:</p>
        <div class="tc-admon-note" MadCap:conditions="Version.14-5-died">
            <MadCap:snippetBlock src="../Resources/Snippets/InsighedgeXap.flsnp" />
        </div>
        <div class="easyui-tabs" plain="true" data-options="">
            <div title="Linux/Mac" style="padding:10px"><pre><code class="language-bash">./<MadCap:variable name="General.CLI-script-Linux" /> --server 192.168.99.100:<MadCap:conditionalText MadCap:conditions="Version.15-0-died">30990</MadCap:conditionalText><MadCap:conditionalText MadCap:conditions="Version.15-0-born">9090</MadCap:conditionalText>space info --type-stats demo </code></pre>
            </div>
            <div title="Windows" style="padding:10px"><pre><code class="language-bash"><MadCap:variable name="General.CLI-script-Windows" /> --server 192.168.99.100:<MadCap:conditionalText MadCap:conditions="Version.15-0-died">30990</MadCap:conditionalText><MadCap:conditionalText MadCap:conditions="Version.15-0-born">9090</MadCap:conditionalText>space info --type-stats demo </code></pre>
            </div>
        </div>
        <p>The <code>insightedge-submit</code> script accepts any Space name when running an <MadCap:variable name="General.ProductNameIE" /> example in Kubernetes, by  adding the configuration property: <code>--conf spark.insightedge.space.name=&lt;space name&gt;</code>.</p>
        <p>For example, the Helm commands below will install the following stateful sets: <code>testmanager-insightedge-manager</code>, <code>testmanager-insightedge-zeppelin</code>, <code>testspace-demo-*\[i\]*</code></p>
        <p>The <MadCap:variable name="General.ProductNameIE" /> submit command will submit the SaveRDD example with the <code>testspace</code> and <code>testmanager</code> configuration parameters.</p>
        <div><pre MadCap:conditions="Version.15-5-died"><code>$ helm install insightedge-manager --name testmanager

$ helm install insightedge-zeppelin --name testzeppelin

$ helm install insightedge-pu --name testspace --set manager.name=testmanager</code></pre>
            <p MadCap:conditions="Version.15-5-born">
                <div class="easyui-tabs" plain="true">
                    <div title="Helm 3">
                        <p><pre><code>$ helm install testmanager insightedge-manager

$ helm install testzeppelin insightedge-zeppelin

$ helm install testspace insightedge-pu --set manager.name=testmanager  </code></pre>
                        </p>
                    </div>
                    <div title="Helm 2" MadCap:conditions="Version.16-1-died">
                        <p><pre><code>$ helm install insightedge-manager --name testmanager

$ helm install insightedge-zeppelin --name testzeppelin

$ helm install insightedge-pu --name testspace --set manager.name=testmanager</code></pre>
                        </p>
                    </div>
                </div>
            </p>
        </div><pre><code class="language-bash">$ ./insightedge-submit \
--master k8s://https://192.168.99.100:8443 \
--deploy-mode cluster \
--name i9e-saveRdd \
--class org.insightedge.examples.basic.SaveRdd \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
--conf spark.kubernetes.container.image=gigaspaces/insightedge-enterprise:[%=Versions.helm-version-MX%] \
--conf spark.insightedge.space.name=testspace \
--conf spark.insightedge.space.manager=testmanager \
local:///opt/gigaspaces/insightedge/examples/jars/insightedge-examples.jar
</code></pre>
    </body>
</html>