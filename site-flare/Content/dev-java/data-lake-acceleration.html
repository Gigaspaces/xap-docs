<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1>Data Lake Acceleration</h1>
        <p MadCap:conditions="Default.DoNotShow"><MadCap:variable name="General.ProductNameIE" /> only</p>
        <h1>Overview</h1>
        <p MadCap:conditions="Default.DoNotShow">Big data adoption in enterprises is increasing all the time, with no sign of stopping. The Apache Spark MLlib&#160;and&#160;Tensorflow&#160;are among the most-adopted big data analytics platforms, while Cloudera,&#160;Amazon EMR,&#160;Hortonworks, and&#160;MapR&#160;are popular big data distributions. Patching all these technologies together into the classic Lambda architecture presents a number of customer challenges. </p>
        <ul MadCap:conditions="Default.DoNotShow">
            <li>Increasing system complexity - traditional Lambda architectures simply keep adding components as new technologies come into play, which slows down queries and is difficult to maintain.</li>
            <li>Data freshness - New data may only be accessible to users once every X hours, which is too slow to make real-time decisions.</li>
            <li>One-way, immutable data flow - If data contains a lot of update operations, and big data platforms such as HDFS and Parquet don't support data updates, all ingestion jobs needed to create new snapshots.</li>
        </ul>
        <p>AnalyticsXtreme is a data lake accelerator that  operationalizes your data lake for real-time analytics, which can run simultaneously on both real-time, mutable streaming data and on historical data that is stored on data lakes based on Hadoop, Amazon S3 or Azure Blob Storage, without exposing a separate data load procedure or data duplication.  Moving from on-premise to the cloud, or changing technology stacks for example from Cloudera to Amazon S3, is seamless to machine learning applications; increasing flexibility while reducing development and maintenance. </p>
        <p>With AnalyticsXtreme, your data is available for immediate searching, queries, and running analytics; there is a single logical view for hot, warm and cold data. The hot data resides on <MadCap:variable name="General.ProductNameIE" />'s in-memory data grid, while cold (historical) data can be stored on any big-data platform such as HDFS or Amazon S3. Additionally, the hot data is mutable, supporting real-time updates. The data becomes immutable when it is stored on the external big data platform.</p>
        <p>&#160;</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/AnalyticsXtreme.png" class="tc-picture80" />
            </p>
        </div>
        <p>This approach enables smooth access to frequently used historical data, because applications can access any data - hot or cold - via a unified layer using Spark SQL or JDBC. You can easily integrate BI&#160;tools such as <a href="tableau.html">Tableau</a>, Looker, and PowerBI.</p>
        <p> AnalyticsXtreme provides automatic life cycle management, handling the underlying data movement, optimization and deletion using an internal data life cycle policy.</p>
        <h1>Implementation</h1>
        <p>AnalyticsXtreme is a time-based feature that can be used in either automatic data tiering mode, or in external data tiering mode. </p>
        <p>In automatic data tiering mode, AnalyticsXtreme is implemented as shown above. Data is streamed to the speed layer, and from that point on it is managed by the feature's data life cycle policy as it ages and eventually gets moved to the external data source, based on the life cycle that was defined in the policy. </p>
        <p>In external data tiering mode, data is streamed to both the <MadCap:variable name="General.ProductNameIE" /> data grid and the external data source in parallel. This means that AnalyticsXtreme provides only the speed layer for the purpose of facilitating accelerated query results, so the data life cycle policy only needs to manage the data until it reaches its expiration point. After it expires, the data is evicted from the data grid.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/AnalyticsXtreme-mode2.png" class="tc-picture80" />
            </p>
        </div>
        <h2>Supported Data Formats</h2>
        <p>AnalyticsXtreme supports all the data formats that are supported by Apache Spark, such as Apache Parquet and Apache Avro.</p>
        <h2>Supported APIs</h2>
        <p>AnalyticsXtreme supports Spark/SQL and JDBC for querying the speed and batch layers via the <MadCap:variable name="General.ProductNameIE" /> JDBC&#160;driver.</p>
        <h1>Data Life Cycle Policy</h1>
        <p>An important function of AnalyticsXtreme is managing the life cycle of the data from the moment it is streamed to the <MadCap:variable name="General.ProductNameIE" /> data grid. In automatic data tiering mode, this includes moving the data from the speed layer (data grid) to the batch layer (external data storage) as it ages and becomes cold. In external data tiering mode, this means evicting the data when it reaches the end of the life cycle. </p>
        <p>In order to handle this data transfer or deletion transparently, several things must to be taken into consideration. For example, the business application may trigger a query on the data at any point in time, and so needs to know where the data is located in order to successfully complete the query and return accurate results. The query may be complex, and therefore may take a relatively long time to complete. Additionally, there may be remote clients sending their queries, which means network latency needs to be taken into account. And finally, if the network connection isn't stable, the latency period may be even longer for some queries before they are finally received and executed.</p>
        <p>The data life cycle policy was designed to handle this movement of data from the speed layer to the batch layer in a safe and predictable way. For example, we may have a system where data that is up to 5 hours old is considered hot and should be held in the speed layer, while anything older is considered cold and therefore must be moved to the batch layer. This 5-hour interval is the <code>speedPeriod </code>. The end of the <code>speedPeriod </code>is the query threshold; if a query is sent at 6 PM that requires data up to 5 hours old, the query threshold is 1 PM, and the query manager will look for the data in the speed layer only. If the query needs data that is more than 5 hours old, the query manager will look for that data in either the batch layer only, or in both the speed and batch layers and combine the results.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-1-speedperiod.png" class="tc-picture80" />
            </p>
        </div>
        <p>When the data is in the speed layer, it is dynamic and can be updated as necessary. When it is in the batch layer, the data is immutable. As the data nears the end of the <code>speedPeriod</code>, the data life cycle policy has to prepare for moving it to the batch layer. Therefore, the policy includes a <code>mutabilityPeriod</code>, during which time the data remains fully dynamic. When the data ages out of the <code>mutabilityPeriod</code>, it becomes immutable so that it is ready to be moved to the batch layer. By default, the <code>mutabilityPeriod </code>is set to 80% of the <code>speedPeriod</code>; looking at our example, if the <code>speedPeriod </code>is 5 hours, then the <code>mutabilityPeriod </code>is 4 hours, and data that is between 4-5 hours old is in an immutable window.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-2-mutabilityperiod.png" class="tc-picture80" />
            </p>
        </div>
        <p>In order to keep system performance consistent, and to ensure that the data can be easily verified when it is moved between layers, if AnalyticsXtreme has been implemented in automatic data tiering mode, the data life cycle policy copies the data from the speed layer to the batch layer in small chunks as it nears the end of the immutable window, according to the <span class="tc-italic">batchFeedInterval</span>. At this point, the aging data exists both in the speed layer and in the batch layer.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-3-batchfeed.png" class="tc-picture80" />
            </p>
        </div>
        <p>After the aging data is residing safely to the batch layer and the <code>speedPeriod </code>expires, the data needs to be evicted from the speed layer. However, since the query threshold is a sliding window, a small safety margin is needed to ensure that long-running queries can complete, and to account for network latency regarding remote clients that may have sent queries before the <code>speedPeriod </code>for that data expired. This safety margin is the <span class="tc-italic">evictionBuffer</span>, set by default to 10 minutes.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-4-eviction.png" class="tc-picture80" />
            </p>
        </div>
        <p>After the data is evicted from the speed layer, it exists as historical data in the batch layer. Any queries that need data that is older than the query threshold (in this example, 1 PM) will access the batch layer only.</p>
        <h1>Configuring AnalyticsXtreme</h1>
        <p> AnalyticsXtreme is implemented in conjunction with a specific Space. This is part of the standard configuration of a pu.xml file.</p>
        <div class="tc-admon-note">
            <p>For more information about pu.xml files and their properties, read the <MadCap:xref href="configuring-processing-unit-elements.html">Configuration</MadCap:xref> topic in <MadCap:xref href="the-processing-unit-overview.html">The Processing Unit</MadCap:xref> section of the developer guide.</p>
        </div>
        <p>Configuring AnalyticsXtreme involves the following steps:</p>
        <ol>
            <li>Defining the Space.</li>
            <li>Setting up the AnalyticsXtreme Manager (this includes configuring the data life cycle policy).</li>
            <li>Defining the data source and the data target.</li>
            <li>Exporting the AnalyticsXtreme Manager service for discovery (so that client applications can access it).</li>
        </ol>
        <p>The Space definition is part of the standard pu.xml configuration as described in <MadCap:xref href="the-processing-unit-overview.html">The Processing Unit</MadCap:xref> section of the developer guide. The rest of the steps, which are specific to AnalyticsXtreme, are explained in the sections below.</p>
        <h2>Setting Up the AnalyticsXtreme Manager</h2>
        <p>As mentioned above,  all of the relevant information about AnalyticsXtreme is provided to the data grid in a dedicated pu.xml file. The first step in the configuration process is creating an AnalyticsXtreme Manager bean, with a bean ID and class. In this part of the pu.xml, you can also modify the AnalyticsXtreme logging policy configuration if necessary.</p>
        <p>In the following code snippet, taken from the full pu.xml file, the AnalyticsXtreme Manager is assigned a bean ID of <code>ax-manager</code>, and the logging policy has been changed from its default value.</p><pre><code class="language-xml">&lt;bean id="ax-manager" class="com.gigaspaces.analytics_xtreme.server.AnalyticsXtremeManagerFactory"&gt;
     &lt;property name="config"&gt;
          &lt;bean class="com.gigaspaces.analytics_xtreme.AnalyticsXtremeConfigurationFactoryBean"&gt;
               &lt;!-- Verbose is recommended for getting started, usually turned off in production --&gt;
               &lt;property name="verbose" value="true"/&gt;
               &lt;!-- more properties --&gt;
          &lt;/bean&gt;
     &lt;/property&gt;
&lt;/bean&gt;</code></pre>
        <h3>Global Properties</h3>
        <p>The global configuration contains a list of DataLifecyclePolicy properties, as well as the AnalyticsXtreme logging level that is set for the entire application.</p>
        <table style="width: 100%;" class="tc-standard">
            <col />
            <col />
            <col />
            <col />
            <col />
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Description</th>
                    <th>Unit</th>
                    <th>Default Value</th>
                    <th>Required/Optional</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>verbose </td>
                    <td>Increases the log levels for both the client and the server to provide verbose AnalyticsXtreme information (useful for troubleshooting).</td>
                    <td>True/False</td>
                    <td>False</td>
                    <td>Optional</td>
                </tr>
            </tbody>
        </table>
        <h2>Configuring the Data Life Cycle Policy</h2>
        <p>The next step in configuring AnalyticsXtreme is to define the data life cycle policy, which specifies how and when data is archived from the data grid (speed layer) to the external data storage (batch layer). This policy is configured in the AnalyticsXtreme Manager for each data object, or table. You must define the following properties in the policy: </p>
        <ul>
            <li><code>typeName</code>
            </li>
            <li><code>timeColumn</code>
            </li>
            <li><code>speedPeriod</code>
            </li>
            <li><code>batchDataSource</code>
            </li>
        </ul>
        <p>All other properties are optional, and you can leave the default values unless your specific environment has different requirements. See <MadCap:xref href="#Table">Table (Object) Properties</MadCap:xref> below for a full list of the data life cycle policy properties and their descriptions.</p>
        <h3>Defining the Data Source and Data Target</h3>
        <p>Part of configuring the data life cycle policy is defining the <code>batchDataSource </code>property. You can define it directly, or you can use a <code>ref </code>to point to the definition. If you are implementing AnalyticsXtreme in external data tiering mode, this is the only required property because the data is simply evicted at the end of the life cycle.</p>
        <p>If you are implementing AnalyticsXtreme in automatic data tiering mode, you must also define the <code>batchDataTarget </code>property using the same method. Towards the end of the <code>speedPeriod </code>, the data will be moved to the target defined here.</p>
        <div class="tc-admon-note">
            <p>You can define a custom data  target, as explained below in <MadCap:xref href="#Adding">Adding a Custom Batch Data Target</MadCap:xref>.</p>
        </div>
        <h3>Example</h3>
        <p>The following code snippet from the pu.xml file shows a data life cycle policy that was configured for specific trading information that needs to be stored in the data grid for 5 hours, and then moved to external data storage. Both the data source and the data target have their full definitions in another area of the pu.xml file.</p><pre><code class="language-xml">&lt;!-- Data life cycle policy for Trade class --&gt;
&lt;bean class="com.gigaspaces.analytics_xtreme.DataLifecyclePolicyFactoryBean"&gt;
     &lt;property name="typeName" value="com.gigaspaces.demo.Trade"/&gt;
     &lt;property name="timeColumn" value="dateTimeTrade"/&gt;
     &lt;property name="speedPeriod" value="pt5h"/&gt;
     &lt;property name="batchDataSource" ref="ax-datasource"/&gt;
     &lt;property name="batchDataTarget" ref="ax-datatarget"/&gt;
&lt;/bean&gt;</code></pre>
        <h3><a name="Table"></a>Table (Object) Properties</h3>
        <p>The DataLifecyclePolicy table contains the following parameters. The first four properties are required. The others are optional, and it is recommended to leave the default values as is unless your business application environment has specific requirements.</p>
        <table style="width: 100%;" class="tc-standard">
            <col />
            <col />
            <col />
            <col />
            <col />
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Description</th>
                    <th>Unit</th>
                    <th>Default Value</th>
                    <th>Required/Optional</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>typeName*</td>
                    <td>Name of the table/type/class to which this policy applies.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>timeColumn** </td>
                    <td>Name of column/property/field that contains the time data used to manage this policy.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>speedPeriod </td>
                    <td>Time period or fixed timestamp.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>batchDataSource </td>
                    <td>Endpoint for querying the batch layer</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>batchDataTarget </td>
                    <td>Endpoint for feeding data to the batch layer. Only necessary when AnalyticsXtreme is implemented in Automatic data tiering mode.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>timeFormat </td>
                    <td>Time format for the data in the timeColumn parameter.</td>
                    <td>See <MadCap:xref href="#Java">Java Time Pattern</MadCap:xref> below</td>
                    <td>&#160;</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>mutabilityPeriod </td>
                    <td>Period of time during which data can be updated.</td>
                    <td> Duration in time units or % of speedPeriod.</td>
                    <td>80%</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>batchFeedInterval </td>
                    <td>Data is fed from the Space (speed layer) to the batch layer in these time-based intervals at the end of the speedPeriod, after the mutabilityPeriod has expired.</td>
                    <td>Minutes (m)</td>
                    <td>1 m</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>batchFeedSize </td>
                    <td>Maximum data entries per batch feed interval.</td>
                    <td>&#160;</td>
                    <td>
                        <p class="tc-tablebody">1000 </p>
                    </td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>evictionPollingInterval </td>
                    <td>Polling interval for querying and evicting each policy.</td>
                    <td>Minutes (m)</td>
                    <td>1 m</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>evictionBuffer </td>
                    <td>Additional waiting period before evicting data from the Space (speed layer)&#160;after it was fed to the batch layer, so that long queries and clock differences won't cause errors or generate exceptions. </td>
                    <td>Duration in time units or % of speedPeriod.</td>
                    <td>10 m</td>
                    <td>Optional</td>
                </tr>
            </tbody>
        </table>
        <p>* This correlates to an object or JSON in the object store.</p>
        <p>** This correlates to a property or entity in an object or JSON.</p>
        <h3><a name="Java"></a>Java Time Pattern</h3>
        <p>AnalyticsXtreme is a time-based feature, and therefore uses the Java time pattern (required as of Java 8) in the <code>timeFormat</code> property. This time pattern requires use of the letter "p" in either upper or lower case in all duration fields, as well as the letter "t" for any duration that is less than 24 hours long.</p>
        <p>For example, <code>pt15m</code> represents a duration of 15 minutes, and <code>pt10h</code> represents a duration of 10 hours. A duration of 2 days should be written as <code>p2d</code>.</p>
        <p>For more information about the Java time pattern, see the <a href="https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-" target="_blank">Java API documentation</a>.</p>
        <h2>Exporting the AnalyticsXtreme Manager Service</h2>
        <p>In order for client applications to access the speed layer, the AnalyticsXtreme Manager needs to be exported  to enable discovery. This mechanism leverages the data grid's remoting mechanism, which registers the AnalyticsXtreme Manager as a remote service.</p>
        <p>The following code snippet demonstrates how to implement this mechanism in the pu.xml.</p><pre><code class="language-xml">&lt;!-- Register the ax-manager bean as a remote service so clients can load configuration &amp; stats from it --&gt;
&lt;os-remoting:service-exporter id="serviceExporter"&gt;
     &lt;os-remoting:service ref="ax-manager"/&gt;
&lt;/os-remoting:service-exporter&gt;</code></pre>
        <div class="tc-admon-note">
            <p>For more information about the data grid's remoting mechanism, see the <MadCap:xref href="space-based-remoting-overview.html">Space-Based Remoting</MadCap:xref> section of the developer guide.</p>
        </div>
        <h1><a name="Adding"></a>Adding a Custom Batch Data Target</h1>
        <p>AnalyticsXtreme supports HDFS and Amazon S3 without requiring any custom configuration. However, you can add any external data source that is supported by Spark. </p>
        <p>The following code snippet demonstrates how to configure AnalyticsXtreme to work with Hive.</p><pre><code class="language-xml">&lt;!-- Data source plugin based on Spark Hive --&gt;
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataSourceFactoryBean"&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Data target plugin based on Spark Hive --&gt;
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataTargetFactoryBean"&gt;
     &lt;property name="format" value="hive"/&gt;
     &lt;property name="mode" value="append"/&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Spark session provider --&gt;
&lt;bean id="ax-sparkSessionFactory" class="org.insightedge.spark.SparkSessionProviderFactoryBean"&gt;
     &lt;property name="master" value="local[*]"/&gt;
     &lt;property name="enableHiveSupport" value="true"/&gt;
     &lt;property name="configOptions"&gt;
          &lt;map&gt;
               &lt;entry key="hive.metastore.uris" value="thrift://hive-metastore:9083"/&gt;
          &lt;/map&gt;
     &lt;/property&gt;
&lt;/bean&gt;</code><![CDATA[
]]></pre>
        <h1>Using JDBC&#160;with AnalyticsXtreme </h1>
        <p>AnalyticsXtreme is configured to work with Spark on the <code>batchDataSource </code>and <code>batchDataTarget</code>. However, GigaSpaces also provides a JDBC&#160;interpeter in the InsightEdge Apache Zeppelin notebook that is packaged with the product. Additionally, you can use the <MadCap:variable name="General.ProductNameIE" /> JDBC driver for SQL and Spark queries.</p>
        <p>In order to configure these features to work with AnalyticsXtreme, you need to add the <code>analyticsXtreme.enabled=true</code> property to the JDBC connection string. </p>
        <div class="tc-admon-note">
            <p>For more information about the InsightEdge Apache Zeppelin notebook and the JDBC&#160;interpreter, see the <MadCap:xref href="../started/insightedge-zeppelin.html">Using the [%=General.ProductNameIE%] Web Notebook</MadCap:xref> topic. For more information about the JDBC driver, see the <MadCap:xref href="sql-driver.html">SQL Driver</MadCap:xref> topic in the <MadCap:xref href="sql-query-intro.html">[%=General.ProductNameIE%] JDBC&#160;Driver</MadCap:xref> section.</p>
        </div>
        <p>The following code snippet demonstrates how to configure AnalyticsXtreme to use JDBC; it replaces the referenced data source, data target, and Spark session information in the pu.xml. </p><pre><code class="language-xml">&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataSourceFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
&lt;/bean&gt;
	
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataTargetFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
     &lt;property name="useLowerCase" value="true"/&gt;
&lt;/bean&gt;</code></pre>
        <h1>Sample pu.xml File</h1>
        <p>The following pu.xml file shows all the above steps, including defining a Space. </p><pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;beans xmlns="http://www.springframework.org/schema/beans"
     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
     xmlns:context="http://www.springframework.org/schema/context"
     xmlns:os-core="http://www.openspaces.org/schema/core"
     xmlns:os-remoting="http://www.openspaces.org/schema/remoting"
     xsi:schemaLocation="
   http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd
   http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd
   http://www.openspaces.org/schema/core http://www.openspaces.org/schema/14.0/core/openspaces-core.xsd
   http://www.openspaces.org/schema/remoting http://www.openspaces.org/schema/remoting/openspaces-remoting.xsd"&gt;

     &lt;context:annotation-config /&gt;

     &lt;os-core:annotation-support /&gt;

     &lt;os-core:embedded-space id="space" space-name="demo-space" /&gt;

     &lt;os-core:giga-space id="gigaSpace" space="space"/&gt;


&lt;bean id="ax-manager" class="com.gigaspaces.analytics_xtreme.server.AnalyticsXtremeManagerFactory"&gt;
     &lt;property name="config"&gt;
          &lt;bean class="com.gigaspaces.analytics_xtreme.AnalyticsXtremeConfigurationFactoryBean"&gt;
               &lt;!-- Verbose is recommended for getting started, usually turned off in production --&gt;
               &lt;property name="verbose" value="true"/&gt;
               &lt;!-- more properties --&gt;
          &lt;/bean&gt;
     &lt;/property&gt;
&lt;/bean&gt;
	
&lt;!-- Data life cycle policy for Trade class --&gt;
&lt;bean class="com.gigaspaces.analytics_xtreme.DataLifecyclePolicyFactoryBean"&gt;
     &lt;property name="typeName" value="com.gigaspaces.demo.Trade"/&gt;
     &lt;property name="timeColumn" value="dateTimeTrade"/&gt;
     &lt;property name="speedPeriod" value="pt5h"/&gt;
     &lt;property name="batchDataSource" ref="ax-datasource"/&gt;
     &lt;property name="batchDataTarget" ref="ax-datatarget"/&gt;
&lt;/bean&gt;

&lt;!-- Register the ax-manager bean as a remote service so clients can load configuration &amp; stats from it --&gt;
&lt;os-remoting:service-exporter id="serviceExporter"&gt;
     &lt;os-remoting:service ref="ax-manager"/&gt;
&lt;/os-remoting:service-exporter&gt;


&lt;!-- Data source plugin based on Spark Hive --&gt;
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataSourceFactoryBean"&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Data target plugin based on Spark Hive --&gt;
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataTargetFactoryBean"&gt;
     &lt;property name="format" value="hive"/&gt;
     &lt;property name="mode" value="append"/&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Spark session provider --&gt;
&lt;bean id="ax-sparkSessionFactory" class="org.insightedge.spark.SparkSessionProviderFactoryBean"&gt;
     &lt;property name="master" value="local[*]"/&gt;
     &lt;property name="enableHiveSupport" value="true"/&gt;
     &lt;property name="configOptions"&gt;
          &lt;map&gt;
               &lt;entry key="hive.metastore.uris" value="thrift://hive-metastore:9083"/&gt;
          &lt;/map&gt;
     &lt;/property&gt;
&lt;/bean&gt;

&lt;!-- To use jdbc instead of spark, replace ax-datasource, ax-datatarget and sparkSessionFactory with these beans
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataSourceFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
&lt;/bean&gt;
	
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataTargetFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
     &lt;property name="useLowerCase" value="true"/&gt;
&lt;/bean&gt;
--&gt;
&lt;/beans&gt;</code><![CDATA[
]]></pre>
        <h1 MadCap:conditions="Default.DoNotShow">Limitations</h1>
        <p MadCap:conditions="Default.DoNotShow">Gaps: Join (Ayelet)</p>
        <p MadCap:conditions="Default.DoNotShow">TBD</p>
    </body>
</html>