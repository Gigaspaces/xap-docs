<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1>Data Lake Acceleration</h1>
        <p MadCap:conditions="Default.DoNotShow"><MadCap:variable name="General.ProductNameIE" /> only</p>
        <h1>Overview</h1>
        <p>AnalyticsXtreme is a data lake accelerator that  operationalizes your data lake for real-time analytics, which can run simultaneously on both real-time, mutable streaming data and on historical data that is stored on data lakes based on Hadoop, Amazon S3 or Azure Blob Storage, without exposing a separate data load procedure or data duplication.  Moving from on-premise to the cloud, or changing technology stacks for example from Cloudera to Amazon S3, is seamless to machine learning applications; increasing flexibility while reducing development and maintenance. </p>
        <p>With AnalyticsXtreme, your data is available for immediate searching, queries, and running analytics; there is a single logical view for hot, warm and cold data. The hot data resides on <MadCap:variable name="General.ProductNameIE" />'s in-memory data grid, while cold (historical) data can be stored on any big-data platform such as HDFS or Amazon S3. Additionally, the hot data is mutable, supporting real-time updates. The data becomes immutable when it is stored on the external big data platform.</p>
        <p>&#160;</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/AnalyticsXtreme.png" class="tc-picture80" />
            </p>
        </div>
        <p>This approach enables smooth access to frequently used historical data, because applications can access any data - hot or cold - via a unified layer using Spark SQL or JDBC. You can easily integrate BI&#160;tools such as <a href="tableau.html">Tableau</a>, Looker, and PowerBI.</p>
        <p> AnalyticsXtreme provides automatic life cycle management, handling the underlying data movement, optimization and deletion using an internal data life cycle policy.</p>
        <h1>Implementation</h1>
        <p>AnalyticsXtreme is a time-based feature that can be used in either automatic data tiering mode, or in external data tiering mode. </p>
        <p>In automatic data tiering mode, AnalyticsXtreme is implemented as shown above. Data is streamed to the speed layer, and from that point on it is managed by the feature's data life cycle policy as it ages and eventually gets moved to the external data source, based on the life cycle that was defined in the policy. </p>
        <p>In external data tiering mode, data is streamed to both the <MadCap:variable name="General.ProductNameIE" /> data grid and the external data source in parallel. This means that AnalyticsXtreme provides only the speed layer for the purpose of facilitating accelerated query results, so the data life cycle policy only needs to manage the data until it reaches its expiration point. After it expires, the data is evicted from the data grid.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/AnalyticsXtreme-mode2.png" class="tc-picture80" />
            </p>
        </div>
        <h2>Supported Data Formats</h2>
        <p>AnalyticsXtreme supports all the data formats that are supported by Apache Spark, such as Apache Parquet and Apache Avro.</p>
        <h2>Supported APIs</h2>
        <p>AnalyticsXtreme supports Spark/SQL and JDBC for querying the speed and batch layers via the <MadCap:variable name="General.ProductNameIE" /> JDBC&#160;driver.</p>
        <h1>Data Life Cycle Policy</h1>
        <p>An important function of AnalyticsXtreme is managing the life cycle of the data from the moment it is streamed to the <MadCap:variable name="General.ProductNameIE" /> data grid. In automatic data tiering mode, this includes moving the data from the speed layer (data grid) to the batch layer (external data storage) as it ages and becomes cold. In external data tiering mode, this means evicting the data when it reaches the end of the life cycle. </p>
        <p>In order to handle this data transfer or deletion transparently, several things must to be taken into consideration. For example, the business application may trigger a query on the data at any point in time, and so needs to know where the data is located in order to successfully complete the query and return accurate results. The query may be complex, and therefore may take a relatively long time to complete. Additionally, there may be remote clients sending their queries, which means network latency needs to be taken into account. And finally, if the network connection isn't stable, the latency period may be even longer for some queries before they are finally received and executed.</p>
        <p>The data life cycle policy was designed to handle this movement of data from the speed layer to the batch layer in a safe and predictable way. For example, we may have a system where data that is up to 5 hours old is considered hot and should be held in the speed layer, while anything older is considered cold and therefore must be moved to the batch layer. This 5-hour interval is the <code>speedPeriod</code>. The end of the <code>speedPeriod</code> is the query threshold; if a query is sent at 6 PM that requires data up to 5 hours old, the query threshold is 1 PM, and the query manager will look for the data in the speed layer only. If the query needs data that is more than 5 hours old, the query manager will look for that data in either the batch layer only, or in both the speed and batch layers and combine the results.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-1-speedperiod.png" class="tc-picture80" />
            </p>
        </div>
        <p>When the data is in the speed layer, it is dynamic and can be updated as necessary. When it is in the batch layer, the data is immutable. As the data nears the end of the <code>speedPeriod</code>, the data life cycle policy has to prepare for moving it to the batch layer. Therefore, the policy includes a <code>mutabilityPeriod</code>, during which time the data remains fully dynamic. When the data ages out of the <code>mutabilityPeriod</code>, it becomes immutable so that it is ready to be moved to the batch layer. By default, the <code>mutabilityPeriod</code> is set to 80% of the <code>speedPeriod</code>; looking at our example, if the <code>speedPeriod</code> is 5 hours, then the <code>mutabilityPeriod</code> is 4 hours, and data that is between 4-5 hours old is in an immutable window.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-2-mutabilityperiod.png" class="tc-picture80" />
            </p>
        </div>
        <p>In order to keep system performance consistent, and to ensure that the data can be easily verified when it is moved between layers, if AnalyticsXtreme has been implemented in automatic data tiering mode, the data life cycle policy copies the data from the speed layer to the batch layer in small chunks as it nears the end of the immutable window, according to the <span class="tc-italic">batchFeedInterval</span>. At this point, the aging data exists both in the speed layer and in the batch layer.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-3-batchfeed.png" class="tc-picture80" />
            </p>
        </div>
        <p>After the aging data is residing safely to the batch layer and the <code>speedPeriod</code> expires, the data needs to be evicted from the speed layer. However, since the query threshold is a sliding window, a small safety margin is needed to ensure that long-running queries can complete, and to account for network latency regarding remote clients that may have sent queries before the <code>speedPeriod</code> for that data expired. This safety margin is the <span class="tc-italic">evictionBuffer</span>, set by default to 10 minutes.</p>
        <div class="tc-align-center">
            <p>
                <img src="../Resources/Static/attachment_files/lambda/ax-4-eviction.png" class="tc-picture80" />
            </p>
        </div>
        <p>After the data is evicted from the speed layer, it exists as historical data in the batch layer. Any queries that need data that is older than the query threshold (in this example, 1 PM) will access the batch layer only.</p>
        <h1>Configuring AnalyticsXtreme on the Server Side</h1>
        <p> AnalyticsXtreme is implemented in conjunction with a specific Space. This is part of the standard configuration of a pu.xml file.</p>
        <div class="tc-admon-note">
            <p>For more information about pu.xml files and their properties, read the <MadCap:xref href="configuring-processing-unit-elements.html">Configuration</MadCap:xref> topic in <MadCap:xref href="the-processing-unit-overview.html">The Processing Unit</MadCap:xref> section of the developer guide.</p>
        </div>
        <p>Configuring AnalyticsXtreme on the server side involves the following steps:</p>
        <ol>
            <li>Defining the Space.</li>
            <li>Setting up the AnalyticsXtreme Manager</li>
            <li> Configuring the data life cycle policy.</li>
            <li>Defining the data source and the data target.</li>
            <li>Exporting the AnalyticsXtreme Manager service for discovery (so that client applications can access it).</li>
        </ol>
        <p>The Space definition is part of the standard pu.xml configuration as described in <MadCap:xref href="the-processing-unit-overview.html">The Processing Unit</MadCap:xref> section of the developer guide. The rest of the steps, which are specific to AnalyticsXtreme, are explained in the sections below.</p>
        <h1>Setting Up the AnalyticsXtreme Manager</h1>
        <p>As mentioned above,  all of the relevant information about AnalyticsXtreme is provided to the data grid in a dedicated pu.xml file. The first step in the configuration process is creating an AnalyticsXtreme Manager bean, with a bean ID and class. In this part of the pu.xml, you can also modify the AnalyticsXtreme logging policy configuration if necessary.</p>
        <p>In the following code snippet, taken from the full pu.xml file, the AnalyticsXtreme Manager is assigned a bean ID of <code>ax-manager</code>, and the logging policy has been changed from its default value.</p><pre><code class="language-xml">&lt;bean id="ax-manager" class="com.gigaspaces.analytics_xtreme.server.AnalyticsXtremeManagerFactory"&gt;
     &lt;property name="config"&gt;
          &lt;bean class="com.gigaspaces.analytics_xtreme.AnalyticsXtremeConfigurationFactoryBean"&gt;
               &lt;!-- Verbose is recommended for getting started, usually turned off in production --&gt;
               &lt;property name="verbose" value="true"/&gt;
               &lt;!-- more properties --&gt;
          &lt;/bean&gt;
     &lt;/property&gt;
&lt;/bean&gt;</code></pre>
        <p>For persistency reasons, <MadCap:variable name="General.ProductNameIE" /> maintains a record of the last confirmed data that was copied from the Space to the external data source. If you need to undeploy and then redeploy a Processing Unit, for example as part of a maintenance activity, you can configure whether you want to perform a cold start of the Space (copy data from the Space to the external data source from the beginning), or whether you want the redeployed Processing Unit to continue copying the data from where it left off when it was undeployed.</p>
        <h2>Global Properties</h2>
        <p>The global configuration contains a list of DataLifecyclePolicy properties, as well as the AnalyticsXtreme logging level that is set for the entire application.</p>
        <table style="width: 100%;" class="tc-standard">
            <col />
            <col />
            <col />
            <col />
            <col />
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Description</th>
                    <th>Unit</th>
                    <th>Default Value</th>
                    <th>Required/Optional</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>verbose </td>
                    <td>Increases the log levels for both the client and the server to provide verbose AnalyticsXtreme information (useful for troubleshooting).</td>
                    <td>True/False</td>
                    <td>False</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>coldStart</td>
                    <td>(Automatic data tiering only) After redeploying a Processing Unit, set the value to "true" if you want to copy data from the <code>batchDataSource</code> to the<code> batchDataTarget</code> from the beginning. Leave the default value to continue copying data from the last confirmed data that was copied from the <code>batchDataSource</code> to the <code>batchDataTarget</code>.</td>
                    <td>True/False</td>
                    <td>False</td>
                    <td>Optional</td>
                </tr>
            </tbody>
        </table>
        <h1>Configuring the Data Life Cycle Policy</h1>
        <p>The next step in configuring AnalyticsXtreme is to define the data life cycle policy, which specifies how and when data is archived from the data grid (speed layer) to the external data storage (batch layer). This policy is configured in the AnalyticsXtreme Manager for each data object, or table. You must define the following properties in the policy: </p>
        <ul>
            <li><code>typeName</code>
            </li>
            <li><code>timeColumn</code>
            </li>
            <li><code>speedPeriod</code>
            </li>
            <li><code>batchDataSource</code>
            </li>
        </ul>
        <p>All other properties are optional, and you can leave the default values unless your specific environment has different requirements. See <MadCap:xref href="#Table">Table (Object) Properties</MadCap:xref> below for a full list of the data life cycle policy properties and their descriptions.</p>
        <div class="tc-admon-attention">
            <p>The relationship between the data object and the data life cycle policy is one to one. You cannot have more than one policy per data object, and each policy applies to only a single data object.</p>
        </div>
        <h2><a name="Adding"></a>Defining the Data Source and Data Target</h2>
        <p>Part of configuring the data life cycle policy is defining the <code>batchDataSource</code> property. You can define it directly, or you can use a <code>ref </code>to point to the definition. If you are implementing AnalyticsXtreme in external data tiering mode, this is the only required property because the data is simply evicted at the end of the life cycle.</p>
        <p>If you are implementing AnalyticsXtreme in automatic data tiering mode, you must also define the <code>batchDataTarget</code> property using the same method. Towards the end of the <code>speedPeriod</code>, the data will be moved to the target defined here.</p>
        <h3>Example</h3>
        <p>The following code snippet from the pu.xml file shows a data life cycle policy that was configured for specific trading information that needs to be stored in the data grid for 5 hours, and then moved to external data storage. Both the data source and the data target have their full definitions in another area of the pu.xml file.</p><pre><code class="language-xml">&lt;list&gt;
     &lt;!-- Data life cycle policy for Trade class --&gt;
     &lt;bean class="com.gigaspaces.analytics_xtreme.DataLifecyclePolicyFactoryBean"&gt;
          &lt;property name="typeName" value="com.gigaspaces.demo.Trade"/&gt;
          &lt;property name="timeColumn" value="dateTimeTrade"/&gt;
          &lt;property name="speedPeriod" value="pt5h"/&gt;
          &lt;property name="batchDataSource" ref="ax-datasource"/&gt;
          &lt;property name="batchDataTarget" ref="ax-datatarget"/&gt;
     &lt;/bean&gt;
     &lt;!-- Add a life cycle policy for each additional class --&gt;
&lt;/list&gt;</code><![CDATA[    ]]></pre>
        <h3>Configuring the Data Source</h3>
        <p>AnalyticsXtreme supports HDFS<MadCap:conditionalText MadCap:conditions="Default.DoNotShow"> and Amazon S3</MadCap:conditionalText> out of the box. However, you can add any external data source that is supported by Spark. </p>
        <p>The following code snippet demonstrates how to configure AnalyticsXtreme to work with a Spark Hive data source.</p><pre><code class="language-xml">&lt;!-- Data source plugin based on Spark Hive --&gt;
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataSourceFactoryBean"&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;</code><![CDATA[
]]></pre>
        <h3>Configuring the Data Target</h3>
        <p>There are three possible implementations for the data target out of the box; JDBC, Spark, and Hive. Each is wrapped in a factory bean. The <code>BatchDataTarget</code> interface can be used for other data targets by implementing the <code>feed</code> method. </p>
        <p><span class="tc-bold">Spark Hive Batch Data Target Exampl</span>e</p>
        <p>This example demonstrates the Spark Hive implementation.</p><pre><code class="language-xml">&lt;!-- Data target plugin based on Spark Hive --&gt;
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataTargetFactoryBean"&gt;
     &lt;property name="format" value="hive"/&gt;
     &lt;property name="mode" value="append"/&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Spark session provider --&gt;
&lt;bean id="ax-sparkSessionFactory" class="org.insightedge.spark.SparkSessionProviderFactoryBean"&gt;
     &lt;property name="master" value="local[*]"/&gt;
     &lt;property name="enableHiveSupport" value="true"/&gt;
     &lt;property name="configOptions"&gt;
          &lt;map&gt;
               &lt;entry key="hive.metastore.uris" value="thrift://hive-metastore:9083"/&gt;
          &lt;/map&gt;
     &lt;/property&gt;
&lt;/bean&gt;</code><![CDATA[
]]></pre>
        <p><span class="tc-bold">JDBC&#160;Batch Data Target Example</span>
        </p>
        <p>This example demonstrates the JDBC implementation.</p><pre><code class="language-xml">&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataSourceFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
&lt;/bean&gt;
	
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataTargetFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
     &lt;property name="useLowerCase" value="true"/&gt;
&lt;/bean&gt;</code></pre>
        <h3>Using a Different Apache Hive Version</h3>
        <p>Apache Spark currently is packaged with Apache Hive version 1.2.1. If you want to use a different Hive version for the batch data source or the batch data target, you need to do the following:</p>
        <ol>
            <li>Import the relevant JAR&#160;files.</li>
            <li>
                <p>Add the following to the pu.xml under the <code>sparkSessionFactory</code> <code>configOptions</code>:</p><pre><code class="language-xml">property name="configOptions"&gt;
       &lt;map&gt;
           &lt;entry key="hive.metastore.uris" value="thrift://hive-metastore:9083"/&gt;
           &lt;entry key="spark.sql.hive.metastore.version" value="2.3.2"/&gt;
           &lt;entry key="spark.sql.hive.metastore.jars"
                  value="#{systemEnvironment['SPARK_HOME']}/jars/hive/*:#{systemEnvironment['SPARK_HOME']}/jars/*"/&gt;
           &lt;entry key="spark.ui.enabled" value="false"/&gt;
       &lt;/map&gt;
   &lt;/property&gt;</code></pre>
            </li>
        </ol>
        <div class="tc-admon-note">
            <p>AnalyticsXtreme is tested and certified on Apache Hive version 2.3.2.</p>
        </div>
        <h2><a name="Table"></a>Table (Object) Properties</h2>
        <p>The DataLifecyclePolicy table contains the following parameters. The first four properties are required. The others are optional, and it is recommended to leave the default values as is unless your business application environment has specific requirements.</p>
        <table style="width: 100%;" class="tc-standard">
            <col />
            <col />
            <col />
            <col />
            <col />
            <thead>
                <tr>
                    <th>Parameter</th>
                    <th>Description</th>
                    <th>Unit</th>
                    <th>Default Value</th>
                    <th>Required/Optional</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>typeName*</td>
                    <td>Name of the table/type/class to which this policy applies.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>timeColumn** </td>
                    <td>Name of column/property/field that contains the time data used to manage this policy.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>speedPeriod </td>
                    <td>Time period or fixed timestamp.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>batchDataSource </td>
                    <td>Endpoint for querying the batch layer</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Required</td>
                </tr>
                <tr>
                    <td>batchDataTarget </td>
                    <td>Endpoint for feeding data to the batch layer. Only necessary when AnalyticsXtreme is implemented in Automatic data tiering mode.</td>
                    <td>&#160;</td>
                    <td>&#160;</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>timeFormat </td>
                    <td>Time format for the data in the timeColumn parameter.</td>
                    <td>See <MadCap:xref href="#Java">Java Time Pattern</MadCap:xref> below</td>
                    <td>Java time format (see the <a href="https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_LOCAL_DATE_TIME" target="_blank">Java API documentation)</a></td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>mutabilityPeriod </td>
                    <td>Period of time during which data can be updated.</td>
                    <td> Duration in time units or % of speedPeriod.</td>
                    <td>80%</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>batchFeedInterval </td>
                    <td>Data is fed from the Space (speed layer) to the batch layer in these time-based intervals at the end of the speedPeriod, after the mutabilityPeriod has expired.</td>
                    <td>See <MadCap:xref href="#Java">Java Time Pattern</MadCap:xref> below</td>
                    <td>pt1m</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>batchFeedSize </td>
                    <td>Maximum data entries per batch feed interval.</td>
                    <td>Integer&#160;(number of entries)</td>
                    <td>
                        <p class="tc-tablebody">1000 </p>
                    </td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>evictionPollingInterval </td>
                    <td>Polling interval for querying and evicting each policy.</td>
                    <td>See <MadCap:xref href="#Java">Java Time Pattern</MadCap:xref> below</td>
                    <td>pt1m</td>
                    <td>Optional</td>
                </tr>
                <tr>
                    <td>evictionBuffer </td>
                    <td>Additional waiting period before evicting data from the Space (speed layer)&#160;after it was fed to the batch layer, so that long queries and clock differences won't cause errors or generate exceptions. </td>
                    <td>Duration in time units or % of speedPeriod.</td>
                    <td>pt10m</td>
                    <td>Optional</td>
                </tr>
            </tbody>
        </table>
        <p>* This correlates to an object or JSON in the object store.</p>
        <p>** This correlates to a property or entity in an object or JSON.</p>
        <h3><a name="Java"></a>Java Time Pattern</h3>
        <p>AnalyticsXtreme is a time-based feature, and therefore uses the Java time pattern (required as of Java 8) in the <code>timeFormat</code> property. This time pattern requires use of the letter "p" in either upper or lower case in all duration fields, as well as the letter "t" for any duration that is less than 24 hours long.</p>
        <p>For example, <code>pt15m</code> represents a duration of 15 minutes, and <code>pt10h</code> represents a duration of 10 hours. A duration of 2 days should be written as <code>p2d</code>.</p>
        <p>For more information about the Java time pattern, see the <a href="https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-" target="_blank">Java API documentation</a>.</p>
        <h1>Exporting the AnalyticsXtreme Manager Service</h1>
        <p>In order for client applications to access the speed layer, the AnalyticsXtreme Manager needs to be exported  to enable discovery. This mechanism leverages the data grid's remoting mechanism, which registers the AnalyticsXtreme Manager as a remote service.</p>
        <p>The following code snippet demonstrates how to implement this mechanism in the pu.xml.</p><pre><code class="language-xml">&lt;!-- Register the ax-manager bean as a remote service so clients can load configuration &amp; stats from it --&gt;
&lt;os-remoting:service-exporter id="serviceExporter"&gt;
     &lt;os-remoting:service ref="ax-manager"/&gt;
&lt;/os-remoting:service-exporter&gt;</code></pre>
        <div class="tc-admon-note">
            <p>For more information about the data grid's remoting mechanism, see the <MadCap:xref href="space-based-remoting-overview.html">Space-Based Remoting</MadCap:xref> section of the developer guide.</p>
        </div>
        <h1>Sample pu.xml File</h1>
        <p>The following pu.xml file shows all the above steps, including defining a Space. </p><pre><code class="language-xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;beans xmlns="http://www.springframework.org/schema/beans"
     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
     xmlns:context="http://www.springframework.org/schema/context"
     xmlns:os-core="http://www.openspaces.org/schema/core"
     xmlns:os-remoting="http://www.openspaces.org/schema/remoting"
     xsi:schemaLocation="
   http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-<MadCap:variable name="Versions.spring-short" />.xsd
   http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-<MadCap:variable name="Versions.spring-short" />.xsd
   http://www.openspaces.org/schema/core http://www.openspaces.org/schema/14.0/core/openspaces-core.xsd
   http://www.openspaces.org/schema/remoting http://www.openspaces.org/schema/remoting/openspaces-remoting.xsd"&gt;

     &lt;context:annotation-config /&gt;

     &lt;os-core:annotation-support /&gt;

     &lt;os-core:embedded-space id="space" space-name="demo-space" /&gt;

     &lt;os-core:giga-space id="gigaSpace" space="space"/&gt;


&lt;bean id="ax-manager" class="com.gigaspaces.analytics_xtreme.server.AnalyticsXtremeManagerFactory"&gt;
     &lt;property name="config"&gt;
          &lt;bean class="com.gigaspaces.analytics_xtreme.AnalyticsXtremeConfigurationFactoryBean"&gt;
               &lt;!-- Verbose is recommended for getting started, usually turned off in production --&gt;
               &lt;property name="verbose" value="true"/&gt;
               &lt;!-- more properties --&gt;
          &lt;/bean&gt;
     &lt;/property&gt;
&lt;/bean&gt;
	
&lt;!-- Data life cycle policy for Trade class --&gt;
&lt;bean class="com.gigaspaces.analytics_xtreme.DataLifecyclePolicyFactoryBean"&gt;
     &lt;property name="typeName" value="com.gigaspaces.demo.Trade"/&gt;
     &lt;property name="timeColumn" value="dateTimeTrade"/&gt;
     &lt;property name="speedPeriod" value="pt5h"/&gt;
     &lt;property name="batchDataSource" ref="ax-datasource"/&gt;
     &lt;property name="batchDataTarget" ref="ax-datatarget"/&gt;
&lt;/bean&gt;

&lt;!-- Register the ax-manager bean as a remote service so clients can load configuration &amp; stats from it --&gt;
&lt;os-remoting:service-exporter id="serviceExporter"&gt;
     &lt;os-remoting:service ref="ax-manager"/&gt;
&lt;/os-remoting:service-exporter&gt;


&lt;!-- Data source plugin based on Spark Hive --&gt;
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataSourceFactoryBean"&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Data target plugin based on Spark Hive --&gt;
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.spark.SparkHiveBatchDataTargetFactoryBean"&gt;
     &lt;property name="format" value="hive"/&gt;
     &lt;property name="mode" value="append"/&gt;
     &lt;property name="sparkSessionProvider" ref="ax-sparkSessionFactory"/&gt;
&lt;/bean&gt;

&lt;!-- Spark session provider --&gt;
&lt;bean id="ax-sparkSessionFactory" class="org.insightedge.spark.SparkSessionProviderFactoryBean"&gt;
     &lt;property name="master" value="local[*]"/&gt;
     &lt;property name="enableHiveSupport" value="true"/&gt;
     &lt;property name="configOptions"&gt;
          &lt;map&gt;
               &lt;entry key="hive.metastore.uris" value="thrift://hive-metastore:9083"/&gt;
          &lt;/map&gt;
     &lt;/property&gt;
&lt;/bean&gt;

&lt;!-- To use jdbc instead of spark, replace ax-datasource, ax-datatarget and sparkSessionFactory with these beans
&lt;bean id="ax-datasource" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataSourceFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
&lt;/bean&gt;
	
&lt;bean id="ax-datatarget" class="com.gigaspaces.analytics_xtreme.jdbc.JdbcBatchDataTargetFactoryBean"&gt;
     &lt;property name="connectionString" value="jdbc:hive2://hive-server:10000/;ssl=false"/&gt;
     &lt;property name="useLowerCase" value="true"/&gt;
&lt;/bean&gt;
--&gt;
&lt;/beans&gt;</code><![CDATA[
]]></pre>
        <h1>Configuring AnalyticsXtreme on the Client Side</h1>
        <p>After the server side has been configured, you need to set up your client application to use AnalyticsXtreme. You must configure the JDBC&#160;connection string to enable communication between the client and the data grid, and then you can run queries using the following:</p>
        <ul>
            <li>Spark/JDBC API from any client application.</li>
            <li>Web notebook - while you can configure any web notebook to work with AnalyticsXtreme, we recommend using the <MadCap:variable name="General.ProductNameIE" /> Zeppelin notebook that comes packaged with <MadCap:variable name="General.ProductNameIE" />, which contains a custom JDBC&#160;interpreter.</li>
        </ul>
        <div class="tc-admon-note">
            <p>For more information about the InsightEdge Apache Zeppelin notebook, see the <MadCap:xref href="../started/insightedge-zeppelin.html">Using the [%=General.ProductNameIE%] Web Notebook</MadCap:xref> topic.</p>
        </div>
        <h2>Configuring the JDBC&#160;Connection String</h2>
        <p>To configure the client side to work with AnalyticsXtreme, you need to add the <code>analyticsXtreme.enabled=true</code> property to the JDBC connection string,  as described in the <MadCap:xref href="sql-driver.html">SQL Driver</MadCap:xref> topic, in the <MadCap:xref href="insightedge-apis.html">Running Analytics</MadCap:xref> section of the developer guide. When this property is true,  all queries (Spark, JDBC, etc.) pass through the AnalyticsXtreme engine.</p>
        <h2>Using Apache Spark with AnalyticsXtreme</h2>
        <p><MadCap:variable name="General.ProductNameIE" /> provides a simple API to initialize Spark over AnalyticsXtreme. Initializing Spark to work with AnalyticsXtreme is a two-step process:</p>
        <ol>
            <li>Connect Spark to the data grid.</li>
            <li>Create a Spark Dataframe with AnalyticsXtreme enabled.</li>
        </ol>
        <p>To implement the AnalyticsXtreme API, add the following Scala code snippet to any client code that will have to access AnalyticsXtreme. This code specifies the Space, initializes Spark, and creates the Spark Dataframe (per class).</p><pre><![CDATA[  ]]><code class="language-scala">import org.insightedge.spark.implicits.all._
    import org.insightedge.spark.context.InsightEdgeConfig
    
    //spaceName is required
    val ieConfig = new InsightEdgeConfig(spaceName = "speedSpace")
    
    //spark context initialization
    spark.initializeInsightEdgeContext(ieConfig)
	
	//create the AX dataframe object for the specific class
	val analyticsXtremeDataframe: Dataframe = spark.analyticsXtreme("Trade")</code></pre>
        <div class="tc-admon-note">
            <p>For more information about Spark data source APIs such as the Dataframe, see the <a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html" target="_blank">Spark documentation</a>.</p>
        </div>
        <h2>Spark Query Execution</h2>
        <p>The way in which Spark executes a query depends on how the Dataframe is initialized (as a table or as a raw query). Simple queries are identical in behavior, while queries that contain aggregations, such as <code>AVERAGE</code>, have significant differences in behavior depending on whether they are submitted as a table and are therefore executed on the client side, or as a raw query and therefore executed on the server side. This second method is called predicate pushdown.</p>
        <h3>Simple Query</h3>
        <p>The following code snippet contains a simple query submitted to the Spark Dataframe as a table.</p><pre><code class="language-sql">val volumeDataFrame = analyticsXtremeDataframe.select("volume").where("volume &lt; 1000")</code></pre>
        <p>This next code snippet contains the same simple query, but submitted as a raw query.</p><pre><code class="language-sql">val rawVolumeDataFrame = spark.read.analyticsXtreme("SELECT volume FROM Trade WHERE volume &lt; 1000")</code></pre>
        <p>The behavior is the same in both cases, because only data retrieval is required without any additional action in order to return the required result.</p>
        <h3>Query with Aggregation</h3>
        <p>For a more complex query that includes aggregation, the method by which the query is submitted causes important behavioral differences.</p>
        <p>In the code snippet below, the query is submitted to the Spark Dataframe as a table. Only the raw data is retrieved from the data grid; the client application has to perform the aggregation in order to finish executing the query and return results. </p><pre><code class="language-sql">val aggregationDataFrame = analyticsXtremeDataframe.agg("volume -&gt; avg", "endPrice -&gt; max", "startPrice -&gt; min")</code></pre>
        <p>When the query is submitted as a raw query, like in the following code snippet, Spark pushes the entire query (predicate) to the data grid. The query is executed from beginning to end in the data grid (both data retrieval and aggregation), and only the final results are returned to the client application. This predicate pushdown can significantly reduce the processing time of the query because it filters data out at the source, so that the query can complete faster and return only the required data back to the client application.</p><pre><code class="language-sql">val rawAggregationDataFrame = spark.read.analyticsXtreme("SELECT AVERAGE(volume) as avg_volume, MAX(endPrice) as max_endPrice, MIN(endPrice) as min_endPrice FROM Trade")</code></pre>
        <h1>Supported SQL Query Functions</h1>
        <p>The following SQL keywords and operations can be used with AnalyticsXtreme:</p>
        <ul>
            <li>Support   <code>SELECT</code>  SQL statement</li>
            <li><code>AND/OR</code> operators to join two or more conditions in a <code>WHERE</code> clause</li>
            <li><code>NOT IN</code> and <code>NOT NULL</code></li>
            <li>Aggregate functions: <code>COUNT</code>, <code>MAX</code>, <code>MIN</code>, <code>SUM</code>, <code>AVG</code></li>
            <li>All basic logical operations to create conditions: =, &lt;&gt;, &lt;,&gt;, &gt;=, &lt;=, <code>[NOT] like</code>, <code>is [NOT] null</code>, <code>IN</code>, <code>BETWEEN</code></li>
            <li><code>ORDER BY</code> for multiple columns</li>
            <li><code>GROUP BY</code> for multiple columns</li>
            <li>Column aliases</li>
        </ul>
        <h1>Limitations</h1>
        <div class="tc-admon-note">
            <p>This feature is meant for querying data that is being fed by other sources into the target database(s). It cannot be used to alter the data (<code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>).</p>
        </div>
        <p>The following SQL&#160;query functionality is not supported in this release:</p>
        <ul>
            <li>SQL keywords: <code>JOIN</code>, <code>HAVING</code></li>
            <li>Only the TIMESTAMP 'yyyy-MM-dd HH:mm:ss' format is supported in the SQL query</li>
            <li>Hot deploy&#160;of policy changes/updates is not supported. The Processing Unit must be undeployed and then redeployed.</li>
        </ul>
        <div class="tc-admon-note" MadCap:conditions="Default.DoNotShow">
            <p>The following keywords do not align with Space functionality and there is no plan to support them in the future: <code>INSERT</code>, <code>DELETE</code>, <code>UPDATE</code></p>
        </div>
    </body>
</html>