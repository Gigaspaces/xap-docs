<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1 class="tc-pagetitle">Predicate Pushdown</h1>
        <p><MadCap:variable name="General.ProductNameIE" /> enhances the Agile Spark pushdown predicate capability by leveraging our native advanced indexing mechanism, to retrieve only relevant data entries when running a query (filter). This ability to filter directly on the source (instead of on the target as is done in the vanilla Apache Spark architecture) dramatically improves performance and reduces network overhead.</p>
        <h1><a name="indexing"></a>Indexing</h1>
        <p>Pushdown predicates are dependent on indexing the relevant fields. The indexing can be adjusted to improve performance further, as described in the <a href="indexing-overview.html">Indexing</a> section. You can also refer to the <a href="insightedge-modeling.html">Data Modeling</a> topic.</p>
        <h1><a name="pushing-predicates-to-the-data-grid"></a>Pushing Predicates to the Data Grid</h1>
        <p>When executing a query in <MadCap:variable name="General.ProductNameIE" />, the following predicates are pushed down to the grid level.</p>
        <p>Basic equality comparisons:</p>
        <ul>
            <li><code>EqualTo</code>, <code>GreaterThan</code>, <code>GreaterThanOrEqual</code>, <code>LessThan</code>, <code>LessThanOrEqual</code></li>
            <li><code>IsNull</code>, <code>IsNotNull</code></li>
            <li><code>In</code>
            </li>
        </ul>
        <p>Geospatial predicates:</p>
        <ul>
            <li><code>GeoIntersects</code>
            </li>
            <li><code>GeoWithin</code>
            </li>
            <li><code>GeoContains</code>
            </li>
        </ul>
        <p>For more information about the geospatial predicates, refer to the <a href="insightedge-geospatial.html">GeoSpatial API</a> topic.</p>
        <h1><a name="rdd-api"></a>RDD API</h1>
        <p>To use the pushdown predicates with RDD, you can use the gridSql API as shown in the following example. With this API, the queries are performed in the data grid.</p><pre><code class="language-scala">val rdd1 = sc.gridSql[Product]("quantity &gt; 0")...

val rdd2 = sc.gridSql[Product]("quantity &gt; ? and featuredProduct = ?", Seq(10, true))
</code></pre>
        <h2><a name="geospatial-api-support"></a>Geospatial API Support</h2>
        <p>When using the geospatial API with the RDD API, the predicate is pushed down to the grid and the filtering is done on the data grid side. The gridSql API receives a Data Grid SQL query.</p><pre><code class="language-scala">val rdd = sc.gridSql[SpatialData]("location spatial:within ?", Seq(circle(point(10,10), 10)))
</code></pre>
        <p>For more information about these types of queries, refer to the <a href="insightedge-rdd.html">RDD API</a> topic.</p>
        <h1><a name="dataframe-dataset-apis"></a>DataFrame/DataSet APIs</h1>
        <p>When using the DataFrame or DataSet APIs, filters are performed in the client side (executors). If you choose to work with these APIs, use the regular syntax of the <span class="tc-bold">filter</span> and <span class="tc-bold">where</span> APIs. <MadCap:variable name="General.ProductNameIE" /> pushes down the supported predicates to the grid level.</p><pre><code class="language-scala">//DataFrame
val df = spark.read.grid[Product].where("quantity &gt; 0")

//Dataset
val ds = spark.read.grid[Product].as[Product].where("quantity &gt; 0")
</code></pre>
        <div class="tc-admon-note">
            <p>When using the custom function syntax for filtering, the filtering itself is performed on the client side, not in the data grid.</p><pre><code class="language-scala">//This does not use pushdown predicates and is performed in the client side.
spark.read.grid[Product].as[Product].filter(p =&gt; p.quantity &gt; 0).count()
</code></pre>
        </div>
        <h2><a name="geospatial-api-support-1"></a>Geospatial API Support</h2>
        <p>Although this API is supported, the predicates are not pushed down to the data grid and therefore filtering is done on the client side.</p><pre><code class="language-scala">val df = sqlContext.read.grid[SpatialData]
val data = df.filter(df("location") geoWithin circle(point(10,10), 10))
</code></pre>
        <p>For more information about using geospatial predicates with these APIs, refer to the <a href="insightedge-datasets.html">Dataset API</a> and <a href="insightedge-dataframes.html">DataFrame API</a> topics.</p>
        <h2><a name="aggregations"></a>Aggregations</h2>
        <p>Currently, Apache Spark does not push down aggregation queries (<code>groupBy</code>, <code>count</code>, etc.) to the data source. When this functionality becomes available in Apache Spark, it can also be used in <MadCap:variable name="General.ProductNameIE" />.</p>
    </body>
</html>