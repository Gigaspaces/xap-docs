<?xml version="1.0" encoding="utf-8"?>
<html xmlns:MadCap="http://www.madcapsoftware.com/Schemas/MadCap.xsd">
    <head>
    </head>
    <body>
        <h1 class="tc-pagetitle">Dataset API</h1>
        <p>A Dataset is a distributed collection of data. Datasets provide the benefits of RDDs (strong typing, ability to use powerful lambda functions)
with the benefits of Spark SQL’s optimized execution engine. Both the typed transformations (e.g., map, filter, and groupByKey) and
untyped transformations (e.g., select and groupBy) are available on the Dataset class.</p>
        <p>Datasets use a specialized Encoder to serialize the objects for processing or transmitting over the network.
Encoders for most common types are automatically provided by importing <code>spark.implicits._</code></p>
        <p>To convert a <code>DataFrame</code> to a <code>Dataset</code> use the <code>as[U]</code> conversion method.</p>
        <div class="tc-admon-refer">
            <p>To read more about Dataset API, please refer to <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank">Spark Documentation</a>.
<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank">Spark class Dataset</a>.</p>
        </div>
        <p>This section describes how to use the Dataset API with the Data Grid.</p>
        <h1><a name="preparing"></a>Preparing</h1>
        <p>The entry point to Dataset features is Spark <code>SparkSession</code> (replaces the old <code>SQLContext</code>).</p>
        <div class="easyui-tabs" plain="true" data-options="">
            <div title="Scala" style="padding:10px"><pre><code class="language-scala">import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder().getOrCreate()

// For implicit conversions like converting RDDs to DataFrames to Dataset
import spark.implicits._

// This is used to simplify calling Data Grid features
import org.insightedge.spark.implicits.all._
</code></pre>
            </div>
        </div>
        <h1><a name="person-case-class"></a>Person Case Class</h1>
        <p>For the following code snippets, we will use a simple case class named Person.
This case class can be written to (and loaded from) the Data Grid or an external persisted storage.</p>
        <div class="easyui-tabs" plain="true" data-options="">
            <div title="Scala" style="padding:10px"><pre><code class="language-scala">import org.insightedge.scala.annotation._
import scala.beans.BeanProperty

case class Person(
    @BeanProperty @SpaceId(autoGenerate = true) var id: String,
    @BeanProperty var name: String,
    @BeanProperty var age: Int ) {
   
    def this() = this(null, null, -1)
}

</code></pre>
            </div>
        </div>
        <h1><a name="creating-datasets"></a>Creating Datasets</h1>
        <p>RDDs are stored in Data Grid simply as collections of objects of a certain type.
You can create Dataset by such type with the next syntax:</p>
        <div class="easyui-tabs" plain="true" data-options="">
            <div title="Using SparkSession" style="padding:10px"><pre><code class="language-scala">import org.insightedge.spark.implicits.all._

val spark: SparkSession // An existing SparkSession.

val ds: Dataset[Person] = spark.read.grid[Person].as[Person]

// Displays the content of the Dataset to stdout
ds.show()

// We use the dot notation to access individual fields
// and count how many people are below 60 years old
val below60 = ds.filter( p =&gt; p.age &lt; 60).count()
</code></pre>
            </div>
            <div title="SQL syntax" style="padding:10px"><pre><code class="language-scala">val spark: SparkSession // An existing SparkSession.

spark.sql(
  s"""
     |create temporary table people
     |using org.apache.spark.sql.insightedge
     |options (class "${classOf[Person].getName}")
  """.stripMargin)

val ds: Dataset[Person] = spark.sql("select * from people").as[Person]

// Displays the content of the Dataset to stdout
ds.show()
</code></pre>
            </div>
            <div title="Without imports" style="padding:10px"><pre><code class="language-scala">val spark: SparkSession // An existing SparkSession.

val ds = spark.read
              .format("org.apache.spark.sql.insightedge")
              .option("class", classOf[Person].getName)
              .load().as[Person]

// Displays the content of the Dataset to stdout
ds.show()
</code></pre>
            </div>
        </div>
        <h1><a name="persisting-datasets-to-data-grid"></a>Persisting Datasets to the Data Grid</h1>
        <p>To write a Dataset use <code>Dataset.write</code>. The content of the Dataset is saved with a specified collection name.
The behavior of the write operation is controlled by the <code>SaveMode</code>.</p>
        <p>Since Datasets are no longer linked to object type, the content of the Dataset is persisted by the specified collection name.
Thus, when saving a Dataset to the Data Grid, you must provide a collection name, and when loading persisted Dataset,
the same collection name must be used instead of object type.</p>
        <p>To persist and load persisted Datasets you can use the following syntax:</p>
        <div class="easyui-tabs" plain="true" data-options="">
            <div title="Simplified syntax" style="padding:10px"><pre><code class="language-scala">import org.insightedge.spark.implicits.all._

val spark: SparkSession // An existing SparkSession.
val ds: Dataset[Person] // An existing Dataset of type Person

// Filter out teens (ages 13-19) and persist to the specified path "teens"
ds.filter( p =&gt; p.age &gt;= 13 &amp;&amp; p.age &lt;= 19).write.mode(SaveMode.Overwrite).grid("teens")

// Load persisted data from the specified path
val persisted: Dataset[Person] = spark.read.grid("teens").as[Person]

// Displays the content of the Dataset to stdout
persisted.show()
</code></pre>
            </div>
            <div title="Without imports" style="padding:10px"><pre><code class="language-scala">val ds: Dataset[Person] // An existing Dataset of type Person

ds.write
    .format("org.apache.spark.sql.insightedge")
    .mode(SaveMode.Overwrite)
    .save("people")

val persisted: Dataset[Person] = spark.read
    .format("org.apache.spark.sql.insightedge")
    .load("people").as[Person]

// Displays the content of the Dataset to stdout
persisted.show()
</code></pre>
            </div>
        </div>
        <div class="tc-admon-note">
            <p>Similar to DataFrames, nested properties are stored as <code>DocumentProperties</code> in Data Grid when Dataset is persisted</p>
        </div>
        <p>Persisted Datasets are shared among multiple Spark jobs and stay alive after the jobs are complete.</p>
        <h1><a name="nested-properties"></a>Nested Properties</h1>
        <p>Let's enhance <code>Person</code> class with an additional property <code>Address</code> (that has some properties of it's own).
If you write some <code>Person</code> to a Data Grid, e.g. by persisting an RDD, and then load
them as Dataset, you can see that Dataset schema includes nested properties:</p>
        <div class="easyui-tabs" plain="true" data-options="">
            <div title="Scala" style="padding:10px"><pre><code class="language-scala">import org.insightedge.spark.implicits.all._

val spark: SparkSession // An existing SparkSession.

// Write some person to a Data Grid
val person: Person //Some person
val rdd = spark.sparkContext.parallelize(Seq(person))
rdd.saveToGrid()

// Load the RDD as a Dataset
val ds: Dataset[Person] = spark.read.grid[Person].as[Person]

// Displays the schema of the Dataset to stdout
ds.printSchema()
</code></pre>
            </div>
        </div>
        <p>This code will print the next schema:</p><pre><code>root
 |-- id: string (nullable = true)
 |-- name: string (nullable = true)
 |-- age: integer (nullable = false)
 |-- address: struct (nullable = true)
 |    |-- city: string (nullable = true)
 |    |-- state: string (nullable = true)
</code></pre>
        <p>Nested properties can be accessed using dot-separated syntax, e.g. <code>address.city</code>.
Here is an example of filtering by nested properties using Dataset API:</p>
        <div class="easyui-tabs" plain="true" data-options="">
            <div title="Scala" style="padding:10px"><pre><code class="language-scala">val spark: SparkSession // An existing SparkSession.

val ds: Dataset[Person] = spark.read.grid[Person].as[Person]
val countInBuffalo = ds.filter( p =&gt; p.address.city == "Buffalo").count()

// Displays number of people from Buffalo
println(countInBuffalo)
</code></pre>
            </div>
        </div>
    </body>
</html>